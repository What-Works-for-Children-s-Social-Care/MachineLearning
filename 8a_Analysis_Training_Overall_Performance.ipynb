{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable set-up\n",
    "\n",
    "rq = 'rq2' # Options: 'rq1', 'rq2'\n",
    "cv = 'ts' # Options: 'ts' (time series split, ignore siblings), 'ss' (stratified shuffle, ignore siblings)\n",
    "data_type = 'all' # Options: 'str' (just structured data), 'all' (structured data and list of strings)\n",
    "algorithm_names = ['decision_tree', 'logistic_regression', 'gradient_boosting'] \n",
    "rcv_n_iter = 50 # The more iterations, the more the randomised search searches for an optimal solution\n",
    "parameters = 2 # \n",
    "\n",
    "# Don't change\n",
    "file_stub_y_siblings = rq + '_' + cv + '_str' # use 'str' for all \n",
    "file_stub = rq + '_' + cv + '_' + data_type # Creates file stub for saving in the format e.g. rq1_ss_str\n",
    "levers =  str(rcv_n_iter) + '_' + str(parameters)\n",
    "print(file_stub + '_' + levers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## File directories\n",
    "local_dir = '/Users/[username]/Documents/Final transfer out data and code to WWC Jan 2020' # Insert [username]\n",
    "hard_drive_dir = '/Volumes/diskAshur2/Final transfer out data and code to WWC Jan 2020/Data for model/Use'\n",
    "summary_info = '/Users/[username]/Documents/Summary information' # Insert [username]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vary hyperparameters\n",
    "\n",
    "## Starting hyperparameters\n",
    "# Logistic regression (regularised, unregularised) - start with regularisation strength of 1\n",
    "# Gradient boosting - start with 50 trees, max_depth = 10, min_leaf_node = 5, max_features = sqrt(no. of features) \n",
    "# Decision Tree\n",
    "\n",
    "# https://stackoverflow.com/questions/44572109/what-are-the-arguments-for-scipy-stats-uniform\n",
    "# ower bound, and the second argument is the range of the distribution\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from sklearn.utils.fixes import loguniform\n",
    "\n",
    "parameters_dtc = [{'dtc__max_features': np.random.uniform(low=0,high=0.8, size=50), \n",
    "                'dtc__max_depth': stats.uniform(5, 15),\n",
    "                'dtc__min_samples_split': stats.randint(20, 40)\n",
    "                  }]\n",
    "\n",
    "parameters_lr = [{'lr__penalty': ['l1', 'l2'], # 'none' = no regularisation (didn't accept), l1 selects features, l2 pulls the weight down\n",
    "        'lr__C': loguniform(1e-3, 1e-1)  # smaller values => stronger regularisation\n",
    "                 }] \n",
    "\n",
    "parameters_gbc = [{'gbc__n_estimators': stats.randint(50, 150),\n",
    "                'gbc__max_depth': stats.randint(5, 10), \n",
    "                'gbc__max_features': np.random.uniform(low=0,high=0.8, size=50)  \n",
    "                }]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below here - generic set-up - don't vary. Change which models are run and output saved by varying the parameters above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load user-written functions \n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import analysis_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory\n",
    "import os\n",
    "import pickle\n",
    "os.chdir(hard_drive_dir)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import structured data -train, test data\n",
    "import pandas as pd\n",
    "X_tr = pd.read_csv(\"X_train_{}.csv\".format(file_stub), index_col = 0)\n",
    "print(X_tr.shape)\n",
    "X_tr.reset_index(inplace = True, drop = True)\n",
    "print(X_tr.index)\n",
    "\n",
    "X_test = pd.read_csv(\"X_test_{}.csv\".format(file_stub), index_col = 0)\n",
    "print(X_test.shape)\n",
    "X_test.reset_index(inplace = True, drop = True)\n",
    "print(X_test.index)\n",
    "\n",
    "y_tr = pd.read_csv(\"y_train_{}.csv\".format(file_stub_y_siblings), index_col = 0, header = None)\n",
    "print(y_tr.shape)\n",
    "y_tr.reset_index(inplace = True, drop = True)\n",
    "y_tr = pd.Series(y_tr[1])\n",
    "print(y_tr.index)\n",
    "\n",
    "y_test = pd.read_csv(\"y_test_{}.csv\".format(file_stub_y_siblings), index_col = 0, header = None)\n",
    "print(y_test.shape)\n",
    "y_test.reset_index(inplace = True, drop = True)\n",
    "y_test = pd.Series(y_test[1])\n",
    "print(y_test.index)\n",
    "\n",
    "siblings_tr = pd.read_csv(\"siblings_train_{}.csv\".format(file_stub_y_siblings), index_col = 0, header = None)\n",
    "print(siblings_tr.shape)\n",
    "siblings_tr.reset_index(inplace = True, drop = True)\n",
    "siblings_tr = pd.Series(siblings_tr[1])\n",
    "print(siblings_tr.index)\n",
    "\n",
    "siblings_test = pd.read_csv(\"siblings_test_{}.csv\".format(file_stub_y_siblings), index_col = 0, header = None)\n",
    "print(siblings_test.shape)\n",
    "siblings_test.reset_index(inplace = True, drop = True)\n",
    "siblings_test = pd.Series(siblings_test[1])\n",
    "print(siblings_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#plt.scatter(X_tr['ReferralDatetime'], y_tr)\n",
    "y_tr[0:2*int(y_tr.shape[0]/3)].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(X_test.columns).difference(set(X_tr.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(set(X_test.columns).difference(set(X_tr.columns))) == 0)\n",
    "assert(len(set(X_tr.columns).difference(set(X_test.columns))) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (len([col for col in X_tr.columns if 'ethnicity' in col]) == 0)\n",
    "assert (len([col for col in X_test.columns if 'ethnicity' in col]) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sample_size = y_tr.shape[0] + y_test.shape[0]\n",
    "print(sample_size)\n",
    "percentage_positive_case = round((y_tr.value_counts() + y_test.value_counts())[1] / sample_size * 100, 2)\n",
    "print(percentage_positive_case)\n",
    "\n",
    "data_description = pd.Series({'Sample size': sample_size,\n",
    "                    '% positive case': percentage_positive_case})\n",
    "\n",
    "\n",
    "data_description.to_csv('{}/Sample Sizes/Data Description_{}.csv'.format(local_dir, file_stub))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert 1==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check no overlapping Child_IDs or siblings\n",
    "assert len(list(set(X_tr['PSID']).intersection(set(X_test['PSID'])))) <= 1\n",
    "\n",
    "assert len(list(set(siblings_tr).intersection(set(siblings_test)))) <= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr.select_dtypes(exclude='number').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete duplicate columns\n",
    "print(X_tr.shape)\n",
    "X_tr = X_tr.loc[:,~X_tr.columns.duplicated()]\n",
    "print(X_tr.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "X_test = X_test.loc[:,~X_test.columns.duplicated()]\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Check unique values\n",
    "# Lots of unique values => personal ID\n",
    "# Only one unique value => feature doesn't vary\n",
    "import operator\n",
    "dict_unique = {}\n",
    "for col in X_tr.columns:\n",
    "    len_unique = len(X_tr[col].unique())\n",
    "    dict_unique[col] = len_unique\n",
    "non_varying_cols = [col for col, len_unique in dict_unique.items() if len_unique ==1]\n",
    "dict_unique_sorted = sorted(dict_unique.items(), key=operator.itemgetter(1))\n",
    "print('Unique dictionary: ', dict_unique_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non varying colums (as they won't add anything to the model)\n",
    "print(X_tr.shape)\n",
    "print(X_test.shape)\n",
    "X_tr.drop(columns = non_varying_cols, inplace = True, errors = 'ignore')\n",
    "X_test.drop(columns = non_varying_cols, inplace = True, errors = 'ignore')\n",
    "print(X_tr.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr.select_dtypes(exclude='number').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that are missing in test and test even if not missing in the training\n",
    "print(X_tr.shape)\n",
    "X_tr.dropna(axis=1, how = 'all', inplace = True)\n",
    "print(X_tr.shape)\n",
    "X_test = X_test[X_tr.columns]\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr.select_dtypes(exclude='number').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop missing with more than 30%\n",
    "# Missingness is otherwise handled within each fold in the gridsearch cv pipeline\n",
    "import pandas as pd\n",
    "print(X_tr.shape)\n",
    "X_tr = X_tr.dropna(axis=1, how='all')\n",
    "print(X_tr.shape)\n",
    "\n",
    "percent_missing = X_tr.isnull().sum() * 100 / len(X_tr)\n",
    "missing_value_df = pd.DataFrame({'column_name': X_tr.columns,\n",
    "                                 'percent_missing': percent_missing})\n",
    "missing_value_df.reset_index(inplace = True)\n",
    "cols_to_drop = list(missing_value_df.loc[missing_value_df['percent_missing'] >=70,'column_name']) # CORRECTED: 0.7 to 70\n",
    "X_tr.drop(columns = cols_to_drop, inplace = True)\n",
    "print(X_tr.shape)\n",
    "\n",
    "# Test and test data should match the columns in training data\n",
    "print(X_test.shape)\n",
    "X_test = X_test[list(X_tr.columns)]\n",
    "print(X_test.shape)\n",
    "\n",
    "assert (X_tr.shape[1] == X_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr.select_dtypes(exclude='number').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with possible information leakage\n",
    "\n",
    "possible_information_leakage_cols = (['previous_mean_gaptoOtherEscalationstart', 'previous_mean_gaptocpplanstart',\n",
    "                                    'previous_mean_gaptoLACstart', 'previous_mean_opencaselength', \n",
    "                                     'AssessmentType_ Child Social Work Assessment',\n",
    "                                     'AssessmentType_ Child Social Work Assessment for Review Child Protection Conference',\n",
    "                                'AssessmentType_ Child Social Work Assessment to Initial Child Protection Conference'])\n",
    "X_tr.drop(columns = possible_information_leakage_cols, inplace = True, errors = 'ignore')\n",
    "X_test.drop(columns = possible_information_leakage_cols, inplace = True, errors = 'ignore')\n",
    "print(X_tr.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr.select_dtypes(exclude='number').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle year (this was missed in data cleaning)\n",
    "\n",
    "X_tr['ReferralDatetime_month_year'] = pd.to_datetime(X_tr['ReferralDatetime_month_year'])\n",
    "X_tr['ReferralDatetime_month_year'] = X_tr['ReferralDatetime_month_year'].dt.year.map(int)\n",
    "\n",
    "X_test['ReferralDatetime_month_year'] = pd.to_datetime(X_test['ReferralDatetime_month_year'])\n",
    "X_test['ReferralDatetime_month_year'] = X_test['ReferralDatetime_month_year'].dt.year.map(int)\n",
    "\n",
    "# Drop key cols for the purposes of handling missing and resampling (will add back into the saved dataset later)\n",
    "key_cols = ['PSID','ReferralDatetime', 'ReferralDatetime_previous']\n",
    "key_cols = [col for col in key_cols if col in X_tr.columns]\n",
    "X_tr_key_cols_df = X_tr[key_cols ] # 'ReferralDatetime_previous'\n",
    "print(X_tr_key_cols_df.shape)\n",
    "print(X_tr_key_cols_df.index)\n",
    "\n",
    "X_test_key_cols_df = X_test[key_cols ] #'ReferralDatetime_previous'\n",
    "print(X_test_key_cols_df.shape)\n",
    "print(X_test_key_cols_df.index)\n",
    "\n",
    "X_tr.drop(columns = key_cols, inplace = True) # 'ReferralDatetime_previous'\n",
    "X_test.drop(columns = key_cols, inplace = True) # 'ReferralDatetime_previous'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Downsample the majority class (fine that it's outside the cross validation loop)\n",
    "# \n",
    "from imblearn.pipeline import Pipeline # Use this Pipeline, sklearn doesn't like ADASYN in its Pipeline (not a transformer)\n",
    "from imblearn.under_sampling import OneSidedSelection\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "handle_missing = SimpleImputer(strategy='constant', add_indicator=True)\n",
    "X_tr_no_na = handle_missing.fit_transform(X_tr)\n",
    "cols_w_missing_data = [col for col in X_tr if X_tr[col].isna().sum() != 0]\n",
    "missing_cols = [col+'_missing' for col in cols_w_missing_data]\n",
    "X_tr_no_na_missing_cols = list(X_tr.columns) + list(missing_cols)\n",
    "X_tr_no_na = pd.DataFrame(data = X_tr_no_na, columns = X_tr_no_na_missing_cols)\n",
    "print(X_tr_no_na.shape)\n",
    "\n",
    "X_test_no_na = handle_missing.fit_transform(X_test)\n",
    "cols_w_missing_data = [col for col in X_test if X_test[col].isna().sum() != 0]\n",
    "missing_cols = [col+'_missing' for col in cols_w_missing_data]\n",
    "X_test_no_na_missing_cols = list(X_test.columns) + list(missing_cols)\n",
    "X_test_no_na = pd.DataFrame(data = X_test_no_na, columns = X_test_no_na_missing_cols)\n",
    "print(X_test_no_na.shape)\n",
    "\n",
    "resampling = OneSidedSelection(random_state=3005)\n",
    "pipeline = Pipeline([('resampling',resampling)])\n",
    "\n",
    "X_tr_transformed, y_tr_transformed = pipeline.fit_resample(X_tr_no_na, y_tr) # Don't use - not sorted correctly\n",
    "sample_indices = pipeline.named_steps['resampling'].sample_indices_\n",
    "\n",
    "# Select sample indices, sort and reset index because the resampled data resorts the data so all the positive class \n",
    "# are at the end\n",
    "y_tr_resampled = y_tr[sample_indices]\n",
    "y_tr_resampled.sort_index(inplace = True)\n",
    "y_tr_resampled.reset_index(drop=True, inplace = True)\n",
    "print(y_tr_resampled.index)\n",
    "\n",
    "X_tr_resampled = X_tr_no_na.loc[sample_indices,]\n",
    "X_tr_resampled.sort_index(inplace = True)\n",
    "X_tr_resampled.reset_index(drop=True, inplace = True)\n",
    "print(X_tr_resampled.index)\n",
    "\n",
    "siblings_tr_resampled = siblings_tr.loc[sample_indices,]\n",
    "siblings_tr_resampled.sort_index(inplace = True)\n",
    "siblings_tr_resampled.reset_index(drop=True, inplace = True)\n",
    "print(siblings_tr_resampled.index)\n",
    "\n",
    "X_tr_key_cols_df_resampled = X_tr_key_cols_df.loc[sample_indices,]\n",
    "X_tr_key_cols_df_resampled.sort_index(inplace = True)\n",
    "X_tr_key_cols_df_resampled.reset_index(drop=True, inplace = True)\n",
    "print(X_tr_key_cols_df_resampled.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes missing column doesn't appear in test so add manually\n",
    "cols_not_in_test = list(set([col for col in X_tr_no_na.columns if 'missing' in col]).difference(set([col for col in X_test_no_na.columns if 'missing' in col])))\n",
    "\n",
    "for col in cols_not_in_test:\n",
    "    X_test_no_na[col] = 0\n",
    "    \n",
    "print(X_tr_resampled.shape)\n",
    "print(X_test_no_na.shape)\n",
    "assert (X_tr_resampled.shape[1] == X_test_no_na.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check no missing after handling NA\n",
    "assert (X_tr_resampled.isna().sum().sum() == 0)\n",
    "assert (X_test_no_na.isna().sum().sum() == 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save so above transformations on the data persists for fairness models too\n",
    "\n",
    "# Adding back in key cols to uniquely identify the rows\n",
    "print(X_tr_resampled.shape)\n",
    "X_tr_resampled = X_tr_resampled.merge(X_tr_key_cols_df_resampled, left_index = True, right_index = True)\n",
    "print(X_tr_resampled.shape)\n",
    "\n",
    "print(X_test_no_na.shape)\n",
    "X_test_no_na = X_test_no_na.merge(X_test_key_cols_df, left_index = True, right_index = True)\n",
    "print(X_test_no_na.shape)\n",
    "\n",
    "X_tr_resampled.to_csv(\"Final/X_train_{}_final.csv\".format(file_stub))\n",
    "y_tr_resampled.to_csv(\"Final/y_train_{}_final.csv\".format(file_stub))\n",
    "siblings_tr_resampled.to_csv(\"Final/siblings_train_{}_final.csv\".format(file_stub))\n",
    "X_test_no_na.to_csv(\"Final/X_test_{}_final.csv\".format(file_stub)) # Remains unsampled so that it stays close to real world scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_no_na.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_tr_resampled.select_dtypes(exclude = 'number').columns)\n",
    "print(X_test_no_na.select_dtypes(exclude = 'number').columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop key columns (after data is saved as still need the keys for merging in 7a_Combine_and_Split_Data and 8_Fairness)\n",
    "print(X_tr_resampled.shape)\n",
    "print(X_test_no_na.shape)\n",
    "\n",
    "key_cols = ['PSID','ReferralDatetime', 'ReferralDatetime_previous']\n",
    "key_cols = [col for col in key_cols if col in X_tr.columns]\n",
    "X_tr_resampled.drop(columns = key_cols, inplace = True, errors = 'ignore')\n",
    "X_test_no_na.drop(columns = key_cols, inplace = True, errors = 'ignore')\n",
    "print(X_tr_resampled.shape)\n",
    "print(X_test_no_na.shape)\n",
    "\n",
    "X_tr_resampled = X_tr_resampled.select_dtypes(include = 'number') \n",
    "X_test_no_na = X_test_no_na.select_dtypes(include = 'number') \n",
    "print(X_tr_resampled.shape)\n",
    "print(X_test_no_na.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check no string columns except those defined as text\n",
    "assert len(X_tr_resampled.select_dtypes(exclude = 'number').columns) == 0\n",
    "assert len(X_test_no_na.select_dtypes(exclude = 'number').columns) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe for saving model results\n",
    "# But mode='x' so doesn't overwrite saved results\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "cols_for_model_results = ['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time',\n",
    "       'split0_test_score', 'split1_test_score','split2_test_score', 'mean_test_score',\n",
    "       'std_test_score', 'rank_test_score', 'dtc__max_depth', 'dtc__max_features', \n",
    "       'dtc__min_samples_split', 'lr__C', 'lr__penalty', 'gbc__max_depth', 'gbc__max_features', \n",
    "       'gbc__n_estimators', 'algorithm', 'rcv_n_iter',\n",
    "        'parameter_combination']\n",
    "\n",
    "df_model_outputs = pd.DataFrame(columns = cols_for_model_results)\n",
    "try:\n",
    "    df_model_outputs.to_csv('{}/Models/model_output_{}_decision_tree_{}.csv'.format(local_dir, file_stub, levers), mode='x')\n",
    "except(FileExistsError):\n",
    "    pass\n",
    "try:\n",
    "    df_model_outputs.to_csv('{}/Models/model_output_{}_logistic_regression_{}.csv'.format(local_dir, file_stub, levers), mode='x')\n",
    "except(FileExistsError):\n",
    "    pass\n",
    "try:\n",
    "    df_model_outputs.to_csv('{}/Models/model_output_{}_gradient_boosting_{}.csv'.format(local_dir, file_stub, levers), mode='x')\n",
    "except(FileExistsError):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FeatureUnion concatenates whilsts ColumnTransformer allows transformations on select columns and then concatenation\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.impute import SimpleImputer, MissingIndicator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from text_functions import vulnerabilities_phrases_dict\n",
    "\n",
    "numerical_transformer = None\n",
    "\n",
    "# FillNA for numerics (mean imputing only within cross-validation folds)\n",
    "if data_type == 'str':\n",
    "    preprocessor = numerical_transformer\n",
    "    \n",
    "# If text columns included\n",
    "\n",
    "if rq == 'rq1':\n",
    "    string_features = (['Contact and Referral Form_text',\n",
    "       'Child Social Work Assessment for Review Child Protection Conference_text_prev',\n",
    "       'Child Social Work Assessment to Initial Child Protection Conference_text_prev',\n",
    "       'Child Social Work Assessment_text_prev'])\n",
    "else:\n",
    "    string_features = (['Child Social Work Assessment for Review Child Protection Conference_text',\n",
    "           'Child Social Work Assessment to Initial Child Protection Conference_text',\n",
    "           'Child Social Work Assessment_text', 'Contact and Referral Form_text',\n",
    "           'Child Social Work Assessment for Review Child Protection Conference_text_prev',\n",
    "           'Child Social Work Assessment to Initial Child Protection Conference_text_prev',\n",
    "           'Child Social Work Assessment_text_prev',\n",
    "           'Contact and Referral Form_text_prev'])\n",
    "\n",
    "\n",
    "# LDA\n",
    "if file_stub == 'rq2_ss_all':\n",
    "    n_components=6\n",
    "else:\n",
    "    n_components=4\n",
    "\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=n_components,\n",
    "                                        learning_method='online',\n",
    "                                     random_state=3005,\n",
    "                                     evaluate_every=-1,\n",
    "                                     learning_decay=.7,\n",
    "                                     batch_size=64)\n",
    "\n",
    "if data_type == 'all':\n",
    "    transformers = []\n",
    "\n",
    "    text_transformers = FeatureUnion(\n",
    "                            transformer_list = [\n",
    "                                ('lda', lda_model)]) \n",
    "        \n",
    "    for f in string_features:\n",
    "        try:\n",
    "            f_all = [col for col in X_tr_resampled.columns if f in col]\n",
    "            f_tfidf = [s for s in f_all if not any(xs in s for xs in list(vulnerabilities_phrases_dict.keys()))]\n",
    "            transformers.append((f+'_lda', text_transformers, f_tfidf)) # LDA\n",
    "        except:\n",
    "            continue\n",
    "    preprocessor = ColumnTransformer(transformers = transformers,\n",
    "                                        remainder = 'passthrough') # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis_functions import StratifiedShuffleSplitGroups, TimeSeriesSplitIgnoreSiblings, grid_search_save_output\n",
    "\n",
    "#from imblearn.over_sampling import ADASYN, SMOTE\n",
    "#from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline # Use this Pipeline, sklearn doesn't like ADASYN in its Pipeline (not a transformer)\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_selection import SelectFpr, f_classif, VarianceThreshold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "    \n",
    "# Cross validation\n",
    "if cv == 'ts':\n",
    "    tss_sib = TimeSeriesSplitIgnoreSiblings(n_splits=3, sibling_group = siblings_tr, sibling_na = 99999.0)\n",
    "    cross_val = tss_sib\n",
    "else:\n",
    "    sssg = StratifiedShuffleSplitGroups(n_splits=5, test_size=0.2, random_state=3005, sibling_group = siblings_tr, sibling_na = 99999.0)\n",
    "    cross_val = sssg\n",
    "\n",
    "## Algorithm options\n",
    "\n",
    "dtc = DecisionTreeClassifier(random_state=3005)\n",
    "pipeline_dtc = Pipeline([('preprocessor', preprocessor),\n",
    "                        ('dtc', dtc)])\n",
    "\n",
    "# Logistic Regression\n",
    "scaler = StandardScaler()\n",
    "lr = LogisticRegression(random_state=3005, fit_intercept=True, solver = 'liblinear') \n",
    "pipeline_lr = (Pipeline([('preprocessor', preprocessor),\n",
    "                    ('scale',scaler), # regularisation requires features in same scale\n",
    "                    ('lr', lr)]))\n",
    "\n",
    "# Gradient Boosting\n",
    "gbc = GradientBoostingClassifier(random_state = 3005)\n",
    "pipeline_gbc = Pipeline([('preprocessor', preprocessor),\n",
    "                            ('gbc', gbc)])\n",
    "\n",
    "\n",
    "parameter_dict = ({'decision_tree': parameters_dtc, \n",
    "                    'logistic_regression': parameters_lr, \n",
    "                    'gradient_boosting': parameters_gbc})\n",
    "\n",
    "pipeline_dict = ({'decision_tree': pipeline_dtc, \n",
    "                 'logistic_regression': pipeline_lr, \n",
    "                 'gradient_boosting': pipeline_gbc})\n",
    "\n",
    "parameter_list, pipeline_list = [], []\n",
    "for algorithm in algorithm_names:\n",
    "    parameter_list.append(parameter_dict[algorithm])\n",
    "    pipeline_list.append(pipeline_dict[algorithm])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 1==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file_stub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Training\n",
    "# Runtime warnings were because when selecting features, all the feature importances were\n",
    "# below the alpha value set\n",
    "predictive_model_results, best_parameters_list = [], []\n",
    "best_estimator_dict = {}\n",
    "for i in range(0,len(pipeline_list)):\n",
    "    print('Current algorithm: ', algorithm_names[i])\n",
    "    # Refit refits on whole dataset using the best parameters and allows you to call predict on the gridsearchcv instance\n",
    "    rcv = RandomizedSearchCV(pipeline_list[i], parameter_list[i], n_iter = rcv_n_iter, cv=cross_val, scoring = 'average_precision', verbose = 5, error_score=0.0, refit = True, random_state=3005)\n",
    "    results, best_parameters, best_estimator = grid_search_save_output(rcv, algorithm_names[i], rcv_n_iter, parameters, X_tr_resampled, y_tr_resampled, '{}/Models/model_output_{}_{}_{}.csv'.format(local_dir, file_stub, algorithm_names[i], levers))\n",
    "    predictive_model_results.append(results)\n",
    "    best_parameters_list.append(best_parameters)\n",
    "    best_estimator_dict[algorithm_names[i]] = best_estimator\n",
    "    \n",
    "with open(\"{}/Models/best_estimator_{}_{}_dict.pkl\".format(local_dir, file_stub, levers), \"wb\") as handle:\n",
    "    pickle.dump(best_estimator_dict, handle, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart here after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "file_list = glob.glob(\"{}/Models/model_output_{}_*_50_2.csv\".format(local_dir, file_stub))\n",
    "\n",
    "file_dict = {}\n",
    "for file_name in file_list:\n",
    "    file = pd.read_csv(file_name)\n",
    "    file_name = file_name.replace(\"{}/Models/model_output_\".format(local_dir), \"\")\n",
    "    file_name = file_name.replace(\".csv\", \"\")\n",
    "    file_dict[file_name] = file\n",
    "    \n",
    "print(file_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "model_output = pd.concat(file_dict.values(), axis = 0, ignore_index = True)\n",
    "\n",
    "# Find the model which finds the best worst case scenario\n",
    "# Mean - 2 * sd (i.e. if normally distributed the model which has the highest 95% lower bound of scores)\n",
    "if cv == 'ts':\n",
    "    n_splits = 3\n",
    "else:\n",
    "    n_splits = 5\n",
    "    \n",
    "model_dict = {'decision_tree': 'dtc',\n",
    "        'logistic_regression': 'lr',\n",
    "        'gradient_boosting': 'gbc'}\n",
    "\n",
    "# Identify best algorithm from the maximum LB\n",
    "max_min_test_score = np.argmax(model_output['mean_test_score'] - 1.96* model_output['std_test_score']/math.sqrt(n_splits)) \n",
    "best_algorithm = model_output.loc[max_min_test_score,'algorithm']\n",
    "print(\"Best algorithm: \", best_algorithm)\n",
    "print(\"Max min score\", model_output.loc[max_min_test_score, 'mean_test_score'])\n",
    "\n",
    "# For each algorithm find the max LB, max mean and accompanying SD\n",
    "# Then extract the parameters\n",
    "max_min_params_dict_all, max_min_mean_AP_dict, mean_AP_dict, sd_AP_dict = {}, {}, {}, {}\n",
    "for a in model_output['algorithm'].unique():\n",
    "    # Metrics\n",
    "    model_output_algorithm = model_output.loc[model_output['algorithm'] == a,]\n",
    "    model_output_algorithm.reset_index(drop= True, inplace = True)\n",
    "    max_min_mean_idx = np.argmax(model_output_algorithm['mean_test_score'] - 1.96* model_output_algorithm['std_test_score']/math.sqrt(n_splits))\n",
    "    max_min_mean = model_output_algorithm.loc[max_min_mean_idx, 'mean_test_score']\n",
    "    best_mean = model_output_algorithm['mean_test_score'].max()\n",
    "    max_min_mean_AP_dict[a] = round(max_min_mean, 4)\n",
    "    mean_AP_dict[a] = round(best_mean, 4)\n",
    "    sd_AP_dict[a] = round(model_output_algorithm.loc[model_output_algorithm['mean_test_score'] == best_mean, 'std_test_score'].values[0], 4)\n",
    "    \n",
    "    # Extract the parameters\n",
    "    param_cols = [col for col in model_output_algorithm.columns if 'param_{}'.format(model_dict[a]) in col]\n",
    "    #max_min_test_score = max_min_mean_AP_dict[a]\n",
    "    max_min_params_dict = dict(model_output_algorithm.loc[max_min_mean_idx,param_cols])\n",
    "    max_min_params_dict_keys = [k.replace('param_{}__'.format(model_dict[a]), '') for k in max_min_params_dict.keys()]\n",
    "    max_min_params_dict = dict(zip(max_min_params_dict_keys, max_min_params_dict.values()))\n",
    "    max_min_params_dict_all[a] = max_min_params_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load saved models\n",
    "filename = open(\"{}/Models/best_estimator_{}_{}_dict.pkl\".format(local_dir, file_stub, levers), \"rb\")\n",
    "best_estimator_dict = pickle.load(filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retraining model with max_min\n",
    "\n",
    "# Check whether it's the same as the maximum\n",
    "    # Set the parameters and re-fit\n",
    "    # Label as fitted_model\n",
    "\n",
    "best_estimator_dict_new = {}\n",
    "if max_min_mean_AP_dict['gradient_boosting'] != mean_AP_dict['gradient_boosting']:\n",
    "\n",
    "    # Gradient boosting\n",
    "    max_min_params_dict_all['gradient_boosting']['n_estimators'] = int(max_min_params_dict_all['gradient_boosting']['n_estimators']) # n_estimators has to be integer\n",
    "    gbc.set_params(**max_min_params_dict_all['gradient_boosting'])\n",
    "    pipeline_gbc = Pipeline([('preprocessor', preprocessor),\n",
    "                                ('gbc', gbc)])\n",
    "    print(\"Fitting GBC\")\n",
    "    pipeline_gbc.fit(X_tr_resampled, y_tr_resampled)\n",
    "    best_estimator_dict_new['gradient_boosting'] = pipeline_gbc     \n",
    "\n",
    "else:\n",
    "    best_estimator_dict_new['gradient_boosting'] = best_estimator_dict['gradient_boosting']\n",
    "\n",
    "# Decision tree\n",
    "if max_min_mean_AP_dict['decision_tree'] != mean_AP_dict['decision_tree']:\n",
    "    max_min_params_dict_all['decision_tree']['min_samples_split'] = int(max_min_params_dict_all['decision_tree']['min_samples_split']) # min_samples_split has to be integer   \n",
    "    dtc.set_params(**max_min_params_dict_all['decision_tree'])\n",
    "    pipeline_dtc = Pipeline([('preprocessor', preprocessor),\n",
    "                        ('dtc', dtc)])\n",
    "    print(\"Fitting DTC\")\n",
    "    pipeline_dtc.fit(X_tr_resampled, y_tr_resampled)\n",
    "    best_estimator_dict_new['decision_tree'] = pipeline_dtc\n",
    "\n",
    "else:\n",
    "    best_estimator_dict_new['decision_tree'] = best_estimator_dict['decision_tree']\n",
    "    \n",
    "# Logistic regression    \n",
    "if max_min_mean_AP_dict['logistic_regression'] != mean_AP_dict['logistic_regression']:  \n",
    "    lr.set_params(**max_min_params_dict_all['logistic_regression'])\n",
    "    pipeline_lr = (Pipeline([('preprocessor', preprocessor),\n",
    "                    ('scale',scaler), # regularisation requires features in same scale\n",
    "                    ('lr', lr)]))\n",
    "    print(\"Fitting LR\")\n",
    "    pipeline_lr.fit(X_tr_resampled, y_tr_resampled)\n",
    "    best_estimator_dict_new['logistic_regression'] = pipeline_lr\n",
    "    \n",
    "else:\n",
    "    best_estimator_dict_new['logistic_regression'] = best_estimator_dict['logistic_regression']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 1==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fitted max min model\n",
    "\n",
    "with open(\"{}/Models/best_estimator_{}_maxmin.pkl\".format(local_dir, file_stub, levers), \"wb\") as handle:\n",
    "    pickle.dump(best_estimator_dict_new[best_algorithm] , handle, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import final data again (because encrypted hard drive locks)\n",
    "import os\n",
    "hard_drive_dir = '/Volumes/diskAshur2/Final transfer out data and code to WWC Jan 2020/Data for model/Use'\n",
    "os.chdir(hard_drive_dir)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "X_tr_resampled = pd.read_csv(\"Final/X_train_{}_final.csv\".format(file_stub), index_col = 0)\n",
    "print(X_tr_resampled.shape)\n",
    "X_tr_resampled.reset_index(inplace = True, drop = True)\n",
    "print(X_tr_resampled.index)\n",
    "\n",
    "X_test_no_na = pd.read_csv(\"Final/X_test_{}_final.csv\".format(file_stub), index_col = 0)\n",
    "print(X_test_no_na.shape)\n",
    "X_test_no_na.reset_index(inplace = True, drop = True)\n",
    "print(X_test_no_na.index)\n",
    "\n",
    "\n",
    "# Drop key columns (after data is saved as still need the keys for merging in 7a_Combine_and_Split_Data and 8_Fairness)\n",
    "print(X_tr_resampled.shape)\n",
    "print(X_test_no_na.shape)\n",
    "\n",
    "key_cols = ['PSID',  'ReferralDatetime']\n",
    "X_tr_resampled.drop(columns = key_cols, inplace = True, errors = 'ignore')\n",
    "X_test_no_na.drop(columns = key_cols, inplace = True, errors = 'ignore')\n",
    "print(X_tr_resampled.shape)\n",
    "print(X_test_no_na.shape)\n",
    "\n",
    "X_tr_resampled = X_tr_resampled.select_dtypes(include = 'number') \n",
    "X_test_no_na = X_test_no_na.select_dtypes(include = 'number') \n",
    "print(X_tr_resampled.shape)\n",
    "print(X_test_no_na.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Summmary statistics for each dataset (after it's been transformed)\n",
    "# Just needs to be one algorithm - could be any because we're just using the preprocessing step\n",
    "feature_labels = pd.read_csv('{}/Graphs Analysis - All variables.csv'.format(summary_info))\n",
    "feature_labels = feature_labels.loc[feature_labels['LA'] == 2]\n",
    "\n",
    "X_tr_resampled_summary = round(X_tr_resampled.describe(),2)\n",
    "\n",
    "rename_cols_dict = dict(zip(feature_labels['Variable Name'], feature_labels['Name of variable in the report']))\n",
    "print(set(X_tr_resampled_summary.columns).difference(set(rename_cols_dict.keys())))\n",
    "#assert (len(set(X_tr_resampled_summary.columns).difference(set(rename_cols_dict.keys()))) == 0)\n",
    "\n",
    "X_tr_resampled_summary.rename(columns = rename_cols_dict, inplace = True)\n",
    "X_tr_resampled_summary.to_csv('{}/Sample Sizes/Summary statistics for training data {}.csv'.format(local_dir, file_stub))\n",
    "\n",
    "# Also report the number of features\n",
    "number_features_no_na = X_tr_resampled.shape[1]\n",
    "\n",
    "# Balance before and after sampling\n",
    "class_balance_after_resampling = pd.DataFrame(data = {'Class balance before resampling - training data': round(y_tr.value_counts(normalize=True),4),\n",
    "                     'Class balance after resampling - training data': round(y_tr_resampled.value_counts(normalize=True),4)}\n",
    "                    )\n",
    "class_balance_after_resampling.to_csv('{}/Sample Sizes/Class imbalance after resampling {}.csv'.format(local_dir, file_stub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## What's happening inside the cv folds?\n",
    "from statsmodels.api import OLS\n",
    "import analysis_functions\n",
    "\n",
    "sibling_fold_tr, sibling_fold_test, class_imb_tr, class_imb_test, ttest_pvalue_dict, ftest_pvalue_dict, ftest_pvalue_dict_no_prev = {}, {}, {}, {}, {}, {}, {}\n",
    "split_no = 0\n",
    "for train_idx, test_idx in cross_val.split(X_tr_resampled, y_tr_resampled):\n",
    "\n",
    "    split_no +=1\n",
    "    split = 'split ' + str(split_no)\n",
    "    \n",
    "    #train_idx_resampled = y_tr_no_na_resampled.index[train_idx]\n",
    "    #test_idx_resampled = y_tr_no_na_resampled.index[test_idx]\n",
    "\n",
    "    # Either the same child or the same sibling group\n",
    "    sibling_fold_tr[split] = siblings_tr_resampled[train_idx].value_counts().value_counts()\n",
    "    sibling_fold_test[split] = siblings_tr_resampled[test_idx].value_counts().value_counts() # siblings_tr because we're cross validating within the training dataset\n",
    "\n",
    "    # Class balance\n",
    "    class_imb_tr[split] = y_tr_resampled[train_idx].value_counts()\n",
    "    class_imb_test[split] = y_tr_resampled[test_idx].value_counts() # y_tr because we're cross validating within the training dataset\n",
    "\n",
    "    # Does test and training class balance look similar in cv?\n",
    "    ttest = stats.ttest_ind(y_tr_resampled[train_idx], y_tr_resampled[test_idx])\n",
    "    pvalue = round(ttest[1], 2)\n",
    "    ttest_pvalue_dict[split] = pvalue\n",
    "    \n",
    "    print(pvalue)\n",
    "\n",
    "    # Do test and training datasets look similar in cv?\n",
    "    joint_orth_test_data = X_tr_resampled.copy()\n",
    "    joint_orth_test_data['Train'] = np.where(joint_orth_test_data.index.isin(train_idx), 1, 0)\n",
    "    results = OLS(joint_orth_test_data['Train'], joint_orth_test_data.drop(columns = 'Train')).fit()\n",
    "    ftest_pvalue_dict[split] = round(results.f_pvalue,2)\n",
    "    \n",
    "    print(results.f_pvalue)\n",
    "    \n",
    "    # Do the test and training datasets look similar in cv excluding the previous columns?\n",
    "    count_cols = [col for col in joint_orth_test_data.columns if 'count' in col]\n",
    "    mean_cols = [col for col in joint_orth_test_data.columns if 'mean' in col]\n",
    "    previous_cols = count_cols + mean_cols\n",
    "    joint_orth_test_data_no_previous = joint_orth_test_data.drop(columns = previous_cols)\n",
    "    results_no_prev = OLS(joint_orth_test_data_no_previous['Train'], joint_orth_test_data_no_previous.drop(columns = 'Train')).fit()\n",
    "    ftest_pvalue_dict_no_prev[split] = round(results_no_prev.f_pvalue,2)    \n",
    "    \n",
    "    print(results_no_prev.f_pvalue)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as dataframes\n",
    "sibling_fold_tr_df = pd.DataFrame(sibling_fold_tr)\n",
    "sibling_fold_tr_df = sibling_fold_tr_df.add_prefix('Train ')\n",
    "sibling_fold_test_df = pd.DataFrame(sibling_fold_test)\n",
    "sibling_fold_test_df = sibling_fold_test_df.add_prefix('Test ')\n",
    "sibling_fold_df = pd.concat([sibling_fold_tr_df, sibling_fold_test_df], axis = 1)\n",
    "sibling_fold_df.to_csv('{}/Sample Sizes/Number of occurences of the same sibling group in cv folds {}.csv'.format(local_dir, file_stub))\n",
    "\n",
    "# Class imbalance\n",
    "class_imb_tr_df = pd.DataFrame(class_imb_tr)\n",
    "class_imb_tr_df = class_imb_tr_df.add_prefix('Train ')\n",
    "class_imb_test_df = pd.DataFrame(class_imb_test)\n",
    "class_imb_test_df = class_imb_test_df.add_prefix('Test ')\n",
    "class_imb_df = pd.concat([class_imb_tr_df, class_imb_test_df], axis = 1)\n",
    "class_imb_df.to_csv('{}/Sample Sizes/Class imbalance in cv folds {}.csv'.format(local_dir, file_stub))\n",
    "\n",
    "\n",
    "# Tests for whether the folds look different\n",
    "ttest_pvalue_dict_df = pd.DataFrame([ttest_pvalue_dict])\n",
    "ttest_pvalue_dict_df = ttest_pvalue_dict_df.add_prefix('T-test p-value class imbalance ')\n",
    "ftest_pvalue_dict_df = pd.DataFrame([ftest_pvalue_dict])\n",
    "ftest_pvalue_dict_df = ftest_pvalue_dict_df.add_prefix('F-test p-value for joint orthonality of features ')\n",
    "tests_pvalue_dict_df = pd.concat([ttest_pvalue_dict_df, ftest_pvalue_dict_df], axis=1)\n",
    "tests_pvalue_dict_df.to_csv('{}/Sample Sizes/T-test and F-test for similarity of cv folds {}.csv'.format(local_dir, file_stub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do folds look different without the test data too?\n",
    "ftest_pvalue_dict_no_prev_df = pd.DataFrame([ftest_pvalue_dict_no_prev])\n",
    "ftest_pvalue_dict_no_prev_df.to_csv('{}/Sample Sizes/F-test for similarity of cv folds on previous {}.csv'.format(local_dir, file_stub))\n",
    "ftest_pvalue_dict_no_prev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 1==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file_stub)\n",
    "print([col for col in X_tr.columns if 'AssessmentType' in col])\n",
    "print([col for col in X_test_no_na.columns if 'AssessmentType' in col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance to check for data leakage\n",
    "#https://eli5.readthedocs.io/en/latest/blackbox/permutation_importance.html\n",
    "\n",
    "# Works fine for the _all data too for LAs where we take the data offsite before the analysis\n",
    "# because the columns already have names rather than being created in the randomisedsearchcv process\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "import operator\n",
    "\n",
    "model_dict = {'decision_tree': 'dtc',\n",
    "            'logistic_regression': 'lr',\n",
    "            'gradient_boosting': 'gbc'}\n",
    "\n",
    "model_list = []\n",
    "for algorithm in algorithm_names:\n",
    "    model_list.append(model_dict[algorithm])\n",
    "\n",
    "\n",
    "feature_importance_dict_dict = {}\n",
    "    \n",
    "for model_name, model_short_name in model_dict.items():\n",
    "    # Fill na as permutation importance doesn't allow NAs\n",
    "    fitted_model = best_estimator_dict_new[model_name][model_short_name]\n",
    "    perm = PermutationImportance(fitted_model, cv = cross_val, scoring = 'average_precision').fit(X_tr_resampled, y_tr_resampled)\n",
    "    feature_importance_dict = dict(zip(X_tr_resampled.columns, perm.feature_importances_))\n",
    "    feature_importance_dict = sorted(feature_importance_dict.items(), key=operator.itemgetter(1))\n",
    "    feature_importance_dict_dict[model_name] = feature_importance_dict\n",
    "    \n",
    "    \n",
    "for key, value in feature_importance_dict_dict.items():\n",
    "    print('Algorithm: ', key)\n",
    "    print('High feature importance: ', [v for v in value if abs(v[1]) > 0.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_feature_imp_list = []\n",
    "for algorithm in feature_importance_dict_dict.keys():\n",
    "    df = pd.DataFrame(feature_importance_dict_dict[algorithm], columns = ['column_name', 'feature_importance'])\n",
    "    df['algorithm'] = algorithm\n",
    "    df_feature_imp_list.append(df)\n",
    "\n",
    "df_feature_imp = pd.concat(df_feature_imp_list, axis=0)\n",
    "df_feature_imp.to_csv('{}/Models/feature_importance_{}.csv'.format(local_dir, file_stub), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "df_feature_imp = pd.read_csv('{}/Models/feature_importance_{}.csv'.format(local_dir, file_stub))\n",
    "df_feature_imp.sort_values(by = 'feature_importance', ascending = False)[0:19]\n",
    "\n",
    "if cv == 'ts':\n",
    "    cv_for_graph = 'predicting the future'\n",
    "if cv == 'ss':\n",
    "    cv_for_graph = 'predicting contemporaneously'\n",
    "\n",
    "df_feature_imp['feature_importance'] = df_feature_imp['feature_importance'].abs()\n",
    "df_feature_imp.sort_values(by = 'feature_importance', ascending = False, inplace = True)\n",
    "df_feature_imp_best_algorithm = df_feature_imp[df_feature_imp['algorithm'] == best_algorithm] \n",
    "df_feature_imp_best_algorithm.reset_index(inplace = True, drop = True)\n",
    "df_feature_imp_best_algorithm = df_feature_imp_best_algorithm.loc[df_feature_imp_best_algorithm['feature_importance']>0,]\n",
    "feature_names = df_feature_imp_best_algorithm.loc[0:19,'column_name']\n",
    "feature_importances = round(df_feature_imp_best_algorithm.loc[0:19,'feature_importance'],4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename features\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "feature_labels = pd.read_csv('{}/Graphs Analysis - All variables.csv'.format(summary_info))\n",
    "feature_names_df = pd.DataFrame(feature_names).merge(feature_labels[['Variable Name', 'Name of variable in the report']], how = 'left', left_on = 'column_name', right_on = 'Variable Name')\n",
    "feature_names_df.drop_duplicates(subset = 'column_name', inplace = True)\n",
    "\n",
    "print(feature_names_df.loc[feature_names_df['Name of variable in the report'].isna(), 'column_name'].unique())\n",
    "assert feature_names_df['Name of variable in the report'].isna().sum() == 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try some automatic renaming\n",
    "if feature_names_df['Name of variable in the report'].isna().sum() != 0:\n",
    "\n",
    "    feature_names_df['Name of variable in the report_all'] = feature_names_df['Name of variable in the report'].fillna(feature_names_df['column_name'])\n",
    "    feature_names_list = [n.replace('previous_exc_current_sum', 'Total Number of Previous') for n in feature_names_df['Name of variable in the report_all']]\n",
    "    feature_names_list = [n.replace('previous_exc_current_mean', 'Average Number of Previous') for n in feature_names_list]\n",
    "\n",
    "    feature_names_list = [n.title() for n in feature_names_list]\n",
    "    feature_names_list = [n.replace('_', ': ') for n in feature_names_list]\n",
    "    feature_names_list = [n.replace('.', ' ') for n in feature_names_list]\n",
    "\n",
    "    word_list = ['date', 'time', 'source', 'code', 'start', 'social', 'work',\n",
    "                 'assessment', 'completion', 'days', 'referral', 'care', 'reason',\n",
    "                'legal', 'status', 'need', 'abuse', 'category', 'of', 'cp', 'length',\n",
    "                'close', 'contact']\n",
    "    for w in word_list:\n",
    "        feature_names_list = [re.sub('{}'.format(w), ' {}'.format(w), t) for t in feature_names_list]\n",
    "    feature_names_list = pd.Series(feature_names_list)\n",
    "    feature_names_list.shape\n",
    "else:\n",
    "    feature_names_df['Name of variable in the report_all'] = feature_names_df['Name of variable in the report']\n",
    "    feature_names_list = pd.Series(feature_names_df['Name of variable in the report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bring in source and base\n",
    "years_sample_sizes = pd.read_csv(\"{}/Years and Sample Sizes.csv\".format(summary_info))\n",
    "\n",
    "# Identify source and base\n",
    "dict_names = {'ss': 'learning from all cases',\n",
    "             'ts': 'learning just from earlier cases',\n",
    "             'str': 'structured data only',\n",
    "             'all': 'structured and text data'}\n",
    "\n",
    "outcome = years_sample_sizes.loc[(years_sample_sizes['Local authority'] == 'LA2') &\n",
    "                       (years_sample_sizes['Research question'] == rq) & \n",
    "                        (years_sample_sizes['Cross-Validation'] == cv),'Shortened outcome'].values[0]    \n",
    "\n",
    "years = years_sample_sizes.loc[(years_sample_sizes['Local authority'] == 'LA2') &\n",
    "                       (years_sample_sizes['Research question'] == rq) & \n",
    "                        (years_sample_sizes['Cross-Validation'] == cv),'Years'].values[0]\n",
    "\n",
    "sample_sizes = years_sample_sizes.loc[(years_sample_sizes['Local authority'] == 'LA2') &\n",
    "                       (years_sample_sizes['Research question'] == rq) &\n",
    "                       (years_sample_sizes['Cross-Validation'] == cv), 'Sample size'].values[0]\n",
    "\n",
    "txt_source = 'Prediction: {}'.format(outcome)\n",
    "print(txt_source)\n",
    "txt_model_desc = 'Model: {}, {}'.format(dict_names[cv], dict_names[data_type])\n",
    "print(txt_model_desc)\n",
    "txt_base = 'Data: {}, {}, N = {}'.format('Local authority 2', years, sample_sizes)\n",
    "print(txt_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ale import ale_plot\n",
    "fitted_model = best_estimator_dict_new[best_algorithm] \n",
    "\n",
    "columns_with_mt_5_values = X_tr_resampled.loc[:, X_tr_resampled.nunique() > 5].columns\n",
    "\n",
    "columns_for_ale = list(set(columns_with_mt_5_values).intersection(set(feature_names)))\n",
    "\n",
    "txt = [txt_source, txt_model_desc, txt_base]    \n",
    "\n",
    "for col in columns_for_ale:\n",
    "    col_label = feature_names_df.loc[feature_names_df['column_name'] == col, 'Name of variable in the report_all'].values[0]\n",
    "    print(col)\n",
    "    if '/' in col:\n",
    "        col_name = col.replace('/', '')\n",
    "    else:\n",
    "        col_name = col\n",
    "    ale_plot(fitted_model, X_tr_resampled, features = col, feature_name = col_label, outcome = outcome, txt = txt, file_name = '{}/Graphs/ALE_{}_{}.png'.format(local_dir, file_stub, col_name), monte_carlo=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert 1==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking other metrics - AUC p/r on test data\n",
    "from sklearn.metrics import accuracy_score, average_precision_score, roc_auc_score, confusion_matrix, precision_score, recall_score, fbeta_score\n",
    "\n",
    "scores_list = []\n",
    "for a in algorithm_names:\n",
    "    try:        \n",
    "        if data_type == 'str':\n",
    "            fitted_model = best_estimator_dict_new[a]\n",
    "            y_pred = fitted_model.predict(X_test_no_na)\n",
    "            y_pred_proba = best_estimator_dict_new[a].predict_proba(X_test_no_na)\n",
    "        else:\n",
    "            X_test_no_na = X_test_no_na[X_tr_resampled.columns] # columns have to be in the same order\n",
    "            X_test_no_na_transformed = best_estimator_dict_new[a]['preprocessor'].transform(X_test_no_na)\n",
    "            y_pred = best_estimator_dict_new[a][model_dict[a]].predict(X_test_no_na_transformed)\n",
    "            y_pred_proba = best_estimator_dict_new[a][model_dict[a]].predict_proba(X_test_no_na_transformed)\n",
    "        \n",
    "        scores = pd.Series({'Algorithm': a,\n",
    "                            'Proportion negative class value': round(y_test.value_counts(normalize=True)[0], 2),\n",
    "                            'Accuracy': round(accuracy_score(y_test, y_pred), 2),\n",
    "                            'Maximum lower bound (training)': round(max_min_mean_AP_dict[a],2), # ADDED_AUGUST\n",
    "                            'Mean average precision (training)': round(mean_AP_dict[a],2),\n",
    "                            'Std average precision (training)': round(sd_AP_dict[a],2),\n",
    "                            'Average precision': round(average_precision_score(y_test, y_pred_proba[:,1]), 2), # CORRECTED\n",
    "                            'AUC': round(roc_auc_score(y_test, y_pred_proba[:,1]), 2), \n",
    "                            'F score (beta = 0.1)': round(fbeta_score(y_test, y_pred, beta = 0.1), 2), \n",
    "                            'Precision': round(precision_score(y_test, y_pred), 2), # CORRECTED\n",
    "                            'Recall': round(recall_score(y_test, y_pred),2)})\n",
    "        scores_list.append(pd.DataFrame(scores))\n",
    "    except(KeyError):\n",
    "        pass\n",
    "\n",
    "scores_multiple = pd.concat(scores_list, axis = 1)\n",
    "print(\"Scores: \", scores_multiple)\n",
    "scores_multiple.to_csv('{}/Models/Scores/scores_{}.csv'.format(local_dir, file_stub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert 1==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "from inspect import signature\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random \n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from textwrap import wrap\n",
    "\n",
    "algorithm_names_full = {'decision_tree': 'Decision tree', \n",
    "                        'logistic_regression': 'Logistic regression', \n",
    "                        'gradient_boosting': 'Gradient boosting'}\n",
    "if cv == 'ts':\n",
    "    cv_for_graph = 'learning just from earlier cases'\n",
    "if cv == 'ss':\n",
    "    cv_for_graph = 'learning from all cases'\n",
    "\n",
    "if data_type == 'str':\n",
    "    data_desc = 'structured data only'    \n",
    "elif data_type == 'all':\n",
    "    data_desc = 'structured and text data' \n",
    "\n",
    "if cv == 'ss' and data_type == 'str':\n",
    "    model_letter = 'a'\n",
    "elif cv == 'ts' and data_type == 'str':\n",
    "    model_letter = 'b'   \n",
    "elif cv == 'ss' and data_type == 'all':\n",
    "    model_letter = 'c'\n",
    "elif cv == 'ts' and data_type == 'all':\n",
    "    model_letter = 'd'\n",
    "\n",
    "# Best estimator - test\n",
    "y_pred_proba = fitted_model.predict_proba(X_test_no_na)\n",
    "average_precision_test = round(average_precision_score(y_test, y_pred_proba[:,1]), 2)\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba[:,1])\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.step(recall, precision, color='#ff7057', alpha=0.2,\n",
    "         where='post')\n",
    "ax.fill_between(recall, precision, alpha=0.2, color='#ff7057', **step_kwargs)\n",
    "ax.tick_params(axis='both', which='both', length=0, colors='#4d4d51')\n",
    "\n",
    "plt.xlabel('Recall', fontname=\"Arial\", color='#4d4d51', fontsize=12)\n",
    "plt.ylabel('Precision', fontname=\"Arial\", color='#4d4d51', fontsize=12)\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(color='#d0dde1')  \n",
    "title = '\\n'.join(wrap('Curve plotting the decrease in precision and the corresponding increase in recall as the threshold for \"at risk\" cases decreases', 60))\n",
    "ax.set_title(title, fontname=\"Arial\", color = '#4d4d51', fontsize=12, loc='left')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "plt.suptitle('PRECISION RECALL CURVE', fontname=\"Arial\", color = '#ff7057', fontsize=12, x=0.31, y=1.05)\n",
    "\n",
    "plt.figtext(0, -0.05, txt_source, wrap=True, fontname=\"Arial\", color='#4d4d51', fontsize=12)\n",
    "plt.figtext(0, -0.1, txt_model_desc, wrap=True, fontname=\"Arial\", color='#4d4d51', fontsize=12)\n",
    "plt.figtext(0, -0.15, txt_base, wrap=True, fontname=\"Arial\", color='#4d4d51', fontsize=12)\n",
    "\n",
    "plt.savefig('{}/Graphs/Precision Recall {} ({}).png'.format(local_dir, file_stub, cv_for_graph), transparent=False, dpi=80, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of risky cases in top 10%\n",
    "fitted_model = best_estimator_dict_new[best_algorithm] \n",
    "y_pred_proba = fitted_model.predict_proba(X_test_no_na)\n",
    "# prob[1] is probability of the positive class\n",
    "y_pred_prob_1 = y_pred_proba[:,1] \n",
    "y_pred_prob_1 = pd.Series(y_pred_prob_1, index = y_test.index) # get index\n",
    "y_pred_prob_1 = y_pred_prob_1.sort_values(ascending = False) # from highest to lowest\n",
    "tenpc = int(len(y_pred_prob_1)*0.1) # 10% through the sample\n",
    "top10pc_pred_index = y_pred_prob_1[0:tenpc].index # indices of top 10%\n",
    "test_postive_class_index = y_test[y_test == 1].index # indices of test data = positive class\n",
    "# Of the positive class cases in the test data, how many are in the top 10% of prediction probabilities?\n",
    "pc_positive_class_top10_pred = len(set(top10pc_pred_index).intersection(test_postive_class_index)) / len(test_postive_class_index) * 100\n",
    "\n",
    "# % of safe cases in bottom 10%\n",
    "bottom10pc_pred_index = y_pred_prob_1[len(y_pred_prob_1) - tenpc:].index # indices of bottom 10%\n",
    "test_negative_class_index = y_test[y_test == 0].index # indices of test data = negative class\n",
    "# Of the negative class cases in the test data, how many are in the bottom 10% of prediction probabilities?\n",
    "pc_positive_class_bottom10_pred = len(set(bottom10pc_pred_index).intersection(test_negative_class_index)) / len(test_negative_class_index) * 100\n",
    "\n",
    "intuitive_metrics = pd.Series({\"% of risky cases in top 10%\": round(pc_positive_class_top10_pred, 0),\n",
    "                              \"% of safe cases in bottom 10%\": round(pc_positive_class_bottom10_pred, 0)})\n",
    "\n",
    "# Number of false negatives and false positives in 1000 cases\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = fitted_model.predict(X_test_no_na)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "intuitive_metrics = intuitive_metrics.append(pd.Series({\"Number of true positives in 1000 cases\": round(tp/len(y_test)*1000, 0), \n",
    "                            \"Number of true negatives in 1000 cases\": round(tn/len(y_test)*1000, 0),\n",
    "                          \"Number of false positives in 1000 cases\": round(fp/len(y_test)*1000, 0), \n",
    "                            \"Number of false negatives in 1000 cases\": round(fn/len(y_test)*1000, 0)}))\n",
    "print(intuitive_metrics)\n",
    "intuitive_metrics.to_csv('{}/Models/Scores/Intuitive metrics {}.csv'.format(local_dir, file_stub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density plots of predictions\n",
    "import seaborn as sns\n",
    "\n",
    "ax = sns.kdeplot(y_pred_proba[:,1][y_test == 0], label='Not at risk', color='#ff7057')\n",
    "sns.kdeplot(y_pred_proba[:,1][y_test == 1], label='At risk', color='#4d4d51')\n",
    "\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(color='#d0dde1')  \n",
    "\n",
    "title = '\\n'.join(wrap('Distribution of predicted probability of {}'.format(outcome), 60))\n",
    "ax.set_title(title, fontname=\"Arial\", color = '#4d4d51', fontsize=12, loc='left')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.tick_params(axis='both', which='both', length=0, colors='#4d4d51')\n",
    "\n",
    "plt.xlabel('Probability', fontname=\"Arial\", color='#4d4d51', fontsize=12)\n",
    "plt.ylabel('Density', fontname=\"Arial\", color='#4d4d51', fontsize=12)\n",
    "plt.xlim([0.0, 1.05])\n",
    "plt.suptitle('KERNEL DENSITY PLOT', fontname=\"Arial\", color = '#ff7057', fontsize=12, x=0.28, y=1.01)\n",
    "legend = plt.legend()\n",
    "plt.setp(legend.get_texts(), color='#4d4d51')\n",
    "\n",
    "plt.figtext(0, -0.05, txt_source, wrap=True, fontname=\"Arial\", color='#4d4d51', fontsize=12)\n",
    "plt.figtext(0, -0.1, txt_model_desc, wrap=True, fontname=\"Arial\", color='#4d4d51', fontsize=12)\n",
    "plt.figtext(0, -0.15, txt_base, wrap=True, fontname=\"Arial\", color='#4d4d51', fontsize=12)\n",
    "\n",
    "plt.savefig('{}/Graphs/Kernel Density Plot {}.png'.format(local_dir, file_stub), transparent=False, dpi=80, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "from textwrap import wrap\n",
    "\n",
    "def plot_learning_curve(estimator, title, subtitle, X, y, axes=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5), scoring = None):\n",
    "    \"\"\"\n",
    "    Generate 3 plots: the test and training learning curve, the training\n",
    "    samples vs fit times curve, the fit times vs score curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    axes : array of 3 axes, optional (default=None)\n",
    "        Axes to use for plotting the curves.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "\n",
    "          - None, to use the default 5-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, optional (default=None)\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the dtype is float, it is regarded as a\n",
    "        fraction of the maximum size of the training set (that is determined\n",
    "        by the selected validation method), i.e. it has to be within (0, 1].\n",
    "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
    "        Note that for classification the number of samples usually have to\n",
    "        be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "    if axes is None:\n",
    "        _, ax = plt.subplots()\n",
    "\n",
    "    ax.set_title(title, fontname=\"Arial\", color = '#4d4d51', fontsize=12, loc='left')\n",
    "    ax.set_xlabel(\"Number of observations\", fontname=\"Arial\", color = '#4d4d51', fontsize=12)\n",
    "    ax.set_ylabel(\"Average Precision\", fontname=\"Arial\", color = '#4d4d51', fontsize=12)\n",
    "\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
    "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
    "                       train_sizes=train_sizes,\n",
    "                       return_times=True, scoring=scoring)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    ax.yaxis.grid(color=\"#d0dde1\")\n",
    "    ax.xaxis.grid(False)\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.set_yticks(np.arange(0,1.2,0.2))\n",
    "    ax.tick_params(axis='both', which='both', length=0, colors='#4d4d51')\n",
    "    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                         color='#ff7057')\n",
    "    ax.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                         color='#4d4d51')\n",
    "    ax.plot(train_sizes, train_scores_mean, '^-', color='#ff7057',\n",
    "                 label=\"Training score\")\n",
    "    ax.plot(train_sizes, test_scores_mean, 'o-', color='#4d4d51',\n",
    "                 label=\"Cross-validation score\")\n",
    "    #ax.legend(loc=\"best\", color='#4d4d51')\n",
    "    \n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    plt.suptitle(subtitle, fontname=\"Arial\", color = '#ff7057', fontsize=12, x=0.25, y=1.03)\n",
    "\n",
    "    legend = plt.legend(loc=\"best\")\n",
    "    plt.setp(legend.get_texts(), color='#4d4d51')\n",
    "\n",
    "    plt.figtext(0, -0.05, txt_source, wrap=True, fontname=\"Arial\", color='#4d4d51', fontsize=12)\n",
    "    plt.figtext(0, -0.1, txt_model_desc, wrap=True, fontname=\"Arial\", color='#4d4d51', fontsize=12)\n",
    "    plt.figtext(0, -0.15, txt_base, wrap=True, fontname=\"Arial\", color='#4d4d51', fontsize=12)\n",
    "    \n",
    "    plt.savefig('{}/Graphs/Learning Curves {} {}.pdf'.format(local_dir, file_stub, levers), transparent=False, dpi=80, bbox_inches=\"tight\")\n",
    "    return train_sizes, train_scores_mean, test_scores_mean\n",
    "\n",
    "\n",
    "title = '\\n'.join(wrap('The effect of increasing the number of observations on model performance', 60))\n",
    "subtitle = 'LEARNING CURVE'\n",
    "\n",
    "# Cross validation with 100 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "fitted_model = best_estimator_dict_new[best_algorithm] \n",
    "learning_curve = plot_learning_curve(fitted_model, title, subtitle, X_tr_resampled, y_tr_resampled,\n",
    "                    cv=cross_val, n_jobs=1, scoring = 'average_precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export learning curve data\n",
    "learning_curve_data = pd.DataFrame()\n",
    "learning_curve_data['Number of observations'], learning_curve_data['Average precision - train'], learning_curve_data['Average precision - test'] = learning_curve[0], learning_curve[1], learning_curve[2]\n",
    "learning_curve_data[['Average precision - train','Average precision - test']] = learning_curve_data[['Average precision - train','Average precision - test']].round(2)\n",
    "learning_curve_data['model_id'] = file_stub\n",
    "learning_curve_data['LA'] = 'LA2'\n",
    "learning_curve_data.to_csv('{}/Sample Sizes/Learning curve data {}.csv'.format(local_dir, file_stub), index = False)\n",
    "learning_curve_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
