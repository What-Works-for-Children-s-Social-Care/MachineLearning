{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compares fairness metrics methodologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in list of model performance metrics\n",
    "import glob\n",
    "import pandas as pd\n",
    "file_list = glob.glob(\"*/*.csv\")\n",
    "\n",
    "file_list = [f for f in file_list if 'Fairness_metrics_comparing_subgroups' in f]\n",
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dict = {}\n",
    "for file_name in file_list:\n",
    "\n",
    "    file = pd.read_csv(file_name, index_col = 0) \n",
    "    file_name = file_name.replace(\"Fairness_metrics_comparing_subgroups_\", \"\")\n",
    "    file_name = file_name.replace(\".csv\", \"\")\n",
    "    LA, model_id = file_name.split('/')\n",
    "    file['LA'] = LA\n",
    "    file['model_id'] = model_id\n",
    "    # Subgroup was index\n",
    "    if (LA == 'LA2') or (LA == 'LA3'):\n",
    "        file.reset_index(inplace = True, drop = False)\n",
    "    file_dict[file_name] = file\n",
    "\n",
    "print(file_dict.keys())\n",
    "\n",
    "results = pd.concat(file_dict.values(), axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA3 = pd.read_csv('Output/Fairness comparison LA3.csv', index_col = 0)\n",
    "LA3.head()\n",
    "LA3['LA'] = 'LA3'\n",
    "LA3.drop(columns = ['value', 'Group'], inplace = True)\n",
    "\n",
    "results = results.merge(LA3, how = 'outer', \n",
    "                            on = ['LA', 'model_id', 'Subgroup 1', 'Subgroup 2',\n",
    "                            'Subgroup 1: Average precision score 95% CI (LL)',\n",
    "                           'Subgroup 1: Average precision score 95% CI (UL)',\n",
    "                           'Subgroup 2: Average precision score 95% CI (LL)',\n",
    "                           'Subgroup 2: Average precision score 95% CI (UL)',\n",
    "                           'Subgroups are significantly different according to a comparison of confidence intervals'])\n",
    "print(results.shape)\n",
    "print(LA3.shape)\n",
    "print(results.shape)\n",
    "# No duplicates by group\n",
    "results.drop_duplicates(subset = ['LA', 'model_id', 'Subgroup 1', 'Subgroup 2']).shape ==results.drop_duplicates().shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which are missing (see below)\n",
    "pd.crosstab(results['LA'], results['model_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA4 = pd.read_csv('Output/Fairness comparison LA4.csv', index_col = 0)\n",
    "LA4.head()\n",
    "LA4['LA'] = 'LA4'\n",
    "LA4.drop(columns = ['value', 'Group'], inplace = True)\n",
    "\n",
    "results = results.merge(LA4, how = 'outer', \n",
    "                            on = ['LA', 'model_id', 'Subgroup 1', 'Subgroup 2',\n",
    "                            'Subgroup 1: Average precision score 95% CI (LL)',\n",
    "                           'Subgroup 1: Average precision score 95% CI (UL)',\n",
    "                           'Subgroup 2: Average precision score 95% CI (LL)',\n",
    "                           'Subgroup 2: Average precision score 95% CI (UL)',\n",
    "                           'Subgroups are significantly different according to a comparison of confidence intervals'])\n",
    "print(results.shape)\n",
    "print(LA4.shape)\n",
    "print(results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which are missing (see below)\n",
    "pd.crosstab(results['LA'], results['model_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differing numbers of observations comes from some models have missing disabilities and missing ages\n",
    "pd.crosstab(results.loc[results['LA'] == 'LA3','Subgroup 1'], results.loc[results['LA'] == 'LA3','model_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(results.loc[results['LA'] == 'LA3','Subgroup 2'], results.loc[results['LA'] == 'LA3','model_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "results['Subgroup 1'] = np.where((results['Subgroup 1']=='Ethnicity Not Known') |\n",
    "                                    (results['Subgroup 1']=='Declined / Missing') ,\n",
    "                                    'Ethnicity Not Known', results['Subgroup 1'])\n",
    "\n",
    "results['Subgroup 1'] = np.where((results['Subgroup 1']=='Black / British Black') |\n",
    "                                    (results['Subgroup 1']=='Black / African / Caribbean /Black British') |\n",
    "                                    (results['Subgroup 1']=='Black/Black British'),\n",
    "                                    'Black / African / Caribbean / Black British', results['Subgroup 1'])\n",
    "\n",
    "results['Subgroup 1'] = np.where((results['Subgroup 1']=='Unknown, Unborn or Indeterminate') |\n",
    "                                     (results['Subgroup 1']=='Unborn'),\n",
    "                                    'Unknown, Unborn or Indeterminate', results['Subgroup 1'])\n",
    "\n",
    "results['Subgroup 1'] = np.where((results['Subgroup 1']=='Asian British') |\n",
    "                                     (results['Subgroup 1']=='Asian/Asian British'),\n",
    "                                    'Asian / Asian British', results['Subgroup 1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "results['Subgroup 2'] = np.where((results['Subgroup 2']=='Ethnicity Not Known') |\n",
    "                                    (results['Subgroup 2']=='Declined / Missing') ,\n",
    "                                    'Ethnicity Not Known', results['Subgroup 2'])\n",
    "\n",
    "results['Subgroup 2'] = np.where((results['Subgroup 2']=='Black / British Black') |\n",
    "                                    (results['Subgroup 2']=='Black / African / Caribbean /Black British') |\n",
    "                                    (results['Subgroup 2']=='Black/Black British'),\n",
    "                                    'Black / African / Caribbean / Black British', results['Subgroup 2'])\n",
    "\n",
    "results['Subgroup 2'] = np.where((results['Subgroup 2']=='Unknown, Unborn or Indeterminate') |\n",
    "                                     (results['Subgroup 2']=='Unborn'),\n",
    "                                    'Unknown, Unborn or Indeterminate', results['Subgroup 2'])\n",
    "\n",
    "results['Subgroup 2'] = np.where((results['Subgroup 2']=='Asian British') |\n",
    "                                     (results['Subgroup 2']=='Asian/Asian British'),\n",
    "                                    'Asian / Asian British', results['Subgroup 2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some are NA because we merged in the LA3 and LA4 gender data which didn't have these values\n",
    "\n",
    "results.loc[results[\"P-value lower than the significance threshold according to Hochberg's step up procedure\"].notna(),\n",
    "           \"P-value lower than the significance threshold according to Hochberg's step up procedure\"].value_counts(normalize=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['Subgroups are significantly different according to a comparison of confidence intervals'].value_counts(normalize=True, dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['Subgroups are significantly different according to a comparison of confidence intervals'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['Subgroups are significantly different according to a comparison of confidence intervals'].replace({True: 1, False:0}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroup1 = results.groupby('Subgroup 1')['Subgroups are significantly different according to a comparison of confidence intervals'].mean()\n",
    "subgroup2 = results.groupby('Subgroup 2')['Subgroups are significantly different according to a comparison of confidence intervals'].mean()\n",
    "results_grouped_subgroup = pd.concat([subgroup1,subgroup2], axis = 1)\n",
    "results_grouped_subgroup.columns = ['Sig and Subgroup 1', 'Sig and Subgroup 2']\n",
    "results_grouped_subgroup.fillna(0, inplace = True)\n",
    "results_grouped_subgroup['Proportion sig'] = round((results_grouped_subgroup['Sig and Subgroup 1'] +\n",
    "                                              results_grouped_subgroup['Sig and Subgroup 2']) * 100, 0)\n",
    "                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_grouped_subgroup['Characteristic'] = results_grouped_subgroup.index\n",
    "results_grouped_subgroup['Characteristic'] = pd.Categorical(results_grouped_subgroup['Characteristic'],\n",
    "                                                 categories = ['Under 1 Year', '1-4 Years', '5-9 Years', '10-15 Years',\n",
    "                                   '16+ Years', 'Missing age', 'Female', 'Male',\n",
    "                                   'Unknown, Unborn or Indeterminate', 'Disabled', 'Not Disabled',\n",
    "                                   'Missing Disability', 'Asian / Asian British',\n",
    "                                   'Black / African / Caribbean / Black British', 'Mixed Ethnicity',\n",
    "                                   'Other Ethnicity', 'Ethnicity Not Known', 'White'], ordered = True)\n",
    "results_grouped_subgroup.sort_values(by = 'Characteristic', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_grouped_subgroup = results_grouped_subgroup[['Characteristic', 'Proportion sig']]\n",
    "results_grouped_subgroup.reset_index(drop = True, inplace = True)\n",
    "results_grouped_subgroup.to_csv('Output/Subgroups which are significantly different by CI overlapping.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data for visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_for_model_summary = results[['LA', 'Subgroup 1', 'Subgroup 1: Average precision score 95% CI (UL)', 'Subgroup 2',\n",
    "       'Subgroup 2: Average precision score 95% CI (LL)',\n",
    "       'Subgroup 2: Average precision score 95% CI (UL)',\n",
    "       'Subgroups are significantly different according to a comparison of confidence intervals',\n",
    "       'model_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_for_model_summary['Category'] = np.where(results_for_model_summary['Subgroup 1'].isin(['1-4 Years', '5-9 Years', '10-15 Years', '16+ Years', 'Missing age']),'Age',\n",
    "                                                np.where(results_for_model_summary['Subgroup 1'].isin(['Male', 'Unknown, Unborn or Indeterminate']),'Gender',\n",
    "                                                  np.where(results_for_model_summary['Subgroup 1'].isin(['Missing Disability','Not Disabled']), 'Disability',\n",
    "                                                  np.where(results_for_model_summary['Subgroup 1'].isin(['Black / African / Caribbean / Black British','Ethnicity Not Known', 'Mixed Ethnicity', 'Other Ethnicity','White']), 'Ethnicity',np.nan))))\n",
    "                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(results_for_model_summary['Category'], results_for_model_summary['Subgroup 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(results_for_model_summary['Category'], results_for_model_summary['Subgroup 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_for_model_summary_by_model_by_group = results_for_model_summary.groupby(['LA', 'model_id', 'Category'])['Subgroups are significantly different according to a comparison of confidence intervals'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_for_model_summary_by_model_by_group = results_for_model_summary_by_model_by_group.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(results_for_model_summary_by_model_by_group['LA'], results_for_model_summary_by_model_by_group['model_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_for_model_summary_by_model_by_group.loc[(results_for_model_summary_by_model_by_group['LA'] == 'LA4') &\n",
    "                                               (results_for_model_summary_by_model_by_group['model_id'] == 'rq2_ts_all'),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = results_for_model_summary_by_model_by_group['model_id'].str.split('_', expand = True)\n",
    "results_for_model_summary_by_model_by_group['rq'] = model_id[0]\n",
    "results_for_model_summary_by_model_by_group['Validation'] = model_id[1]\n",
    "results_for_model_summary_by_model_by_group['Data included'] = model_id[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get name\n",
    "question_names = pd.read_csv(\"Years and Sample Sizes.csv\")\n",
    "question_names['Prediction Number'] = np.where((question_names['Local authority'] == 'LA1')\n",
    "                                               & (question_names['Research question'] == 'rq1'),1,\n",
    "                                        np.where((question_names['Local authority'] == 'LA1')\n",
    "                                               & (question_names['Research question'] == 'rq2'), 2,\n",
    "                                        np.where((question_names['Local authority'] == 'LA2')\n",
    "                                               & (question_names['Research question'] == 'rq1'),3,\n",
    "                                        np.where((question_names['Local authority'] == 'LA2')\n",
    "                                               & (question_names['Research question'] == 'rq2'), 4, \n",
    "                                        np.where((question_names['Local authority'] == 'LA3')\n",
    "                                               & (question_names['Research question'] == 'rq1'),5,\n",
    "                                        np.where((question_names['Local authority'] == 'LA3')\n",
    "                                               & (question_names['Research question'] == 'rq2'), 6,\n",
    "                                         np.where((question_names['Local authority'] == 'LA4')\n",
    "                                               & (question_names['Research question'] == 'rq1'),7, 8)))))))  \n",
    "final_fairness = results_for_model_summary_by_model_by_group.merge(question_names[['Local authority', 'Research question', 'Shortened outcome', 'Prediction Number']], \n",
    "                                                  how = 'left', left_on = ['LA', 'rq'], right_on = ['Local authority', 'Research question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_fairness = final_fairness[['Local authority', 'Prediction Number', 'Shortened outcome', 'Validation', 'Data included', 'Category',                                  \n",
    "                                 'Subgroups are significantly different according to a comparison of confidence intervals']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_fairness['Validation'] = final_fairness['Validation'].replace({'ss': 'Learned from all cases', 'ts': 'Learned only from earlier cases'})\n",
    "final_fairness['Data included'] = final_fairness['Data included'].replace({'str': 'Structured data only', 'all': 'Includes text data'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_fairness.groupby(['Shortened outcome', 'Validation', 'Data included', 'Category'])['Subgroups are significantly different according to a comparison of confidence intervals'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_fairness.shape)\n",
    "final_fairness.drop_duplicates(inplace = True)\n",
    "final_fairness.dropna(axis = 0, subset = ['Category'], inplace = True)\n",
    "print(final_fairness.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_fairness.to_csv('Bias metrics.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
