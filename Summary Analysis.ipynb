{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "best_algorithms = pd.read_csv('Best algorithms.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in maxmin for LA1 to match the others\n",
    "import glob\n",
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "LA1_model_output_list = glob.glob(\"/Documents/LA1 Results July 2020/Models/model_output_*.csv\")\n",
    "\n",
    "LA1_maxmin_df = pd.DataFrame(data = {'model_id': ['rq1_ts_all', 'rq1_ss_str', 'rq2_ss_all', 'rq2_ts_str',\n",
    "       'rq1_ts_str', 'rq1_ss_all', 'rq2_ss_str', 'rq2_ts_all'],\n",
    "                                    'Algorithm': ['Maximum lower bound (training)'] * 8,\n",
    "                                    'LA': ['LA1'] * 8})\n",
    "\n",
    "LA1_model_output_dict = {}\n",
    "for file_name in LA1_model_output_list:\n",
    "    file = pd.read_csv(file_name)\n",
    "    file_name = file_name.replace('/Documents/LA1 Results July 2020/Models/model_output_', '')\n",
    "    file_name = file_name.replace('.csv', '')\n",
    "    if 'ts' in file_name:\n",
    "        n_splits = 3\n",
    "    else:\n",
    "        n_splits = 5\n",
    "    # \n",
    "    file.reset_index(drop= True, inplace = True)\n",
    "    max_min_mean_idx = np.argmax(file['mean_test_score'] - 1.96* file['std_test_score']/math.sqrt(n_splits))\n",
    "    max_min_mean = round(file.loc[max_min_mean_idx, 'mean_test_score'], 2)\n",
    "    model_id = re.findall('rq[12]_[a-z]{2}_[a-z]{2,3}', file_name)[0]\n",
    "    algorithm = file_name.split(model_id+'_')[1]\n",
    "    LA1_maxmin_df.at[LA1_maxmin_df['model_id'] == model_id, algorithm] = max_min_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in list of model performance metrics\n",
    "import glob\n",
    "import pandas as pd\n",
    "file_list_excel = glob.glob(\"*/*.xlsx\")\n",
    "file_list_csv = glob.glob(\"*/*.csv\")\n",
    "file_list = file_list_excel + file_list_csv\n",
    "\n",
    "file_list = [f for f in file_list if 'scores' in f]\n",
    "file_list = [f for f in file_list if 'Best algorithms' not in f]\n",
    "\n",
    "file_dict = {}\n",
    "for file_name in file_list:\n",
    "    if 'LA1' in file_name:\n",
    "        # LA1\n",
    "        file = pd.read_excel(file_name, header = 1)\n",
    "         \n",
    "    else:\n",
    "        # All other LAs' scores are in csv form\n",
    "        file = pd.read_csv(file_name, skiprows = 1)\n",
    "    file_name = file_name.replace(\"../../Summary information FINAL/\", \"\")        \n",
    "    if 'multiple' in file_name:\n",
    "        file_name = file_name.replace(\"scores_multiple_\", \"\")    \n",
    "\n",
    "    else:\n",
    "        file_name = file_name.replace(\"scores_\", \"\")\n",
    "    file_name = file_name.replace(\".xlsx\", \"\")\n",
    "    file_name = file_name.replace(\".csv\", \"\")\n",
    "    LA, model_id = file_name.split('/')\n",
    "    file['LA'] = LA\n",
    "    file['model_id'] = model_id\n",
    "    file_dict[file_name] = file\n",
    "\n",
    "\n",
    "print(file_dict.keys())\n",
    "\n",
    "results = pd.concat(file_dict.values(), axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA1_maxmin_df.to_csv('LA1/LA1 maxmimum lower bound.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc[results['LA'] == 'LA4','Algorithm'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge in LA1 maxmin\n",
    "print(results.shape)\n",
    "results = pd.concat([results, LA1_maxmin_df], axis = 0)\n",
    "print(results.shape)\n",
    "\n",
    "'''\n",
    "results.loc[(results['LA'] == 'LA1') &\n",
    "    (results['Algorithm'].isin(['Maximum lower bound (training)', 'Mean average precision (training)'])),].sort_values(by = 'model_id')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename Algorithm\n",
    "results = results.rename(columns = {'Algorithm': 'Metrics'})\n",
    "\n",
    "# Separate out model_id to allow for easier subsetting of the data\n",
    "model_id = results['model_id'].str.split('_', expand = True)\n",
    "results['rq'], results['cv'], results['data_type'] = model_id[0], model_id[1], model_id[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in which was the best algorithm\n",
    "import numpy as np\n",
    "results = results.merge(best_algorithms, how='left', left_on = ['LA', 'model_id'], right_on=['LA', 'Model'])\n",
    "results.drop(columns = 'model_id', inplace = True)\n",
    "\n",
    "# Identify the value of the metric for the algorithm we chose\n",
    "results['Best algorithm model performance'] = np.where(results['Best algorithm'] == 'decision_tree',\n",
    "                                                        results['decision_tree'], \n",
    "                                                       np.where(results['Best algorithm'] == 'logistic_regression',\n",
    "                                                       results['logistic_regression'], results['gradient_boosting']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe comparing cv results\n",
    "results_cv = pd.pivot_table(results, index=['Metrics', 'LA', 'rq', 'data_type'], columns = 'cv', values='Best algorithm model performance')\n",
    "results_cv.reset_index(inplace = True)\n",
    "results_cv['Validation technique with best model performance'] = np.where((results_cv['ss']>results_cv['ts']) &\n",
    "                                                                          (results_cv['Metrics']!='Proportion negative class value') &\n",
    "                                                           (results_cv['Metrics']!='Std average precision (training)'),\n",
    "                                                       'Learning from all cases', \n",
    "                                                       np.where((results_cv['ss']<results_cv['ts']) &\n",
    "                                                                          (results_cv['Metrics']!='Proportion negative class value') &\n",
    "                                                           (results_cv['Metrics']!='Std average precision (training)'),\n",
    "                                                             'Learning only from earlier cases',   \n",
    "                                                        np.where((results_cv['ss']==results_cv['ts']) &\n",
    "                                                                          (results_cv['Metrics']!='Proportion negative class value') &\n",
    "                                                           (results_cv['Metrics']!='Std average precision (training)'),\n",
    "                                                         'Same performance', np.nan)))\n",
    "\n",
    "results_cv['Validation technique with best model performance'] = np.where((results_cv['ss']<results_cv['ts']) &\n",
    "                                                           (results_cv['Metrics']=='Std average precision (training)'),\n",
    "                                                             'Learning from all cases',\n",
    "                                                                    np.where((results_cv['ss']>results_cv['ts']) &\n",
    "                                                           (results_cv['Metrics']=='Std average precision (training)'),\n",
    "                                                             'Learning only from earlier cases', results_cv['Validation technique with best model performance']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better than threshold?\n",
    "results_cv['Greater than \"success\" threshold (0.65) - learning from all cases'] = np.where((results_cv['ss'] >= 0.65) &\n",
    "                                                        (results_cv['Metrics']!='Proportion negative class value') &\n",
    "                                                           (results_cv['Metrics']!='Std average precision (training)'),\n",
    "                                                                 'Exceeds success threshold',\n",
    "                                                                 np.where((results_cv['ss'] < 0.65) &\n",
    "                                                        (results_cv['Metrics']!='Proportion negative class value') &\n",
    "                                                           (results_cv['Metrics']!='Std average precision (training)'),\n",
    "                                                                         'Does not exceeds success threshold' ,np.nan))\n",
    "                                                                                                   \n",
    "results_cv['Greater than \"success\" threshold (0.65) - Learning only from earlier cases'] = np.where((results_cv['ts'] >= 0.65) &\n",
    "                                                        (results_cv['Metrics']!='Proportion negative class value') &\n",
    "                                                           (results_cv['Metrics']!='Std average precision (training)'),\n",
    "                                                                 'Exceeds success threshold',\n",
    "                                                                 np.where((results_cv['ts'] < 0.65) &\n",
    "                                                        (results_cv['Metrics']!='Proportion negative class value') &\n",
    "                                                           (results_cv['Metrics']!='Std average precision (training)'),\n",
    "                                                                         'Does not exceeds success threshold' ,np.nan))                                                                 \n",
    "                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relabel\n",
    "results_cv.rename(columns = {'LA': 'Local authority',\n",
    "                        'rq': 'Prediction',\n",
    "                        'ss': 'Learning from all cases',\n",
    "                        'ts': 'Learning only from earlier cases',\n",
    "                         'data_type': 'Data included'},\n",
    "              inplace = True)\n",
    "\n",
    "# Renaming before saving\n",
    "print(results_cv['Data included'].value_counts())\n",
    "results_cv['Data included'] = np.where(results_cv['Data included']=='str',\n",
    "                                  'Structured data only',\n",
    "                                   'Structured and text data')\n",
    "print(results_cv['Data included'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming predictions according to the LA number and the rq number \n",
    "results_cv['Prediction Number'] = np.where(((results_cv['Local authority']=='LA1') & \n",
    "                                  (results_cv['Prediction'] == 'rq1')),\n",
    "                                1,\n",
    "                                np.where(((results_cv['Local authority']=='LA1') & \n",
    "                                  (results_cv['Prediction'] == 'rq2')),\n",
    "                                2,\n",
    "                                np.where(((results_cv['Local authority']=='LA2') & \n",
    "                                  (results_cv['Prediction'] == 'rq1')),\n",
    "                                3,\n",
    "                                np.where(((results_cv['Local authority']=='LA2') & \n",
    "                                  (results_cv['Prediction'] == 'rq2')),\n",
    "                                4,\n",
    "                                np.where(((results_cv['Local authority']=='LA3') & \n",
    "                                  (results_cv['Prediction'] == 'rq1')),\n",
    "                                5,\n",
    "                                np.where(((results_cv['Local authority']=='LA3') & \n",
    "                                  (results_cv['Prediction'] == 'rq2')),\n",
    "                                6,\n",
    "                                np.where(((results_cv['Local authority']=='LA4') & \n",
    "                                  (results_cv['Prediction'] == 'rq1')),\n",
    "                                7,8)))))))\n",
    "\n",
    "results_cv.drop(columns = ['Prediction'], inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe comparing data_type results\n",
    "results_data_type = pd.pivot_table(results, index=['Metrics', 'LA', 'rq', 'cv'], columns = 'data_type', values='Best algorithm model performance')\n",
    "results_data_type.reset_index(inplace = True)\n",
    "\n",
    "results_data_type['Data included with best model performance'] = np.where((results_data_type['all']>results_data_type['str']) &\n",
    "                                                                          (results_data_type['Metrics']!='Proportion negative class value') &\n",
    "                                                           (results_data_type['Metrics']!='Std average precision (training)'),\n",
    "                                                       'Structured and text data', \n",
    "                                                       np.where((results_data_type['all']<results_data_type['str']) &\n",
    "                                                                          (results_data_type['Metrics']!='Proportion negative class value') &\n",
    "                                                           (results_data_type['Metrics']!='Std average precision (training)'),\n",
    "                                                             'Structured data only',   \n",
    "                                                        np.where((results_data_type['all']==results_data_type['str']) &\n",
    "                                                                          (results_data_type['Metrics']!='Proportion negative class value') &\n",
    "                                                           (results_data_type['Metrics']!='Std average precision (training)'),\n",
    "                                                         'Same performance', np.nan)))\n",
    "\n",
    "results_data_type['Data included with best model performance'] = np.where((results_data_type['all']<results_data_type['str']) &\n",
    "                                                           (results_data_type['Metrics']=='Std average precision (training)'),\n",
    "                                                             'Structured and text data',\n",
    "                                                                    np.where((results_data_type['all']>results_data_type['str']) &\n",
    "                                                           (results_data_type['Metrics']=='Std average precision (training)'),\n",
    "                                                             'Structured data only', results_data_type['Data included with best model performance']))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better than threshold?\n",
    "results_data_type['Greater than \"success\" threshold (0.65) - structured data only'] = np.where((results_data_type['str'] >= 0.65) &\n",
    "                                                        (results_data_type['Metrics']!='Proportion negative class value') &\n",
    "                                                           (results_data_type['Metrics']!='Std average precision (training)'),\n",
    "                                                                 'Exceeds success threshold',\n",
    "                                                                 np.where((results_data_type['str'] < 0.65) &\n",
    "                                                        (results_data_type['Metrics']!='Proportion negative class value') &\n",
    "                                                           (results_data_type['Metrics']!='Std average precision (training)'),\n",
    "                                                                         'Does not exceeds success threshold' ,np.nan))\n",
    "                                                                                                   \n",
    "results_data_type['Greater than \"success\" threshold (0.65) - structured and text data'] = np.where((results_data_type['all'] >= 0.65) &\n",
    "                                                        (results_data_type['Metrics']!='Proportion negative class value') &\n",
    "                                                           (results_data_type['Metrics']!='Std average precision (training)'),\n",
    "                                                                 'Exceeds success threshold',\n",
    "                                                                 np.where((results_data_type['all'] < 0.65) &\n",
    "                                                        (results_data_type['Metrics']!='Proportion negative class value') &\n",
    "                                                           (results_data_type['Metrics']!='Std average precision (training)'),\n",
    "                                                                         'Does not exceeds success threshold' ,np.nan))    \n",
    "\n",
    "                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessary columns and remaning columns\n",
    "results_data_type.rename(columns = {'LA': 'Local authority',\n",
    "                        'rq': 'Prediction', 'cv': 'Cross Validation Method',\n",
    "                         'all': 'Structured and text data',\n",
    "                        'str': 'Structured data only'},\n",
    "              inplace = True)\n",
    "\n",
    "# Renaming before saving\n",
    "print(results_data_type['Cross Validation Method'].value_counts())\n",
    "results_data_type['Cross Validation Method'] = np.where(results_data_type['Cross Validation Method']=='ss',\n",
    "                                             'Learning from all cases',\n",
    "                                             'Learning only from earlier cases')\n",
    "print(results_data_type['Cross Validation Method'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming predictions according to the LA number and the rq number \n",
    "results_data_type['Prediction Number'] = np.where(((results_data_type['Local authority']=='LA1') & \n",
    "                                  (results_data_type['Prediction'] == 'rq1')),\n",
    "                                1,\n",
    "                                np.where(((results_data_type['Local authority']=='LA1') & \n",
    "                                  (results_data_type['Prediction'] == 'rq2')),\n",
    "                                2,\n",
    "                                np.where(((results_data_type['Local authority']=='LA2') & \n",
    "                                  (results_data_type['Prediction'] == 'rq1')),\n",
    "                                3,\n",
    "                                np.where(((results_data_type['Local authority']=='LA2') & \n",
    "                                  (results_data_type['Prediction'] == 'rq2')),\n",
    "                                4,\n",
    "                                np.where(((results_data_type['Local authority']=='LA3') & \n",
    "                                  (results_data_type['Prediction'] == 'rq1')),\n",
    "                                5,\n",
    "                                np.where(((results_data_type['Local authority']=='LA3') & \n",
    "                                  (results_data_type['Prediction'] == 'rq2')),\n",
    "                                6,\n",
    "                                np.where(((results_data_type['Local authority']=='LA4') & \n",
    "                                  (results_data_type['Prediction'] == 'rq1')),\n",
    "                                7,8)))))))\n",
    "\n",
    "results_data_type.drop(columns = ['Prediction'], inplace = True, errors='ignore')\n",
    "results_data_type = results_data_type[['Metrics', 'Local authority', 'Cross Validation Method',\n",
    "        'Structured data only', 'Structured and text data',\n",
    "       'Data included with best model performance', \n",
    "        'Greater than \"success\" threshold (0.65) - structured data only',\n",
    "       'Greater than \"success\" threshold (0.65) - structured and text data',\n",
    "        'Prediction Number']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by prediction and save into the right files\n",
    "# Save average precision and mean and sd on training\n",
    "\n",
    "for pred in results_cv['Prediction Number'].unique():\n",
    "    LA = results_cv.loc[results_cv['Prediction Number'] == pred, 'Local authority'].reset_index(drop=True)[0]\n",
    "    \n",
    "    # Cross validation\n",
    "    results_cv_pred = results_cv.loc[\n",
    "                    ((results_cv['Metrics'] == 'Proportion negative class value') & \n",
    "                     (results_cv['Prediction Number'] == pred)) |\n",
    "                    ((results_cv['Metrics'] == 'Accuracy') & \n",
    "                    (results_cv['Prediction Number'] == pred)) |\n",
    "                    ((results_cv['Metrics'] == 'Average precision') & \n",
    "                   (results_cv['Prediction Number'] == pred)) |\n",
    "                   ((results_cv['Metrics'] == 'AUC') &\n",
    "                   (results_cv['Prediction Number'] == pred)) |\n",
    "                   ((results_cv['Metrics'] == 'Mean average precision (training)') &\n",
    "                   (results_cv['Prediction Number'] == pred)) |\n",
    "                ((results_cv['Metrics'] == 'Std average precision (training)') &\n",
    "                   (results_cv['Prediction Number'] == pred)),]\n",
    "    \n",
    "    results_cv_pred.drop(columns = ['Local authority', 'Prediction Number'], inplace = True)\n",
    "    results_cv_pred['Metrics'] = pd.Categorical(results_cv_pred['Metrics'], \n",
    "                                            categories = ['Proportion negative class value',\n",
    "                                                          'Accuracy',\n",
    "                                                        'Average precision', \n",
    "                                                          'Mean average precision (training)',\n",
    "                                                          'Std average precision (training)',\n",
    "                                                            'AUC'], \n",
    "                                            ordered = True)\n",
    "    results_cv_pred.sort_values(by = ['Metrics', 'Data included'], inplace = True)\n",
    "\n",
    "    results_cv_pred.to_csv('{}/Model summary cv {}.csv'.format(LA, pred), index = False)\n",
    "    \n",
    "    # Data type\n",
    "    results_data_type_pred = results_data_type.loc[\n",
    "                    ((results_data_type['Metrics'] == 'Proportion negative class value') & \n",
    "                    (results_data_type['Prediction Number'] == pred)) |\n",
    "                    ((results_data_type['Metrics'] == 'Accuracy') & \n",
    "                   (results_data_type['Prediction Number'] == pred)) |\n",
    "                    ((results_data_type['Metrics'] == 'Average precision') & \n",
    "                   (results_data_type['Prediction Number'] == pred)) |\n",
    "                    ((results_data_type['Metrics'] == 'AUC') &\n",
    "                    (results_data_type['Prediction Number'] == pred)) |\n",
    "                   ((results_data_type['Metrics'] == 'Mean average precision (training)') &\n",
    "                   (results_data_type['Prediction Number'] == pred)) |\n",
    "                    ((results_data_type['Metrics'] == 'Std average precision (training)') &\n",
    "                   (results_data_type['Prediction Number'] == pred)),]\n",
    "    \n",
    "    results_data_type_pred.drop(columns = ['Local authority', 'Prediction Number'], inplace = True)\n",
    "    results_data_type_pred['Metrics'] = pd.Categorical(results_data_type_pred['Metrics'], \n",
    "                                            categories = ['Proportion negative class value',\n",
    "                                                          'Accuracy',\n",
    "                                                          'Average precision', \n",
    "                                                          'Mean average precision (training)',\n",
    "                                                          'Std average precision (training)',\n",
    "                                                            'AUC'], \n",
    "                                            ordered = True)\n",
    "    results_data_type_pred.sort_values(by = ['Metrics', 'Cross Validation Method'], inplace = True)\n",
    "    results_data_type_pred.to_csv('{}/Model summary data type {}.csv'.format(LA, pred), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't include accuracy here\n",
    "metrics_to_report = ['Average precision', 'Mean average precision (training)', 'AUC']\n",
    "\n",
    "for metric in metrics_to_report:\n",
    "    metrics_beat_threshold_summary_ss = pd.crosstab(results_cv.loc[results_cv['Metrics'] == metric, 'Data included'], \n",
    "                                    results_cv.loc[results_cv['Metrics'] == metric, \n",
    "                                    'Greater than \"success\" threshold (0.65) - learning from all cases'], margins = True)\n",
    "    metrics_beat_threshold_summary_ts = pd.crosstab(results_cv.loc[results_cv['Metrics'] == metric, 'Data included'], \n",
    "                                    results_cv.loc[results_cv['Metrics'] == metric, \n",
    "                                    'Greater than \"success\" threshold (0.65) - Learning only from earlier cases'], margins = True)\n",
    "    print(metrics_beat_threshold_summary_ss)\n",
    "    print(metrics_beat_threshold_summary_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_beat_threshold_summary_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_beat_threshold_summary_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessary columns and remaning columns\n",
    "results.drop(columns = ['decision_tree', 'gradient_boosting', 'logistic_regression', 'Model'], inplace = True)\n",
    "results.rename(columns = {'LA': 'Local authority',\n",
    "                        'rq': 'Prediction', 'cv': 'Cross Validation Method',\n",
    "                         'data_type': 'Data included'},\n",
    "              inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming before saving\n",
    "print(results['Data included'].value_counts())\n",
    "results['Data included'] = np.where(results['Data included']=='str',\n",
    "                                  'Structured data only',\n",
    "                                   'Structured and text data')\n",
    "print(results['Data included'].value_counts())\n",
    "\n",
    "print(results['Cross Validation Method'].value_counts())\n",
    "results['Cross Validation Method'] = np.where(results['Cross Validation Method']=='ss',\n",
    "                                             'Learning from all cases',\n",
    "                                             'Learning only from earlier cases')\n",
    "print(results['Cross Validation Method'].value_counts())\n",
    "\n",
    "results['Best algorithm'] = np.where(results['Best algorithm']=='gradient_boosting',\n",
    "                                    'Gradient Boosting',\n",
    "                                    np.where(results['Best algorithm']=='decision_tree',\n",
    "                                    'Decision Tree',\n",
    "                                            'Logistic Regression'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming predictions according to the LA number and the rq number \n",
    "results['Prediction Number'] = np.where(((results['Local authority']=='LA1') & \n",
    "                                  (results['Prediction'] == 'rq1')),\n",
    "                                1,\n",
    "                                np.where(((results['Local authority']=='LA1') & \n",
    "                                  (results['Prediction'] == 'rq2')),\n",
    "                                2,\n",
    "                                np.where(((results['Local authority']=='LA2') & \n",
    "                                  (results['Prediction'] == 'rq1')),\n",
    "                                3,\n",
    "                                np.where(((results['Local authority']=='LA2') & \n",
    "                                  (results['Prediction'] == 'rq2')),\n",
    "                                4,\n",
    "                                np.where(((results['Local authority']=='LA3') & \n",
    "                                  (results['Prediction'] == 'rq1')),\n",
    "                                5,\n",
    "                                np.where(((results['Local authority']=='LA3') & \n",
    "                                  (results['Prediction'] == 'rq2')),\n",
    "                                6,\n",
    "                                np.where(((results['Local authority']=='LA4') & \n",
    "                                  (results['Prediction'] == 'rq1')),\n",
    "                                7,8)))))))\n",
    "\n",
    "results.drop(columns = 'Prediction', inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exceeds threshold?\n",
    "\n",
    "results['Greater than \"success\" threshold (0.65)'] = np.where((results['Best algorithm model performance'] >= 0.65) &\n",
    "                                                        (results['Metrics']!='Proportion negative class value') &\n",
    "                                                           (results['Metrics']!='Std average precision (training)'),\n",
    "                                                                 'Exceeds success threshold',\n",
    "                                                                 np.where((results['Best algorithm model performance'] < 0.65) &\n",
    "                                                        (results['Metrics']!='Proportion negative class value') &\n",
    "                                                           (results['Metrics']!='Std average precision (training)'),\n",
    "                                                                         'Does not exceeds success threshold' ,np.nan))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results = results.loc[results['Local authority']!='LA1',] # For the visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ((results['Metrics']!='Proportion negative class value') &\n",
    "       (results['Metrics']!='Std average precision (training)'))\n",
    "\n",
    "metrics_exceed_threshold = pd.DataFrame(pd.crosstab(results.loc[mask, 'Metrics'], \n",
    "            results.loc[mask, 'Greater than \"success\" threshold (0.65)']))\n",
    "\n",
    "metrics_exceed_threshold.to_csv('Output/Exceeds threshold by metric.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_exceed_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get name\n",
    "question_names = pd.read_csv(\"Years and Sample Sizes.csv\")\n",
    "question_names['Prediction Number'] = np.where((question_names['Local authority'] == 'LA1')\n",
    "                                               & (question_names['Research question'] == 'rq1'),1,\n",
    "                                        np.where((question_names['Local authority'] == 'LA1')\n",
    "                                               & (question_names['Research question'] == 'rq2'), 2,\n",
    "                                        np.where((question_names['Local authority'] == 'LA2')\n",
    "                                               & (question_names['Research question'] == 'rq1'),3,\n",
    "                                        np.where((question_names['Local authority'] == 'LA2')\n",
    "                                               & (question_names['Research question'] == 'rq2'), 4, \n",
    "                                        np.where((question_names['Local authority'] == 'LA3')\n",
    "                                               & (question_names['Research question'] == 'rq1'),5,\n",
    "                                        np.where((question_names['Local authority'] == 'LA3')\n",
    "                                               & (question_names['Research question'] == 'rq2'), 6,\n",
    "                                         np.where((question_names['Local authority'] == 'LA4')\n",
    "                                               & (question_names['Research question'] == 'rq1'),7, 8)))))))  \n",
    "results = results.merge(question_names[['Local authority', 'Research question', 'Shortened outcome', 'Prediction Number']], \n",
    "                                                  how = 'left', left_on = ['Local authority', 'Prediction Number'], right_on = ['Local authority', 'Prediction Number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_threshold = results.loc[results['Metrics'].isin(['AUC', 'Average precision']), ]\n",
    "results_threshold = results_threshold[['Local authority','Research question',\n",
    "       'Prediction Number', 'Shortened outcome', 'Metrics',\n",
    "       'Cross Validation Method', 'Data included',\n",
    "       'Best algorithm model performance',\n",
    "       'Greater than \"success\" threshold (0.65)']]\n",
    "print(results_threshold.shape)\n",
    "results_threshold.drop_duplicates(inplace = True)\n",
    "print(results_threshold.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_threshold.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_threshold.to_csv('Meet threshold.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_comparison_metrics = results.loc[results['Metrics'].isin(['Accuracy', 'AUC', 'Average precision']), ]\n",
    "results_comparison_metrics = results_comparison_metrics[['Local authority','Research question',\n",
    "       'Prediction Number', 'Shortened outcome', 'Metrics',\n",
    "       'Cross Validation Method', 'Data included',\n",
    "       'Best algorithm model performance']]\n",
    "print(results_comparison_metrics.shape)\n",
    "results_comparison_metrics.drop_duplicates(inplace = True)\n",
    "print(results_comparison_metrics.shape)\n",
    "results_comparison_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_comparison_metrics.to_csv('Comparison of metrics.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns\n",
    "# Threshold visualisation\n",
    "results = results[['Local authority',\n",
    " 'Prediction Number',\n",
    " 'Cross Validation Method',\n",
    " 'Data included',\n",
    " 'Metrics',\n",
    "  'Best algorithm',\n",
    " 'Best algorithm model performance']]\n",
    "# Sort values\n",
    "results.sort_values(by=['Local authority', 'Prediction Number', 'Cross Validation Method', 'Data included', 'Metrics'], inplace = True)\n",
    "# Save\n",
    "results.to_csv('Output/Summary table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the difference between the AP in validation and the AP on holdout data\n",
    "results_diff_validation_holdout = results.loc[(results['Metrics'] == 'Maximum lower bound (training)') | \n",
    "                                             (results['Metrics'] == 'Average precision'), ]\n",
    "print(results_diff_validation_holdout.shape)\n",
    "results_diff_validation_holdout.sort_values(by = ['Local authority', 'Prediction Number', 'Cross Validation Method',\n",
    "       'Data included', 'Metrics'], inplace = True)\n",
    "results_diff_validation_holdout = results_diff_validation_holdout.groupby(['Local authority', 'Prediction Number', 'Cross Validation Method',\n",
    "       'Data included']).diff()\n",
    "\n",
    "# Merge back in to allow for groupbys\n",
    "results_diff_validation_holdout.rename(columns = {'Best algorithm model performance': \n",
    "                                        'Difference between average precision in validation and on holdout data'},\n",
    "                                      inplace = True)\n",
    "\n",
    "results_diff_validation_holdout = results.merge(results_diff_validation_holdout, left_index = True, right_index = True, how = 'right')\n",
    "results_diff_validation_holdout.dropna(axis = 0, how = 'any', subset = ['Difference between average precision in validation and on holdout data'], inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_diff_validation_holdout['Difference between average precision in validation and on holdout data'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average drop of AP from validation to holdout by algorithm, data included and cv method\n",
    "\n",
    "overfitting_by_algorithm = pd.DataFrame(results_diff_validation_holdout.groupby('Best algorithm')['Difference between average precision in validation and on holdout data'].mean())\n",
    "overfitting_by_data_type = pd.DataFrame(results_diff_validation_holdout.groupby('Data included')['Difference between average precision in validation and on holdout data'].mean())\n",
    "overfitting_by_cv = pd.DataFrame(results_diff_validation_holdout.groupby('Cross Validation Method')['Difference between average precision in validation and on holdout data'].mean())\n",
    "\n",
    "overfitting_by_algorithm['Comparison'] = 'Algorithm'\n",
    "overfitting_by_algorithm.reset_index(drop = False, inplace = True)\n",
    "overfitting_by_algorithm.rename(columns = {'Best algorithm': 'Difference averaged by:'}, inplace = True)\n",
    "\n",
    "overfitting_by_data_type['Comparison'] = 'Data included'\n",
    "overfitting_by_data_type.reset_index(drop = False, inplace = True)\n",
    "overfitting_by_data_type.rename(columns = {'Data included': 'Difference averaged by:'}, inplace = True)\n",
    "\n",
    "overfitting_by_cv['Comparison'] = 'Cross Validation Method'\n",
    "overfitting_by_cv.reset_index(drop = False, inplace = True)\n",
    "overfitting_by_cv.rename(columns = {'Cross Validation Method': 'Difference averaged by:'}, inplace = True)\n",
    "\n",
    "overfitting = pd.concat([overfitting_by_algorithm, overfitting_by_data_type, overfitting_by_cv], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns, round and save\n",
    "\n",
    "overfitting = overfitting[['Comparison', 'Difference averaged by:', 'Difference between average precision in validation and on holdout data']]\n",
    "overfitting['Difference between average precision in validation and on holdout data'] = round(overfitting['Difference between average precision in validation and on holdout data'], 2)\n",
    "overfitting.to_csv('Output/Overfitting by group.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert 1==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of metrics\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of which cv method is best\n",
    "best_cv = pd.DataFrame(results_cv.loc[results_cv['Metrics']=='Average precision','Validation technique with best model performance'].value_counts())\n",
    "best_cv.reset_index(inplace = True)\n",
    "best_cv.rename(columns = {'index': 'Cross validation technique', 'Validation technique with best model performance':'Number of times the technique is the best performing'}, inplace = True)\n",
    "\n",
    "best_cv.to_csv('Output/Best cross validation table.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of which data_type method is best\n",
    "best_data_type = pd.DataFrame(results_data_type.loc[results_cv['Metrics']=='Average precision','Data included with best model performance'].value_counts())\n",
    "best_data_type.reset_index(inplace = True)\n",
    "best_data_type.rename(columns = {'index': 'Data included in model', 'Data included with best model performance':'Number of times the model with this data included is the best performing'}, inplace = True)\n",
    "\n",
    "best_data_type.to_csv('Output/Best data type table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_data_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of which cv method is best\n",
    "best_algorithm = pd.DataFrame(results.loc[results['Metrics']=='Average precision','Best algorithm'].value_counts())\n",
    "best_algorithm.reset_index(inplace = True)\n",
    "best_algorithm.rename(columns = {'index': 'Algorithm', 'Best algorithm':'Number of times the algorithm is the best performing'}, inplace = True)\n",
    "\n",
    "best_algorithm.to_csv('Output/Best algorithm table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank sum test - are the performance metrics better for ss?\n",
    "# Include all of the metrics where > is better. The number of average precision scores is too low\n",
    "# Not prespecified\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "results_mw = results_cv.loc[(results_cv['Metrics']!='Proportion negative class value') &\n",
    "                           (results_cv['Metrics']!='Std average precision (training)')]\n",
    "ss = results_mw['Learning from all cases']\n",
    "ts = results_mw['Learning only from earlier cases']\n",
    "\n",
    "mw_cv = mannwhitneyu(ss, ts, alternative='two-sided')\n",
    "print(mw_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank sum test - are the performance metrics better for all data_type?\n",
    "# Include all of the metrics where > is better. The number of average precision scores is too low\n",
    "# Not prespecified\n",
    "\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "results_mw = results_data_type.loc[(results_data_type['Metrics']!='Proportion negative class value') &\n",
    "                           (results_data_type['Metrics']!='Std average precision (training)')]\n",
    "structured = results_mw['Structured data only']\n",
    "structured_text = results_mw['Structured and text data']\n",
    "\n",
    "mw_data_type = mannwhitneyu(structured, structured_text, alternative='two-sided')\n",
    "print(mw_data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mw_df = pd.DataFrame(data = {'Mann Whitney U Statistic': [mw_cv[0], mw_data_type[0]],\n",
    "                        'P-value': [mw_cv[1], mw_data_type[1]]}, \n",
    "                        index=['Cross-Validation technique', 'Data included in model'])\n",
    "mw_df.to_csv('Output/Mann Whitney tests.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall mean and std of average precision\n",
    "results_mean = results.loc[results['Metrics'] == 'Average precision','Best algorithm model performance'].mean()\n",
    "results_std = results.loc[results['Metrics'] == 'Average precision','Best algorithm model performance'].std()\n",
    "print(results_mean)\n",
    "print(results_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall mean and std of average precision excluding LA1\n",
    "results_excl_LA1 = results.loc[results['Local authority'] !='LA1',]\n",
    "results_excl_LA1_mean = results_excl_LA1.loc[results_excl_LA1['Metrics'] == 'Average precision','Best algorithm model performance'].mean()\n",
    "results_excl_LA1_std = results_excl_LA1.loc[results_excl_LA1['Metrics'] == 'Average precision','Best algorithm model performance'].std()\n",
    "print(results_excl_LA1_mean)\n",
    "print(results_excl_LA1_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_summary = pd.DataFrame(data = {'Mean': [results_mean, results_excl_LA1_mean],\n",
    "                                      'Standard deviation': [results_std, results_excl_LA1_std]},\n",
    "                              index = ['Local authorities 1-4', 'Local authorities 2-4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_summary.to_csv('Output/Means and standard deviations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare 4 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pivot table to compare across the 4 models\n",
    "results_pivoted = pd.pivot_table(results, index=['Metrics', 'Local authority', 'Prediction Number'], columns = ['Cross Validation Method', 'Data included'], values='Best algorithm model performance')\n",
    "\n",
    "# Just include scores where higher is better\n",
    "print(results_pivoted.shape)\n",
    "metrics_higher_is_better = [r for r in results_pivoted.index if 'Proportion negative class value' not in r and 'Std average precision (training)' not in r]\n",
    "results_pivoted = results_pivoted.loc[metrics_higher_is_better,]\n",
    "print(results_pivoted.shape)\n",
    "results_pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts_all vs ts_str\n",
    "results_pivoted['Comparison of data included for models learning from all cases'] = np.where((results_pivoted[('Learning only from earlier cases', 'Structured and text data')]>results_pivoted[('Learning only from earlier cases', 'Structured data only')]),\n",
    "                                                       'Learning only from earlier cases, Structured and text data', \n",
    "                                            np.where((results_pivoted[('Learning only from earlier cases', 'Structured and text data')]<results_pivoted[('Learning only from earlier cases', 'Structured data only')]),\n",
    "                                                             'Learning only from earlier cases, Just structured data',\n",
    "                                                        np.where((results_pivoted[('Learning only from earlier cases', 'Structured and text data')]==results_pivoted[('Learning only from earlier cases', 'Structured data only')]),\n",
    "                                               'Same performance', np.nan)))   \n",
    "\n",
    "# ss_all vs ss_str\n",
    "results_pivoted['Comparison of data included for models learning only from earlier cases'] = np.where((results_pivoted[('Learning from all cases', 'Structured and text data')]>results_pivoted[('Learning from all cases', 'Structured data only')]),\n",
    "                                                       'learning from all cases, Structured and text data', \n",
    "                                            np.where((results_pivoted[('Learning from all cases', 'Structured and text data')]<results_pivoted[('Learning from all cases', 'Structured data only')]),\n",
    "                                                             'learning from all cases, Just structured data',\n",
    "                                                        np.where((results_pivoted[('Learning from all cases', 'Structured and text data')]==results_pivoted[('Learning from all cases', 'Structured data only')]),\n",
    "                                               'Same performance', np.nan)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ss_str vs ts_str\n",
    "results_pivoted['Comparison of cross validation for just structured data models'] = np.where((results_pivoted[('Learning only from earlier cases', 'Structured data only')]>results_pivoted[('Learning from all cases', 'Structured data only')]),\n",
    "                                                       'Learning only from earlier cases, Just structured data', \n",
    "                                            np.where((results_pivoted[('Learning only from earlier cases', 'Structured data only')]<results_pivoted[('Learning from all cases', 'Structured data only')]),\n",
    "                                                             'learning from all cases, Just structured data',\n",
    "                                                        np.where((results_pivoted[('Learning only from earlier cases', 'Structured data only')]==results_pivoted[('Learning from all cases', 'Structured data only')]),\n",
    "                                               'Same performance', np.nan)))   \n",
    "\n",
    "# ts_all vs ss_all\n",
    "results_pivoted['Comparison of cross validation for structured and text data models'] = np.where((results_pivoted[('Learning only from earlier cases', 'Structured and text data')]>results_pivoted[('Learning from all cases', 'Structured and text data')]),\n",
    "                                                       'Learning only from earlier cases, Structured and text data', \n",
    "                                            np.where((results_pivoted[('Learning only from earlier cases', 'Structured and text data')]<results_pivoted[('Learning from all cases', 'Structured and text data')]),\n",
    "                                                             'learning from all cases, Structured and text data',\n",
    "                                                        np.where((results_pivoted[('Learning only from earlier cases', 'Structured and text data')]==results_pivoted[('Learning from all cases', 'Structured and text data')]),\n",
    "                                               'Same performance', np.nan)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_included_time_aware = pd.DataFrame(results_pivoted.loc[[r for r in results_pivoted.index if 'Average precision' in r], 'Comparison of data included for models learning from all cases'].value_counts())\n",
    "data_included_time_aware.to_csv('Output/Comparison of data included for models learning from all cases.csv')\n",
    "\n",
    "data_included_non_time_aware = pd.DataFrame(results_pivoted.loc[[r for r in results_pivoted.index if 'Average precision' in r], 'Comparison of data included for models learning only from earlier cases'].value_counts()) \n",
    "data_included_non_time_aware.to_csv('Output/Comparison of data included for models learning only from earlier cases.csv')\n",
    "\n",
    "\n",
    "cross_validation_structured = pd.DataFrame(results_pivoted.loc[[r for r in results_pivoted.index if 'Average precision' in r], 'Comparison of cross validation for just structured data models'].value_counts())\n",
    "cross_validation_structured.to_csv('Output/Comparison of cross validation for just structured data models.csv')\n",
    "\n",
    "cross_validation_structured_text = pd.DataFrame(results_pivoted.loc[[r for r in results_pivoted.index if 'Average precision' in r], 'Comparison of cross validation for structured and text data models'].value_counts())\n",
    "cross_validation_structured_text.to_csv('Output/Comparison of cross validation for structured and text data models.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_pivoted.to_csv('Output/Summary table - comparison of cross validation and data included.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert 1==2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create values for visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_pivoted.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_for_visualisation = results_pivoted.drop(columns = ['Comparison of data included for models learning from all cases', \n",
    "'Comparison of data included for models learning only from earlier cases',\n",
    "'Comparison of cross validation for just structured data models',\n",
    "'Comparison of cross validation for structured and text data models'\n",
    "])\n",
    "comparison_of_metrics = results_for_visualisation.iloc[(results_for_visualisation.index.get_level_values('Metrics') == 'Accuracy') |\n",
    "                              (results_for_visualisation.index.get_level_values('Metrics') == 'AUC') |\n",
    "                              (results_for_visualisation.index.get_level_values('Metrics') == 'Average precision'), ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_for_visualisation.iloc[(results_for_visualisation.index.get_level_values('Metrics') == 'Precision') |\n",
    "                              (results_for_visualisation.index.get_level_values('Metrics') == 'Recall'),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_alarms_children_missed = results_for_visualisation.iloc[(results_for_visualisation.index.get_level_values('Metrics') == 'Precision') |\n",
    "                              (results_for_visualisation.index.get_level_values('Metrics') == 'Recall'),]\n",
    "\n",
    "\n",
    "false_alarms_children_missed[('Learning from all cases','Structured and text data')] = (1 - false_alarms_children_missed[('Learning from all cases','Structured and text data')]) * 100\n",
    "false_alarms_children_missed[('Learning from all cases','Structured data only')] = (1 - false_alarms_children_missed[('Learning from all cases','Structured data only')]) * 100\n",
    "false_alarms_children_missed[('Learning only from earlier cases','Structured and text data')] = (1 - false_alarms_children_missed[('Learning only from earlier cases','Structured and text data')]) * 100\n",
    "false_alarms_children_missed[('Learning only from earlier cases','Structured data only')] = (1 - false_alarms_children_missed[('Learning only from earlier cases','Structured data only')]) * 100\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add shortened name for predicted outcome\n",
    "false_alarms_children_missed.reset_index(inplace = True)\n",
    "false_alarms_children_missed['Metrics'] = false_alarms_children_missed['Metrics'].replace({'Precision': 'False alarms', 'Recall': 'Children at risk missed'})\n",
    "false_alarms_children_missed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import shortened prediction questions\n",
    "\n",
    "question_names = pd.read_csv(\"Years and Sample Sizes.csv\")\n",
    "question_names['Prediction Number'] = np.where((question_names['Local authority'] == 'LA1')\n",
    "                                               & (question_names['Research question'] == 'rq1'),1,\n",
    "                                        np.where((question_names['Local authority'] == 'LA1')\n",
    "                                               & (question_names['Research question'] == 'rq2'), 2,\n",
    "                                        np.where((question_names['Local authority'] == 'LA2')\n",
    "                                               & (question_names['Research question'] == 'rq1'),3,\n",
    "                                        np.where((question_names['Local authority'] == 'LA2')\n",
    "                                               & (question_names['Research question'] == 'rq2'), 4, \n",
    "                                        np.where((question_names['Local authority'] == 'LA3')\n",
    "                                               & (question_names['Research question'] == 'rq1'),5,\n",
    "                                        np.where((question_names['Local authority'] == 'LA3')\n",
    "                                               & (question_names['Research question'] == 'rq2'), 6,\n",
    "                                         np.where((question_names['Local authority'] == 'LA4')\n",
    "                                               & (question_names['Research question'] == 'rq1'),7, 8)))))))    \n",
    "                    \n",
    "                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_alarms_children_missed_summary = false_alarms_children_missed.merge(question_names[['Prediction Number', 'Shortened outcome']], on = 'Prediction Number', how = 'left')\n",
    "false_alarms_children_missed_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in the shortened outcome names\n",
    "false_alarms_children_missed_summary = false_alarms_children_missed_summary[[\n",
    "                                    ('Local authority', ''), 'Prediction Number', \n",
    "                                    'Shortened outcome',\n",
    "                                     ('Metrics', ''),\n",
    "                                     ('Learning from all cases', 'Structured and text data'),\n",
    "                    ('Learning from all cases', 'Structured data only'),\n",
    "       ('Learning only from earlier cases', 'Structured and text data'),\n",
    "           ('Learning only from earlier cases', 'Structured data only')]]\n",
    "false_alarms_children_missed_summary.sort_values(by = [('Local authority', ''), 'Prediction Number'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_alarms_children_missed_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then check all are available and match with what's in the techincal report\n",
    "false_alarms_children_missed_summary.drop_duplicates(inplace = True)\n",
    "false_alarms_children_missed_summary[('Metrics','')].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_alarms_children_missed_summary.to_csv('False alarms and children at risk.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning curve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "learning_curve_list = glob.glob(\"/Documents/Summary information FINAL/*/Learning curve data *.csv\")\n",
    "\n",
    "\n",
    "learning_curve_dict = {}\n",
    "for file_name in learning_curve_list:\n",
    "    file = pd.read_csv(file_name)\n",
    "    file_name = file_name.replace('/Documents/Summary information FINAL/', '')\n",
    "    file_name = file_name.replace('.csv', '')\n",
    "    learning_curve_dict[file_name] = file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curve_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curve_data = pd.concat(learning_curve_dict, axis = 0)\n",
    "learning_curve_data.reset_index(drop = False, inplace = True)\n",
    "learning_curve_data.rename(columns = {'level_0': 'Graph file name'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curve_data['Cross validation'] = np.where(learning_curve_data['model_id'].str.contains('ss'),\n",
    "                                             'Learning from all cases',\n",
    "                                             'Learning only from earlier cases')\n",
    "\n",
    "learning_curve_data['Data included'] = np.where(learning_curve_data['model_id'].str.contains('str'),\n",
    "                                            'Structured data only',\n",
    "                                           'Structured and text data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming model_ids according to the LA number and the rq number \n",
    "learning_curve_data['Prediction Number'] = np.where(((learning_curve_data['LA']=='LA1') & \n",
    "                                  (learning_curve_data['model_id'].str.contains('rq1'))),\n",
    "                                1,\n",
    "                                np.where(((learning_curve_data['LA']=='LA1') & \n",
    "                                  (learning_curve_data['model_id'].str.contains( 'rq2'))),\n",
    "                                2,\n",
    "                                np.where(((learning_curve_data['LA']=='LA2') & \n",
    "                                  (learning_curve_data['model_id'].str.contains('rq1'))),\n",
    "                                3,\n",
    "                                np.where(((learning_curve_data['LA']=='LA2') & \n",
    "                                  (learning_curve_data['model_id'].str.contains( 'rq2'))),\n",
    "                                4,\n",
    "                                np.where(((learning_curve_data['LA']=='LA3') & \n",
    "                                  (learning_curve_data['model_id'].str.contains('rq1'))),\n",
    "                                5,\n",
    "                                np.where(((learning_curve_data['LA']=='LA3') & \n",
    "                                  (learning_curve_data['model_id'].str.contains( 'rq2'))),\n",
    "                                6,\n",
    "                                np.where(((learning_curve_data['LA']=='LA4') & \n",
    "                                  (learning_curve_data['model_id'].str.contains('rq1'))),\n",
    "                                7,8)))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curve_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curve_data = learning_curve_data[['Number of observations',\n",
    "       'Average precision - train', 'Average precision - test', 'LA',\n",
    "       'Cross validation', 'Data included', 'Prediction Number', 'Graph file name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curve_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curve_data_LA4 = pd.read_csv('/Final/Would more data help 7 and 8.csv')\n",
    "learning_curve_data_LA4['Graph file name'] = 'LA4/' + learning_curve_data_LA4['Graph file name']\n",
    "learning_curve_data_LA4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curve_data_all = pd.concat([learning_curve_data, learning_curve_data_LA4], axis = 0)\n",
    "learning_curve_data_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curve_data_all['Prediction Number'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curve_data_all.to_csv('Final/Take Three/Would more data help all.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
