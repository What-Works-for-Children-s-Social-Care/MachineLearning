{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating text dataset to match the structured dataset before cleaning\n",
    "\n",
    "- Current data uniquely identified by PSID, ReferralStartDate and AssessmentType\n",
    "- Previous data uniquely identified by PSID, ReferralStartDate and AssessmentType dummies\n",
    "- Keys: document has ActualStartDate within ReferralStartDate - ReferralCloseDate period (minus 1 day)\n",
    "- It appears sometimes the document ActualStartDate is the day before\n",
    "\n",
    "- Population: rq1: at the point of referral (only keep Contact and Referral form)\n",
    "- Population: rq2: open cases (documents with ActualStartDate within 2 weeks of ReferralDatetime)\n",
    "- Aggregate text by DocumentName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path\n",
    "sys.path.append('C:\\\\Program Files\\\\Python37\\\\Lib\\\\site-packages')\n",
    "sys.path.append('C:\\Program Files\\Python37')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"C:\\\\Users\\\\[username]\\\\Downloads\\\\Updated Text Data\") # insert [username]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bring all the data and store in a pickled dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 1==2\n",
    "# Takes a while to run\n",
    "# Use \"Created/all_text_dict.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "list_of_text_files = glob.glob(\"My Documents*.xlsx\")\n",
    "\n",
    "pseudo_id_misspellings = ['PseudoId', 'PsuedoID', 'PesudoID','PsueodID']\n",
    "pseudo_id_rename = dict(zip(pseudo_id_misspellings,['PseudoID'] *4))\n",
    "\n",
    "df_list = []\n",
    "for file in list_of_text_files:\n",
    "    print(file)\n",
    "    try:\n",
    "        df = pd.read_excel(file)\n",
    "        print(df.shape)\n",
    "    except(UnicodeDecodeError):\n",
    "        # Some datafiles are encoded differently\n",
    "        df = pd.read_excel(file,  encoding = 'latin')\n",
    "        print(df.shape)\n",
    "    print(df.shape)\n",
    "    # PseudoID spelt slightly differently\n",
    "    if set(pseudo_id_misspellings).intersection(set(df.columns)):\n",
    "        df.rename(columns =pseudo_id_rename, inplace = True)\n",
    "    # Some columns are completely empty as there's no answer of that datatype\n",
    "    df.dropna(axis = 1, how = 'all', inplace = True)\n",
    "    df.dropna(axis = 0, how = 'all', inplace = True)\n",
    "    df.dropna(subset = ['ActualStartDate', 'PSID'], how = 'any', inplace = True)\n",
    "    try:\n",
    "        df.drop(columns = ['Question', 'QuestionType'], inplace = True)\n",
    "    except:\n",
    "        continue\n",
    "    # Standardise document names\n",
    "    df['DocumentName'] = df['DocumentName'].map(lambda x: str(x).strip(\"(c) \"))\n",
    "    df['DocumentName'] = df['DocumentName'].map(lambda x: str(x).strip(\" v3.1\"))\n",
    "    # Make start date date time object\n",
    "    df['ActualStartDate'] = pd.to_datetime(df['ActualStartDate'], format = \"%m/%d/%Y %H:%M\")\n",
    "    df['ActualStartDate'] = pd.to_datetime(df['ActualStartDate'].map(lambda x: x.strftime('%Y-%m-%d')))\n",
    "    print(df.shape)\n",
    "    # Harmonising column names and dropping duplicate columns\n",
    "    pseudoid = df.columns[len(df.columns)-1]\n",
    "    print(pseudoid)\n",
    "    df.rename(columns = {pseudoid: \"PseudoID_keep\"}, inplace = True)\n",
    "    df.drop(columns = df.columns[(len(df.columns) - 3)], inplace = True)\n",
    "    print(df.columns)\n",
    "    df_list.append(df)\n",
    "    \n",
    "# MyDocuments04806 - row 279/ 280 corrupted; lose c.150 observations\n",
    "## Create dictionary of dataframes with key as file name\n",
    "df_dict = dict(zip(list_of_text_files, df_list))\n",
    "df_dict2 = dict(df_dict)\n",
    "\n",
    "with open(\"Created/all_text_dict.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(df_dict2, handle, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined dataframes in pickled dictionary into a single dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 1==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in all text data stored as a dictionary\n",
    "# Keys = document name; values = dataframe\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "os.chdir(\"C:\\\\Users\\\\[username]\\\\Downloads\\\\Updated Text Data\") # insert [username]\n",
    "filename = open(\"Created/all_text_dict.pkl\", \"rb\")\n",
    "df_dict2 = pickle.load(filename)\n",
    "df_dict = dict(df_dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question list is meta-data matching the question ID to the question and the form it's on\n",
    "# At the moment just used for deciding which files to drop\n",
    "import pandas as pd\n",
    "question_list = pd.read_excel(\".\\\\Other\\\\QuestionListfull.xlsx\")\n",
    "question_list['Question ID'] = question_list['Question ID'].map(lambda x: x.strip(\"CHILD \"))\n",
    "question_list['Document Name (All)'] = question_list['Document Name (All)'].map(lambda x: x.strip(\"(c) \"))\n",
    "question_list.head()\n",
    "\n",
    "# Link to structured data using contact and referral documents - identify contact and referral documents\n",
    "question_IDs_contact_referral = list(question_list.loc[question_list['Document Name (All)'] == 'Contact and Referral Form', 'Question ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what questions the documents cover\n",
    "import glob\n",
    "import re\n",
    "list_of_text_files = glob.glob(\"My Documents*.xlsx\")\n",
    "list_of_text_files_num = [re.sub(\"My Documents\", \"\", file) for file in list_of_text_files]\n",
    "list_of_text_files_num = [re.sub(\".xlsx\", \"\", file) for file in list_of_text_files_num]\n",
    "\n",
    "question_id_dict = dict(zip(list_of_text_files, list(question_list.loc[question_list['Question ID'].isin(list_of_text_files_num),'Section Question'])))\n",
    "\n",
    "# Drop documents with prescient columns\n",
    "# Outcome of this assessment - 04778, 04809, 04813\n",
    "# 09996, 09998 - Duplicate of 'My Documents08728.xlsx' (child risk factors at contact)\n",
    "# 09997 - Duplicate of 'My Documents08648.xlsx' (parental risk factors at contact)\n",
    "# 07754 - Parent / carers name\n",
    "documents_to_exclude = (['My Documents04778.xlsx', 'My Documents04809.xlsx', \n",
    "                         'My Documents04813.xlsx', 'My Documents09996.xlsx', 'My Documents09997.xlsx',\n",
    "                        'My Documents09998.xlsx', 'My Documents07754.xlsx'])\n",
    "\n",
    "for k in documents_to_exclude:\n",
    "    question_id_dict.pop(k, None)\n",
    "    df_dict.pop(k, None)\n",
    "    df_dict2.pop(k, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Identify dfs with multiple answer columns\n",
    "multiple_answer_dfs = {}\n",
    "for df_name, df in df_dict2.items():\n",
    "    print(df.columns)\n",
    "    answer_cols = [col for col in df.columns if 'MultipleChoiceAnswer' in col]\n",
    "    if answer_cols != []:\n",
    "        print(df_name)\n",
    "        multiple_answer_dfs[df_name] = df \n",
    "        del df_dict[df_name]\n",
    "print(len(df_dict.keys())) \n",
    "print(len(multiple_answer_dfs.keys()))\n",
    "print(len(df_dict2.keys())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Multiple answer questions have multiple rows per child / actual start date \n",
    "## if multiple answers are given, change from long to wide \n",
    "for df_name, df in multiple_answer_dfs.items():\n",
    "    print(df.shape)\n",
    "    df_de_dup = df.drop_duplicates(subset = ['PSID', 'ActualStartDate', 'MultipleChoiceAnswer'], keep = 'first')\n",
    "    print(df_de_dup.shape)\n",
    "    # Turn into dummies (=> multiple rows, multiple columns)\n",
    "    df_de_dup_multiple = pd.get_dummies(df_de_dup['MultipleChoiceAnswer'], prefix = 'Answer_{}'.format(df_de_dup['QuestionMarker'][0]))\n",
    "    df_de_dup = pd.concat([df_de_dup, df_de_dup_multiple], axis = 1)\n",
    "    df_de_dup.drop(columns = 'MultipleChoiceAnswer', inplace = True)\n",
    "    # Collapse to one row per child / actual start date\n",
    "    df_de_dup_max = df_de_dup.groupby(['PSID', 'ActualStartDate'])[df_de_dup_multiple.columns].max()\n",
    "    df_de_dup_max.reset_index(drop = False, inplace = True)\n",
    "    # Merge back into main dataframe\n",
    "    df_de_dup_wide = df_de_dup[['ActualStartDate', 'DocumentName', 'SectionName', 'QuestionMarker',\n",
    "           'PSID']].merge(df_de_dup_max, how = 'left', on = ['PSID', 'ActualStartDate'])\n",
    "    print(df_de_dup_wide.shape)\n",
    "    df_dict2[df_name] = df_de_dup_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduplicate after having dealt with multiple answer columns\n",
    "for df_name, df in df_dict2.items():\n",
    "    print(df_name)\n",
    "    print(df.shape)\n",
    "    answer = [col for col in df.columns if 'Answer' in col]\n",
    "    print(answer)\n",
    "    if answer:\n",
    "        cols_de_dup = answer +  ['PSID', 'ActualStartDate']\n",
    "        df_de_dup = df.drop_duplicates(subset = cols_de_dup, keep = 'first')\n",
    "        print(df.shape)\n",
    "        df_dict2[df_name] = df_de_dup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Concatenate text in the same document for the same child on the same ActualStartDate\n",
    "for df_name, df in df_dict2.items():\n",
    "    print(df_name)\n",
    "    print(df.shape)\n",
    "    answers = [col for col in df.columns if 'Answer' in col]\n",
    "    for answer in answers:\n",
    "        if \"Answer_CHILD\" not in answer:\n",
    "            print(answer)\n",
    "            df[answer] = df[answer].fillna('')\n",
    "            print(\"Number missing: \", df[answer].isna().sum())\n",
    "            df = df.loc[:,~df.columns.duplicated()]\n",
    "            print(df['DocumentName'].value_counts())\n",
    "            df_answer_concat = df.groupby(['PSID', 'ActualStartDate', 'DocumentName'], as_index = False)[answer].agg(' '.join)\n",
    "            print(df_answer_concat.shape)\n",
    "            df_pivoted = df_answer_concat.set_index(['PSID', 'ActualStartDate']).pivot(columns = 'DocumentName')[answer].reset_index().rename_axis(None, axis=1)\n",
    "            print(df_pivoted.shape)\n",
    "            df_dict2[df_name] = df_pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns in preparation for merging\n",
    "# Otherwise end up with _x and _y repeat columns\n",
    "\n",
    "cols_to_combine = (['Child Social Work Assessment for Review Child Protection Conference',\n",
    "            'Child Social Work Assessment to Initial Child Protection Conference',\n",
    "            'Child Social Work Assessment',\n",
    "            'Contact and Referral Form'])\n",
    "\n",
    "for df_name, df in df_dict2.items():\n",
    "    suf = re.findall('[0-9]+', df_name)\n",
    "    print(suf[0])\n",
    "    #df.columns = df.columns.map(lambda x: x+suf[0] if x != 'PSID' and x != 'ActualStartDate' else x)\n",
    "    df.columns = df.columns.map(lambda x: x+suf[0] if x in cols_to_combine else x)\n",
    "    print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Can now merge together text dataframes\n",
    "from functools import reduce\n",
    "import pickle\n",
    "\n",
    "df_all_text = reduce(lambda x, y: pd.merge(x, y, how = 'outer', on = ['ActualStartDate', 'PSID']), df_dict2.values())\n",
    "\n",
    "with open(\"Created\\\\df_all_text.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(df_all_text, handle, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine text from different forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 1==2\n",
    "# Takes a while to run\n",
    "# Use \"Created\\\\df_combined_all_text.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "os.chdir(\"C:\\\\Users\\\\[username]\\\\Downloads\\\\Updated Text Data\") # insert [username]\n",
    "filename = open(\"Created\\\\df_all_text.pkl\", \"rb\")\n",
    "df_all = pickle.load(filename)\n",
    "\n",
    "repeat_cols_y = [col for col in df_all.columns if '_y' in col]\n",
    "repeat_cols_x = [col for col in df_all.columns if '_x' in col]\n",
    "\n",
    "repeat_cols = repeat_cols_y + repeat_cols_x\n",
    "\n",
    "df_all.drop(columns = repeat_cols, inplace = True)\n",
    "\n",
    "cols_to_combine = (['Child Social Work Assessment for Review Child Protection Conference',\n",
    "            'Child Social Work Assessment to Initial Child Protection Conference',\n",
    "            'Child Social Work Assessment',\n",
    "            'Contact and Referral Form'])\n",
    "\n",
    "relevant_cols_all = []\n",
    "for column in cols_to_combine: \n",
    "    print(\"Column: \", column)\n",
    "    relevant_cols = [col for col in df_all.columns if column in col]\n",
    "    #print(\"Relevant columns: \", relevant_cols)\n",
    "    # Select relevant columns\n",
    "    # Fillna crashes the computer when trying to do all at once\n",
    "    relevant_cols_df = df_all[relevant_cols].fillna('')\n",
    "    # Check that filling na worked - yes\n",
    "    #print(relevant_cols_df.isna().sum())\n",
    "    df_all[column + '_text'] = ''\n",
    "    for idx in df_all.index:\n",
    "        # Set to get rid of duplicates\n",
    "        list_text = list(set([t for t in relevant_cols_df.loc[idx,]]))\n",
    "        # Join together text \n",
    "        df_all.loc[idx, column + '_text'] = ' '.join(list_text)\n",
    "        # Check length of new combined document == sum of documents (Yes plus a few \\n)\n",
    "        #print(\"Sum of length of all relevant cells: \", sum([len(df_all.loc[idx, col]) for col in relevant_cols if type(df_all.loc[idx, col]) != float]))\n",
    "        #if type(df_all.loc[idx,  column + '_text']) != float:\n",
    "            #print(\"Length of new cell: \", len(df_all.loc[idx, column + '_text']))\n",
    "        if (idx % 1000 ==0) & (idx != 0):\n",
    "            print(\"Idx: \", idx)\n",
    "            with open(\"Created\\\\Combined\\\\df_all_combined_{}_{}_{}.pkl\".format(column, (idx-1000), idx), \"wb\") as handle:\n",
    "                pickle.dump(df_all.loc[(idx-1000):idx,['PSID', 'ActualStartDate',column + '_text']], handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "        if idx == (df_all.shape[0]-1):\n",
    "            print(\"Idx: \", idx)\n",
    "            with open(\"Created\\\\Combined\\\\df_all_combined_{}_{}_{}.pkl\".format(column, (idx-(df_all.shape % 1000)), idx), \"wb\") as handle:\n",
    "                pickle.dump(df_all.loc[(idx-(df_all.shape % 1000)):idx,['PSID', 'ActualStartDate',column + '_text']], handle, protocol = pickle.HIGHEST_PROTOCOL)            \n",
    "    relevant_cols_all.extend(relevant_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bring concatenated data back in\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "os.chdir(\"C:\\\\Users\\\\[username]\\\\Downloads\\\\Updated Text Data\") # insert [username]\n",
    "list_of_text_files = glob.glob(\"Created\\\\Combined\\\\*.pkl\")\n",
    "\n",
    "df_combined_files_dict = {}\n",
    "for file in list_of_text_files:\n",
    "    filename = open(file, \"rb\")\n",
    "    df_combined = pickle.load(filename)\n",
    "    df_combined_files_dict[file] = df_combined\n",
    "\n",
    "# Concatenate dataframes with the same columns (axis = 0)\n",
    "# Create a list of dataframes with different columns for merging\n",
    "list_of_text_files_to_concat = list(set([re.sub('[0-9+]', '', file) for file in list_of_text_files]))\n",
    "list_of_text_files_to_concat = list(set([re.sub('_.pkl', '', file) for file in list_of_text_files_to_concat]))\n",
    "\n",
    "df_combined_list_all = []\n",
    "for file in list_of_text_files_to_concat:\n",
    "    print(file)\n",
    "    relevant_files = [f for f in list_of_text_files if file in f]\n",
    "    #print(relevant_files)\n",
    "    df_combined_list = []\n",
    "    for f in relevant_files:\n",
    "        df_combined_list.append(df_combined_files_dict[f])\n",
    "    df_combined = pd.concat(df_combined_list, axis = 0)\n",
    "    print(df_combined.shape)\n",
    "    df_combined_list_all.append(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dataframes with different text columns\n",
    "from functools import reduce\n",
    "df_combined_all_text = reduce(lambda left, right: pd.merge(left, right, on = ['PSID', 'ActualStartDate'], how = 'outer'), df_combined_list_all)\n",
    "\n",
    "# Look at number of unique entries\n",
    "for col in df_combined_all_text.columns:\n",
    "    print(col)\n",
    "    print(df_combined_all_text[col].nunique())\n",
    "\n",
    "with open(\"Created\\\\df_combined_all_text.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(df_combined_all_text, handle, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_all_text.loc[df_combined_all_text['Child Social Work Assessment for Review Child Protection Conference_text'] != '', ['ActualStartDate', 'Child Social Work Assessment for Review Child Protection Conference_text']].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine with rq1data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 1==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "os.chdir(\"C:\\\\Users\\\\[username]\\\\Downloads\\\\Updated Text Data\") # insert [username]\n",
    "filename = open(\"Created\\\\df_combined_all_text.pkl\", \"rb\")\n",
    "df_combined_all_text = pickle.load(filename)\n",
    "print(df_combined_all_text.shape)\n",
    "\n",
    "rq1data = pd.read_csv(\"..\\\\Updated Structured Data\\\\Created\\\\rq1data_w_previous.csv\", index_col = 0)\n",
    "print(rq1data.shape)\n",
    "#print(rq1data.index)\n",
    "#print(rq1data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in just structured data required to select relevant text data\n",
    "# Don't want all structured data too yet as we'll need to have just text data for extracting text features\n",
    "\n",
    "rq1data_text = rq1data[['PSID', 'ReferralDatetime', 'ReferralCloseDate', 'ActualStartDate']].merge(df_combined_all_text, how = 'outer', on = 'PSID')\n",
    "rq1data_text[[ 'Child Social Work Assessment for Review Child Protection Conference_text',\n",
    "       'Child Social Work Assessment to Initial Child Protection Conference_text',\n",
    "       'Child Social Work Assessment_text', 'Contact and Referral Form_text']] = (rq1data_text[[ 'Child Social Work Assessment for Review Child Protection Conference_text',\n",
    "                                                               'Child Social Work Assessment to Initial Child Protection Conference_text',\n",
    "                                                               'Child Social Work Assessment_text', 'Contact and Referral Form_text']].fillna(''))\n",
    "(rq1data_text.rename(columns = {'ActualStartDate_x':'ActualStartDate_str',\n",
    "                                 'ActualStartDate_y':'ActualStartDate_txt'},\n",
    "                                inplace = True))\n",
    "\n",
    "rq1data_text['ActualStartDate_str'] = pd.to_datetime(rq1data_text['ActualStartDate_str'], format = \"%Y-%m-%d\")\n",
    "rq1data_text['ReferralDatetime'] = pd.to_datetime(rq1data_text['ReferralDatetime'], format = \"%Y-%m-%d\")\n",
    "rq1data_text['ReferralCloseDate'] = pd.to_datetime(rq1data_text['ReferralCloseDate'], format = \"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rq1data_text.loc[rq1data_text['Child Social Work Assessment for Review Child Protection Conference_text'] != '', ['ActualStartDate','ActualStartDate_str', 'Child Social Work Assessment for Review Child Protection Conference_text']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rq1data_text.loc[rq1data_text['Child Social Work Assessment for Review Child Protection Conference_text'] != '', ['ActualStartDate','ActualStartDate_str', 'Child Social Work Assessment for Review Child Protection Conference_text']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check merges\n",
    "'''\n",
    "rq1data_text['ActualStartDateMatch_exactmatch'] = (np.where((rq1data_text['ActualStartDate_txt'] - rq1data_text['ActualStartDate_str']).dt.days ==0,\n",
    "                                                 1,0))\n",
    "\n",
    "rq1data_text['ActualStartDateMatch_daybefore'] = (np.where(((rq1data_text['ActualStartDate_txt'] - rq1data_text['ActualStartDate_str']).dt.days <= 0) &\n",
    "                                                 ((rq1data_text['ActualStartDate_txt'] - rq1data_text['ActualStartDate_str']).dt.days >=-1),\n",
    "                                                 1,0))\n",
    "\n",
    "rq1data_text['ActualStartDateMatch_dayeitherside'] = (np.where(((rq1data_text['ActualStartDate_txt'] - rq1data_text['ActualStartDate_str']).dt.days <= 1) &\n",
    "                                                 ((rq1data_text['ActualStartDate_txt'] - rq1data_text['ActualStartDate_str']).dt.days >=-1),\n",
    "                                                 1,0))\n",
    "\n",
    "rq1data_text['ActualStartDateMatch_weekbefore'] = (np.where(((rq1data_text['ActualStartDate_txt'] - rq1data_text['ActualStartDate_str']).dt.days <= 0) &\n",
    "                                                 ((rq1data_text['ActualStartDate_txt'] - rq1data_text['ActualStartDate_str']).dt.days >=-7),\n",
    "                                                 1,0))\n",
    "\n",
    "rq1data_text['ActualStartDateMatch_weekeitherside'] = (np.where(((rq1data_text['ActualStartDate_txt'] - rq1data_text['ActualStartDate_str']).dt.days <= 7) &\n",
    "                                                 ((rq1data_text['ActualStartDate_txt'] - rq1data_text['ActualStartDate_str']).dt.days >=-7),\n",
    "                                                 1,0))\n",
    "\n",
    "rq1data_text['ActualStartDateMatch_twoweekseitherside'] = (np.where(((rq1data_text['ActualStartDate_txt'] - rq1data_text['ActualStartDate_str']).dt.days <= 14) &\n",
    "                                                 ((rq1data_text['ActualStartDate_txt'] - rq1data_text['ActualStartDate_str']).dt.days >=-14),\n",
    "                                                 1,0))\n",
    "print(rq1data_text['ActualStartDateMatch_exactmatch'].value_counts())\n",
    "print(rq1data_text['ActualStartDateMatch_daybefore'].value_counts())\n",
    "print(rq1data_text['ActualStartDateMatch_dayeitherside'].value_counts())\n",
    "print(rq1data_text['ActualStartDateMatch_weekbefore'].value_counts())\n",
    "print(rq1data_text['ActualStartDateMatch_weekeitherside'].value_counts())\n",
    "print(rq1data_text['ActualStartDateMatch_twoweekseitherside'].value_counts())\n",
    "\n",
    "# Most of the additional matches in day either side come from day before (another 4200, 1 day after adds 30ish)\n",
    "# Most of the additional matches in week either side come from week before (another 450, 1 week after adds 50ish)\n",
    "# Additional matches for two weeks either side (another 200)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# If the text actual start date is the same or after the structured data actual start date or the referral date minus 1 BUT..\n",
    "# before the structured data referral close date\n",
    "# Only take open case up to 3 months \n",
    "rq1data_text['BetweenReferralMatch'] = (np.where(((rq1data_text['ActualStartDate_txt'] >= rq1data_text['ReferralDatetime']) & \n",
    "                                                 (rq1data_text['ActualStartDate_txt'] <= rq1data_text['ReferralCloseDate'])) |\n",
    "                                                 ((rq1data_text['ActualStartDate_txt'] >= rq1data_text['ActualStartDate_str']) & \n",
    "                                                 (rq1data_text['ActualStartDate_txt'] <= rq1data_text['ReferralCloseDate'])) ,\n",
    "                                                 1,0))\n",
    "\n",
    "rq1data_text['BetweenReferralMatch_minus_one_day'] = (np.where((((rq1data_text['ActualStartDate_txt'] - rq1data_text['ReferralDatetime']).dt.days >=-1) & \n",
    "                                                 (rq1data_text['ActualStartDate_txt'] <= rq1data_text['ReferralCloseDate'])) |\n",
    "                                                 (((rq1data_text['ActualStartDate_txt'] - rq1data_text['ActualStartDate_str']).dt.days >=-1) & \n",
    "                                                 (rq1data_text['ActualStartDate_txt'] <= rq1data_text['ReferralCloseDate'])) ,\n",
    "                                                 1,0))\n",
    "\n",
    "rq1data_text['BetweenReferralMatch_minus_one_week'] = (np.where((((rq1data_text['ActualStartDate_txt'] - rq1data_text['ReferralDatetime']).dt.days >=-7) & \n",
    "                                                 (rq1data_text['ActualStartDate_txt'] <= rq1data_text['ReferralCloseDate'])) |\n",
    "                                                 (((rq1data_text['ActualStartDate_txt'] - rq1data_text['ActualStartDate_str']).dt.days >=-7) & \n",
    "                                                 (rq1data_text['ActualStartDate_txt'] <= rq1data_text['ReferralCloseDate'])) ,\n",
    "                                                 1,0))\n",
    "\n",
    "print(rq1data_text['BetweenReferralMatch'].value_counts())\n",
    "print(rq1data_text['BetweenReferralMatch_minus_one_day'].value_counts())\n",
    "print(rq1data_text['BetweenReferralMatch_minus_one_week'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact match on ActualStartDates seems too strict (only two that are not within the referral period)\n",
    "# Adding in one day before, adding in one week before only adds another 100ish\n",
    "# rq1data_text['BetweenReferralMatch_minus_one_day']\n",
    "#print(pd.crosstab(rq1data_text['ActualStartDateMatch_exactmatch'], rq1data_text['BetweenReferralMatch']))\n",
    "print(pd.crosstab(rq1data_text['ActualStartDateMatch_dayeitherside'], rq1data_text['BetweenReferralMatch_minus_one_day']))\n",
    "print(pd.crosstab(rq1data_text['ActualStartDateMatch_dayeitherside'], rq1data_text['BetweenReferralMatch_minus_one_week']))\n",
    "#print(pd.crosstab(rq1data_text['ActualStartDateMatch_weekeitherside'], rq1data_text['BetweenReferralMatch']))\n",
    "#print(pd.crosstab(rq1data_text['ActualStartDateMatch_twoweekseitherside'], rq1data_text['BetweenReferralMatch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1500 duplicate C / R using 'BetweenReferralMatch_minus_one_day'\n",
    "# 50 duplicate C / R using 'BetweenReferralMatch'\n",
    "# 150 duplicate Child Social Work Assessment_text using 'BetweenReferralMatch_minus_one_day'\n",
    "# 50 duplicate Child Social Work Assessment_text using 'BetweenReferralMatch'\n",
    "# No observations Child Social Work Assessment to Initial Child Protection Conference_text using 'BetweenReferralMatch' or 'BetweenReferralMatch_minus_one_day' \n",
    "# No observations Child Social Work Assessment for Review Child Protection Conference_text using 'BetweenReferralMatch' or 'BetweenReferralMatch_minus_one_day'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only those which are within the referral period minus one day\n",
    "# Drop duplicates\n",
    "print(rq1data_text.shape) \n",
    "rq1data_text_matched = rq1data_text.loc[(rq1data_text['BetweenReferralMatch_minus_one_day'] ==1),]\n",
    "print(rq1data_text_matched.shape)\n",
    "rq1data_text_matched_de_dup = rq1data_text_matched.drop_duplicates(subset=['PSID', 'ReferralDatetime', 'ReferralCloseDate', 'ActualStartDate_str',\n",
    "       'ActualStartDate_txt', 'Child Social Work Assessment for Review Child Protection Conference_text',\n",
    "       'Child Social Work Assessment to Initial Child Protection Conference_text',\n",
    "       'Child Social Work Assessment_text', 'Contact and Referral Form_text'], keep = 'first')\n",
    "print(rq1data_text_matched_de_dup.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contact / referral forms seem to be the day before CSWA => combine as refers to the same incident\n",
    "text_cols = (['Child Social Work Assessment for Review Child Protection Conference_text',\n",
    "       'Child Social Work Assessment to Initial Child Protection Conference_text',\n",
    "       'Child Social Work Assessment_text', 'Contact and Referral Form_text'])\n",
    "\n",
    "\n",
    "rq1data_text_matched_de_dup_concat = rq1data_text_matched_de_dup.groupby(['PSID', 'ReferralDatetime', 'ReferralCloseDate', 'ActualStartDate_str'], as_index = False)[text_cols].agg(' '.join)\n",
    "\n",
    "print(rq1data_text_matched_de_dup.shape)\n",
    "print(rq1data_text_matched_de_dup_concat.shape)\n",
    "rq1data_text_matched_de_dup_concat.sort_values(by = ['PSID', 'ReferralDatetime', 'ReferralCloseDate', 'ActualStartDate_str'], inplace = True)\n",
    "rq1data_text_matched_de_dup_concat.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify text associated with current referral for rq1\n",
    "cols_to_drop_not_referral = ['Child Social Work Assessment for Review Child Protection Conference_text',\n",
    "                            'Child Social Work Assessment to Initial Child Protection Conference_text', \n",
    "                            'Child Social Work Assessment_text']\n",
    "\n",
    "rq1data_text_matched_de_dup_concat.drop(columns = cols_to_drop_not_referral, inplace = True)\n",
    "rq1data_text_matched_de_dup_concat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rq1data: bring in previous text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in just structured data required to select relevant text data\n",
    "# Don't want all structured data too yet as we'll need to have just text data for extracting text features\n",
    "\n",
    "rq1data_text_prev = rq1data[['PSID', 'ReferralDatetime_previous', 'ReferralCloseDate_previous', 'ActualStartDate_previous']].merge(df_combined_all_text, how = 'outer', on = 'PSID')\n",
    "rq1data_text_prev[[ 'Child Social Work Assessment for Review Child Protection Conference_text',\n",
    "       'Child Social Work Assessment to Initial Child Protection Conference_text',\n",
    "       'Child Social Work Assessment_text', 'Contact and Referral Form_text']] = (rq1data_text_prev[[ 'Child Social Work Assessment for Review Child Protection Conference_text',\n",
    "                                                               'Child Social Work Assessment to Initial Child Protection Conference_text',\n",
    "                                                               'Child Social Work Assessment_text', 'Contact and Referral Form_text']].fillna(''))\n",
    "\n",
    "(rq1data_text_prev.rename(columns = {'ActualStartDate':'ActualStartDate_txt_prev',\n",
    "                                 'ActualStartDate_previous':'ActualStartDate_str_prev',\n",
    "                                'ReferralDatetime_previous':'ReferralDatetime_str_prev',\n",
    "                                 'ReferralCloseDate_previous':'ReferralCloseDate_str_prev'},\n",
    "                                inplace = True))\n",
    "\n",
    "rq1data_text_prev['ActualStartDate_str_prev'] = pd.to_datetime(rq1data_text_prev['ActualStartDate_str_prev'], format = \"%Y-%m-%d\")\n",
    "rq1data_text_prev['ReferralDatetime_str_prev'] = pd.to_datetime(rq1data_text_prev['ReferralDatetime_str_prev'], format = \"%Y-%m-%d\")\n",
    "rq1data_text_prev['ReferralCloseDate_str_prev'] = pd.to_datetime(rq1data_text_prev['ReferralCloseDate_str_prev'], format = \"%Y-%m-%d\")\n",
    "\n",
    "# text ActualStartDate is before structured data ActualStartDate and ReferralDatetime (minus 1 day)\n",
    "rq1data_text_prev['BeforeReferralMatch_minus_one_day_prev'] = (np.where((rq1data_text_prev['ActualStartDate_txt_prev'] < (rq1data_text_prev['ReferralDatetime_str_prev']-pd.Timedelta(days = 1))) |\n",
    "                                                 (rq1data_text_prev['ActualStartDate_txt_prev'] < (rq1data_text_prev['ActualStartDate_str_prev']-pd.Timedelta(days = 1))) ,\n",
    "                                                 1,0))\n",
    "\n",
    "rq1data_text_prev['BeforeReferralMatch_minus_one_day_prev'].value_counts()\n",
    "#rq1data_text_prev.loc[rq1data_text_prev['BeforeReferralMatch_minus_one_day_prev']==1,['ActualStartDate_txt_prev', 'ReferralDatetime_str_prev','ActualStartDate_str_prev']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only those which are within the referral period minus one day\n",
    "# Drop duplicates\n",
    "print(rq1data_text_prev.shape) \n",
    "rq1data_text_prev_matched = rq1data_text_prev.loc[(rq1data_text_prev['BeforeReferralMatch_minus_one_day_prev'] ==1),]\n",
    "print(rq1data_text_prev_matched.shape)\n",
    "rq1data_text_prev_matched_de_dup = rq1data_text_prev_matched.drop_duplicates(subset=['PSID', 'ReferralDatetime_str_prev', 'ReferralCloseDate_str_prev',\n",
    "       'ActualStartDate_str_prev','ActualStartDate_txt_prev', 'Child Social Work Assessment for Review Child Protection Conference_text',\n",
    "       'Child Social Work Assessment to Initial Child Protection Conference_text',\n",
    "       'Child Social Work Assessment_text', 'Contact and Referral Form_text'], keep = 'first')\n",
    "print(rq1data_text_prev_matched_de_dup.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contact / referral forms seem to be the day before CSWA => combine as refers to the same incident\n",
    "text_cols = (['Child Social Work Assessment for Review Child Protection Conference_text',\n",
    "       'Child Social Work Assessment to Initial Child Protection Conference_text',\n",
    "       'Child Social Work Assessment_text', 'Contact and Referral Form_text'])\n",
    "\n",
    "rq1data_text_prev_matched_de_dup_concat = rq1data_text_prev_matched_de_dup.groupby(['PSID', 'ReferralDatetime_str_prev', 'ReferralCloseDate_str_prev',\n",
    "       'ActualStartDate_str_prev'], as_index = False)[text_cols].agg(' '.join)\n",
    "\n",
    "print(rq1data_text_prev_matched_de_dup.shape)\n",
    "print(rq1data_text_prev_matched_de_dup_concat.shape)\n",
    "(rq1data_text_prev_matched_de_dup_concat.sort_values(by = ['PSID', 'ReferralDatetime_str_prev', 'ReferralCloseDate_str_prev',\n",
    "       'ActualStartDate_str_prev'], inplace = True))\n",
    "rq1data_text_prev_matched_de_dup_concat.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rq1 - merge together current and previous text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rq1data['ReferralDatetime'] = pd.to_datetime(rq1data['ReferralDatetime'], format = \"%Y-%m-%d\")\n",
    "rq1data[ 'ReferralDatetime_previous'] = pd.to_datetime(rq1data['ReferralDatetime_previous'], format = \"%Y-%m-%d\")\n",
    "rq1data_for_merging = (rq1data[['PSID', 'ReferralDatetime', 'ReferralDatetime_previous']]\n",
    "        .merge(rq1data_text_matched_de_dup_concat, how = 'left', on = ['PSID', 'ReferralDatetime']))\n",
    "\n",
    "rename_prev = {'Child Social Work Assessment for Review Child Protection Conference_text': \n",
    "               'Child Social Work Assessment for Review Child Protection Conference_text_prev',\n",
    "               'Child Social Work Assessment to Initial Child Protection Conference_text':\n",
    "               'Child Social Work Assessment to Initial Child Protection Conference_text_prev',\n",
    "               'Child Social Work Assessment_text': 'Child Social Work Assessment_text_prev', \n",
    "               'Contact and Referral Form_text': 'Contact and Referral Form_text_prev'}\n",
    "\n",
    "rq1data_text_prev_matched_de_dup_concat.rename(columns = rename_prev, inplace = True)\n",
    "\n",
    "rq1data_text_for_anonymisation = (rq1data_for_merging.merge(rq1data_text_prev_matched_de_dup_concat, how = 'left', \n",
    "                                                 left_on = ['PSID', 'ReferralDatetime_previous'], right_on =  ['PSID', 'ReferralDatetime_str_prev']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Contact and Referral Form_text' and 'Contact and Referral Form_text_prev' are different except a small no. of cases\n",
    "# Most probably just copied across by social worker\n",
    "#rq1data_text_for_anonymisation.sort_values(by = ['PSID', 'ReferralDatetime'], inplace = True)\n",
    "#rq1data_text_for_anonymisation[['PSID', 'ReferralDatetime', 'ReferralDatetime_previous', 'Contact and Referral Form_text_prev', 'Contact and Referral Form_text']].head(50)\n",
    "(rq1data_text_for_anonymisation['Contact and Referral Form_text_prev'] == rq1data_text_for_anonymisation['Contact and Referral Form_text']).value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting just text columns and merging keys\n",
    "date_cols_for_selecting = (['ReferralCloseDate', 'ActualStartDate_str',\n",
    "        'ReferralDatetime_str_prev','ReferralCloseDate_str_prev', 'ActualStartDate_str_prev'])\n",
    "\n",
    "rq1data_text_for_anonymisation.drop(columns = date_cols_for_selecting, inplace = True)\n",
    "\n",
    "print(rq1data_text_for_anonymisation.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Child Social Work Assessment for Review Child Protection Conference_text_prev and\n",
    "# Child Social Work Assessment to Initial Child Protection Conference_text_prev is better with improved way of \n",
    "# merging previous\n",
    "for col in rq1data_text_for_anonymisation.columns:\n",
    "    print(col)\n",
    "    print(rq1data_text_for_anonymisation[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle final text data to feed into anonymisation\n",
    "import pickle\n",
    "with open(\"Created\\\\rq1data_text_for_anonymisation.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(rq1data_text_for_anonymisation, handle, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine with rq2data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 1==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "os.chdir(\"C:\\\\Users\\\\[username]\\\\Downloads\\\\Updated Text Data\") # insert [username]\n",
    "filename = open(\"Created\\\\df_combined_all_text.pkl\", \"rb\")\n",
    "df_combined_all_text = pickle.load(filename)\n",
    "print(df_combined_all_text.shape)\n",
    "\n",
    "rq2data = pd.read_csv(\"..\\\\Updated Structured Data\\\\Created\\\\rq2data_w_previous.csv\", index_col = 0)\n",
    "print(rq2data.shape)\n",
    "#print(rq2data.index)\n",
    "#print(rq2data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in just structured data required to select relevant text data\n",
    "# Don't want all structured data too yet as we'll need to have just text data for extracting text features\n",
    "\n",
    "rq2data_text = rq2data[['PSID', 'ReferralDatetime', 'ReferralCloseDate', 'ActualStartDate', 'AssessmentType']].merge(df_combined_all_text, how = 'outer', on = 'PSID')\n",
    "rq2data_text[[ 'Child Social Work Assessment for Review Child Protection Conference_text',\n",
    "       'Child Social Work Assessment to Initial Child Protection Conference_text',\n",
    "       'Child Social Work Assessment_text', 'Contact and Referral Form_text']] = (rq2data_text[[ 'Child Social Work Assessment for Review Child Protection Conference_text',\n",
    "                                                               'Child Social Work Assessment to Initial Child Protection Conference_text',\n",
    "                                                               'Child Social Work Assessment_text', 'Contact and Referral Form_text']].fillna(''))\n",
    "(rq2data_text.rename(columns = {'ActualStartDate_x':'ActualStartDate_str',\n",
    "                                 'ActualStartDate_y':'ActualStartDate_txt'},\n",
    "                                inplace = True))\n",
    "\n",
    "rq2data_text['ActualStartDate_str'] = pd.to_datetime(rq2data_text['ActualStartDate_str'], format = \"%Y-%m-%d\")\n",
    "rq2data_text['ReferralDatetime'] = pd.to_datetime(rq2data_text['ReferralDatetime'], format = \"%Y-%m-%d\")\n",
    "rq2data_text['ReferralCloseDate'] = pd.to_datetime(rq2data_text['ReferralCloseDate'], format = \"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# If the text actual start date is the same or after the structured data actual start date or the referral date minus 1 BUT..\n",
    "# within 2 weeks of the referral \n",
    "# Checks indicated the same pattern as for rq1\n",
    "# Only include text within first 2 weeks of referral if predicting escalation 3 months out\n",
    "\n",
    "rq2data_text['Twoweekssafterref'] = rq2data_text['ReferralDatetime'] + pd.Timedelta(days = 14)\n",
    "\n",
    "rq2data_text['Match_minus_one_day_Twoweekssafterass'] = (np.where((((rq2data_text['ActualStartDate_txt'] - rq2data_text['ReferralDatetime']).dt.days >=-1) & \n",
    "                                                 (rq2data_text['ActualStartDate_txt'] <= rq2data_text['Twoweekssafterref'])) |\n",
    "                                                 (((rq2data_text['ActualStartDate_txt'] - rq2data_text['ActualStartDate_str']).dt.days >=-1) & \n",
    "                                                 (rq2data_text['ActualStartDate_txt'] <= rq2data_text['Twoweekssafterref'])) ,\n",
    "                                                 1,0))\n",
    "\n",
    "print(rq2data_text['Match_minus_one_day_Twoweekssafterass'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only those which are within two weeks of the referral minus one day\n",
    "# Drop duplicates\n",
    "print(rq2data_text.shape) \n",
    "rq2data_text_matched = rq2data_text.loc[(rq2data_text['Match_minus_one_day_Twoweekssafterass'] ==1),]\n",
    "print(rq2data_text_matched.shape)\n",
    "rq2data_text_matched_de_dup = rq2data_text_matched.drop_duplicates(subset=['PSID', 'ReferralDatetime', 'ReferralCloseDate', 'ActualStartDate_str',\n",
    "       'ActualStartDate_txt', 'AssessmentType', 'Child Social Work Assessment for Review Child Protection Conference_text',\n",
    "       'Child Social Work Assessment to Initial Child Protection Conference_text',\n",
    "       'Child Social Work Assessment_text', 'Contact and Referral Form_text'], keep = 'first')\n",
    "print(rq2data_text_matched_de_dup.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contact / referral forms seem to be the day before CSWA => combine as refers to the same incident\n",
    "text_cols = (['Child Social Work Assessment for Review Child Protection Conference_text',\n",
    "       'Child Social Work Assessment to Initial Child Protection Conference_text',\n",
    "       'Child Social Work Assessment_text', 'Contact and Referral Form_text'])\n",
    "\n",
    "\n",
    "rq2data_text_matched_de_dup_concat = rq2data_text_matched_de_dup.groupby(['PSID', 'ReferralDatetime', 'ReferralCloseDate', 'ActualStartDate_str', 'AssessmentType'], as_index = False)[text_cols].agg(' '.join)\n",
    "\n",
    "print(rq2data_text_matched_de_dup.shape)\n",
    "print(rq2data_text_matched_de_dup_concat.shape)\n",
    "rq2data_text_matched_de_dup_concat.sort_values(by = ['PSID', 'ReferralDatetime', 'ReferralCloseDate', 'ActualStartDate_str'], inplace = True)\n",
    "rq2data_text_matched_de_dup_concat.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rq2data: bring in previous text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in just structured data required to select relevant text data\n",
    "# Don't want all structured data too yet as we'll need to have just text data for extracting text features\n",
    "\n",
    "rq2data_cols_for_merge = (['PSID', 'ReferralDatetime_previous', 'ReferralCloseDate_previous', 'ActualStartDate_previous', \n",
    "                            'previous_count_AssessmentType_ Child Social Work Assessment',\n",
    " 'previous_count_AssessmentType_ Child Social Work Assessment for Review Child Protection Conference',\n",
    " 'previous_count_AssessmentType_ Child Social Work Assessment to Initial Child Protection Conference'])\n",
    "\n",
    "\n",
    "rq2data_text_prev = rq2data[rq2data_cols_for_merge].merge(df_combined_all_text, how = 'outer', on = 'PSID')\n",
    "rq2data_text_prev[[ 'Child Social Work Assessment for Review Child Protection Conference_text',\n",
    "       'Child Social Work Assessment to Initial Child Protection Conference_text',\n",
    "       'Child Social Work Assessment_text', 'Contact and Referral Form_text']] = (rq2data_text_prev[[ 'Child Social Work Assessment for Review Child Protection Conference_text',\n",
    "                                                               'Child Social Work Assessment to Initial Child Protection Conference_text',\n",
    "                                                               'Child Social Work Assessment_text', 'Contact and Referral Form_text']].fillna(''))\n",
    "\n",
    "(rq2data_text_prev.rename(columns = {'ActualStartDate':'ActualStartDate_txt_prev',\n",
    "                                 'ActualStartDate_previous':'ActualStartDate_str_prev',\n",
    "                                'ReferralDatetime_previous':'ReferralDatetime_str_prev',\n",
    "                                 'ReferralCloseDate_previous':'ReferralCloseDate_str_prev'},\n",
    "                                inplace = True))\n",
    "\n",
    "rq2data_text_prev['ActualStartDate_str_prev'] = pd.to_datetime(rq2data_text_prev['ActualStartDate_str_prev'], format = \"%Y-%m-%d\")\n",
    "rq2data_text_prev['ReferralDatetime_str_prev'] = pd.to_datetime(rq2data_text_prev['ReferralDatetime_str_prev'], format = \"%Y-%m-%d\")\n",
    "rq2data_text_prev['ReferralCloseDate_str_prev'] = pd.to_datetime(rq2data_text_prev['ReferralCloseDate_str_prev'], format = \"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "rq2data_text_prev['BeforeReferralMatch_minus_one_day_prev'] = (np.where((rq2data_text_prev['ActualStartDate_txt_prev'] < (rq2data_text_prev['ReferralDatetime_str_prev']-pd.Timedelta(days = 1))) |\n",
    "                                                 (rq2data_text_prev['ActualStartDate_txt_prev'] < (rq2data_text_prev['ActualStartDate_str_prev']-pd.Timedelta(days = 1))) ,\n",
    "                                                 1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only those which are within the referral period minus one day\n",
    "# Drop duplicates\n",
    "print(rq2data_text_prev.shape) \n",
    "rq2data_text_prev_matched = rq2data_text_prev.loc[(rq2data_text_prev['BeforeReferralMatch_minus_one_day_prev'] ==1),]\n",
    "print(rq2data_text_prev_matched.shape)\n",
    "rq2data_text_prev_matched_de_dup = rq2data_text_prev_matched.drop_duplicates(subset=['PSID', 'ReferralDatetime_str_prev', 'ReferralCloseDate_str_prev',\n",
    "       'ActualStartDate_str_prev','ActualStartDate_txt_prev', 'previous_count_AssessmentType_ Child Social Work Assessment',\n",
    " 'previous_count_AssessmentType_ Child Social Work Assessment for Review Child Protection Conference',\n",
    " 'previous_count_AssessmentType_ Child Social Work Assessment to Initial Child Protection Conference',\n",
    "  'Child Social Work Assessment for Review Child Protection Conference_text',\n",
    "       'Child Social Work Assessment to Initial Child Protection Conference_text',\n",
    "       'Child Social Work Assessment_text', 'Contact and Referral Form_text'], keep = 'first')\n",
    "print(rq2data_text_prev_matched_de_dup.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contact / referral forms seem to be the day before CSWA => combine as refers to the same incident\n",
    "text_cols = (['Child Social Work Assessment for Review Child Protection Conference_text',\n",
    "       'Child Social Work Assessment to Initial Child Protection Conference_text',\n",
    "       'Child Social Work Assessment_text', 'Contact and Referral Form_text'])\n",
    "\n",
    "rq2data_text_prev_matched_de_dup_concat = rq2data_text_prev_matched_de_dup.groupby(['PSID', 'ReferralDatetime_str_prev', 'ReferralCloseDate_str_prev',\n",
    "       'ActualStartDate_str_prev', 'previous_count_AssessmentType_ Child Social Work Assessment',\n",
    "       'previous_count_AssessmentType_ Child Social Work Assessment for Review Child Protection Conference',\n",
    "       'previous_count_AssessmentType_ Child Social Work Assessment to Initial Child Protection Conference'], as_index = False)[text_cols].agg(' '.join)\n",
    "\n",
    "print(rq2data_text_prev_matched_de_dup.shape)\n",
    "print(rq2data_text_prev_matched_de_dup_concat.shape)\n",
    "(rq2data_text_prev_matched_de_dup_concat.sort_values(by = ['PSID', 'ReferralDatetime_str_prev', 'ReferralCloseDate_str_prev',\n",
    "       'ActualStartDate_str_prev'], inplace = True))\n",
    "rq2data_text_prev_matched_de_dup_concat.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rq2 - merge together current and previous text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rq2data['ReferralDatetime'] = pd.to_datetime(rq2data['ReferralDatetime'], format = \"%Y-%m-%d\")\n",
    "rq2data_for_merging = (rq2data[['PSID', 'ReferralDatetime', 'ReferralDatetime_previous']]\n",
    "        .merge(rq2data_text_matched_de_dup_concat, how = 'left', on = ['PSID', 'ReferralDatetime']))\n",
    "\n",
    "rename_prev = {'Child Social Work Assessment for Review Child Protection Conference_text': \n",
    "               'Child Social Work Assessment for Review Child Protection Conference_text_prev',\n",
    "               'Child Social Work Assessment to Initial Child Protection Conference_text':\n",
    "               'Child Social Work Assessment to Initial Child Protection Conference_text_prev',\n",
    "               'Child Social Work Assessment_text': 'Child Social Work Assessment_text_prev', \n",
    "               'Contact and Referral Form_text': 'Contact and Referral Form_text_prev'}\n",
    "\n",
    "rq2data_text_prev_matched_de_dup_concat.rename(columns = rename_prev, inplace = True)\n",
    "\n",
    "rq2data_text_for_anonymisation = (rq2data_for_merging.merge(rq2data_text_prev_matched_de_dup_concat, how = 'left', \n",
    "                                                 left_on = ['PSID', 'ReferralDatetime'], right_on =  ['PSID', 'ReferralDatetime_str_prev']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting just text columns and merging keys\n",
    "date_cols_for_selecting = (['ReferralDatetime_previous','ReferralCloseDate', 'ActualStartDate_str',\n",
    "                            'ReferralDatetime_str_prev', 'ReferralCloseDate_str_prev',\n",
    "                           'ActualStartDate_str_prev'])\n",
    "\n",
    "assessments_cols_for_selecting = (['AssessmentType', 'previous_count_AssessmentType_ Child Social Work Assessment',\n",
    "       'previous_count_AssessmentType_ Child Social Work Assessment for Review Child Protection Conference',\n",
    "       'previous_count_AssessmentType_ Child Social Work Assessment to Initial Child Protection Conference'])\n",
    "\n",
    "cols_for_selecting = date_cols_for_selecting + assessments_cols_for_selecting\n",
    "\n",
    "rq2data_text_for_anonymisation.drop(columns = cols_for_selecting, inplace = True)\n",
    "\n",
    "print(rq2data_text_for_anonymisation.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in rq2data_text_for_anonymisation.columns:\n",
    "    print(col)\n",
    "    print(rq2data_text_for_anonymisation[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle final text data to feed into anonymisation\n",
    "import pickle\n",
    "\n",
    "with open(\"Created\\\\rq2data_text_for_anonymisation.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(rq2data_text_for_anonymisation, handle, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
