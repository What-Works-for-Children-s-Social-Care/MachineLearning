{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This covers:\n",
    "\n",
    "- Hyperparameters; timing\n",
    "- Feature importance plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable set-up\n",
    "LA = 'LA2'\n",
    "rq = 'rq1' # Options: 'rq1', 'rq2'\n",
    "cv = 'ss' # Options: 'ts' (time series split, ignore siblings), 'ss' (stratified shuffle, ignore siblings)\n",
    "data_type = 'str' # Options: 'str' (just structured data), 'all' (structured data and list of strings)\n",
    "\n",
    "\n",
    "file_stub = rq + '_' + cv + '_' + data_type # Creates file stub for saving in the format e.g. rq1_ss_str\n",
    "\n",
    "if LA =='LA1':\n",
    "    input_folder = '../LA1 Results July 2020'\n",
    "    output_folder = 'LA1'\n",
    "if LA =='LA2':\n",
    "    input_folder = '../Final transfer out data and code to WWC Jan 2020'\n",
    "    output_folder = 'LA2'\n",
    "if LA =='LA3':\n",
    "    input_folder = '../Anonymised structured data'\n",
    "    output_folder = 'LA3'\n",
    "if LA =='LA4':\n",
    "    input_folder = '../LA4 August 2020'\n",
    "    output_folder = 'LA4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in years and sample sizes\n",
    "import pandas as pd\n",
    "years_sample_sizes = pd.read_csv(\"../Summary information FINAL/Years and Sample Sizes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which is the best algorithm\n",
    "import pickle\n",
    "\n",
    "if LA == 'LA1':\n",
    "    best_algorithm = 'gradient_boosting'\n",
    "\n",
    "else:\n",
    "    filename = open(\"{}/Models/best_estimator_{}_maxmin.pkl\".format(input_folder, file_stub), 'rb')\n",
    "    best_estimator_maxmin = pickle.load(filename)\n",
    "\n",
    "    if 'dtc' in best_estimator_maxmin.named_steps.keys():\n",
    "        best_algorithm = 'decision_tree'\n",
    "        best_params = [p for p in best_estimator_maxmin.get_params() if 'dtc__' in p]\n",
    "    elif 'lr' in best_estimator_maxmin.named_steps.keys():\n",
    "        best_algorithm = 'logistic_regression'  \n",
    "        best_params = [p for p in best_estimator_maxmin.get_params() if 'lr__' in p]\n",
    "    elif 'gbc' in best_estimator_maxmin.named_steps.keys():\n",
    "        best_algorithm = 'gradient_boosting'  \n",
    "        best_params = [p for p in best_estimator_maxmin.get_params() if 'gbc__' in p]\n",
    "\n",
    "    print(best_algorithm)\n",
    "    print(LA)\n",
    "    print(file_stub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 1==2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%script false --no-raise-error\n",
    "## Mean and SD average precision in training\n",
    "# Use model_output csvs\n",
    "# Already available for LA1, LA2, LA3 and available for LA4 after final extract\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "file_list = glob.glob(\"{}/Models/model_output_{}_*.csv\".format(input_folder, file_stub))\n",
    "\n",
    "file_dict = {}\n",
    "for file_name in file_list:\n",
    "    file = pd.read_csv(file_name, index_col=0)\n",
    "    file_name = file_name.replace(\"{}/Models/model_output_{}_\".format(input_folder, file_stub), \"\")\n",
    "    if LA == 'LA2':\n",
    "        file_name = str(file_name).replace('_50_2', '')\n",
    "    else:\n",
    "        file_name = file_name\n",
    "    file_name = file_name.replace(\".csv\", \"\")\n",
    "    file_dict[file_name] = file\n",
    "    \n",
    "print(file_dict.keys())\n",
    "model_output = pd.concat([file_dict['decision_tree'], file_dict['gradient_boosting'], file_dict['logistic_regression']], axis = 0, ignore_index = True)\n",
    "print(model_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%script false --no-raise-error\n",
    "# Range of hyperparameters and the best ones\n",
    "hyperparameters_df = pd.DataFrame(data = {}, index = ['Minimum', 'Maximum', 'Best'])\n",
    "for file_name in file_dict.keys():\n",
    "    hyperparameter_cols = [col for col in file_dict[file_name] if 'param_' in col]\n",
    "    for col in hyperparameter_cols:\n",
    "        col_name = str(file_name) + '_' + str(col).replace('param_', '')\n",
    "        print(col_name)\n",
    "        hyperparameters_df.at['Minimum', col_name] = file_dict[file_name][col].min()\n",
    "        hyperparameters_df.at['Maximum', col_name] = file_dict[file_name][col].max()\n",
    "        # Use the hyperparameters for the final model for the best algorithm\n",
    "        if file_name == best_algorithm:\n",
    "            col_shortened = str(col).replace('param_', '')\n",
    "            hyperparameters_df.at['Best', col_name] = best_estimator_maxmin.get_params()[col_shortened]\n",
    "        # Use the hyperparameters for the max mean test score for the other two algorithms\n",
    "        else:\n",
    "            hyperparameters_df.at['Best', col_name] = file_dict[file_name].loc[file_dict[file_name]['rank_test_score'] ==1,col].values[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%script false --no-raise-error\n",
    "# Check whether there's any columns not covered (perhaps where we keep the text columns separate)\n",
    "parameters_all_text = {'decision_tree_dtc__max_depth', 'decision_tree_dtc__max_features',\n",
    "       'decision_tree_dtc__min_samples_split',\n",
    "       'decision_tree_preprocessor__All_text__tfidf__max_features',\n",
    "       'decision_tree_preprocessor__All_text__tf_lda__lda__max_iter',\n",
    "       'decision_tree_preprocessor__All_text__tf_lda__lda__n_components',\n",
    "       'decision_tree_preprocessor__All_text__tfidf_lda__tfidf__max_features',\n",
    "       'logistic_regression_lr__C', 'logistic_regression_lr__penalty',\n",
    "       'logistic_regression_preprocessor__All_text__tfidf__max_features',\n",
    "       'logistic_regression_preprocessor__All_text__tf_lda__lda__max_iter',\n",
    "       'logistic_regression_preprocessor__All_text__tf_lda__lda__n_components',\n",
    "       'logistic_regression_preprocessor__All_text__tfidf_lda__tfidf__max_features',\n",
    "       'gradient_boosting_gbc__max_depth',\n",
    "       'gradient_boosting_gbc__max_features',\n",
    "       'gradient_boosting_gbc__n_estimators',\n",
    "       'gradient_boosting_preprocessor__All_text__tfidf__max_features',\n",
    "       'gradient_boosting_preprocessor__All_text__tf_lda__lda__max_iter',\n",
    "       'gradient_boosting_preprocessor__All_text__tf_lda__lda__n_components',\n",
    "       'gradient_boosting_preprocessor__All_text__tfidf_lda__tfidf__max_features', \n",
    "        'decision_tree_preprocessor__All_text__tf_lda__tf__max_df',\n",
    " 'decision_tree_preprocessor__All_text__tfidf__max_df',\n",
    " 'gradient_boosting_preprocessor__All_text__tf_lda__tf__max_df',\n",
    " 'gradient_boosting_preprocessor__All_text__tfidf__max_df',\n",
    " 'logistic_regression_preprocessor__All_text__tf_lda__tf__max_df',\n",
    " 'logistic_regression_preprocessor__All_text__tfidf__max_df'}\n",
    "\n",
    "\n",
    "assert len(set(hyperparameters_df.columns).difference(parameters_all_text)) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%script false --no-raise-error\n",
    "hyperparameters_df.rename(columns = {\n",
    "    'decision_tree_dtc__max_depth': 'Decision tree: maximum depth', \n",
    "    'decision_tree_dtc__max_features': 'Decision tree: maximum features',\n",
    "   'decision_tree_dtc__min_samples_split': 'Decision tree: minimum sample split',\n",
    "   'decision_tree_preprocessor__All_text__tfidf__max_features': 'Decision tree: text - term frequency inverse document frequency maximum features',\n",
    "   'decision_tree_preprocessor__All_text__tf_lda__lda__max_iter': 'Decision tree: text - latent dirichlet allocation - maximum iterations',\n",
    "   'decision_tree_preprocessor__All_text__tf_lda__lda__n_components': 'Decision tree: text - latent dirichlet allocation - number of components',\n",
    "   'decision_tree_preprocessor__All_text__tfidf_lda__tfidf__max_features': 'Decision tree: text - term frequency inverse document frequency maximum features to feed into latent dirichlet allocation',\n",
    "   'logistic_regression_lr__C': 'Logistic regression: inverse of the regularisation strength', \n",
    "    'logistic_regression_lr__penalty': 'Logistic regression: penalisation norm',\n",
    "   'logistic_regression_preprocessor__All_text__tfidf__max_features': 'Logistic regression: text - term frequency inverse document frequency maximum features',\n",
    "   'logistic_regression_preprocessor__All_text__tf_lda__lda__max_iter': 'Logistic regression: text - latent dirichlet allocation - maximum iterations',\n",
    "   'logistic_regression_preprocessor__All_text__tf_lda__lda__n_components': 'Logistic regression: text - latent dirichlet allocation - number of components',\n",
    "   'logistic_regression_preprocessor__All_text__tfidf_lda__tfidf__max_features': 'Logistic regression: text - term frequency inverse document frequency maximum features to feed into latent dirichlet allocation',\n",
    "   'gradient_boosting_gbc__max_depth': 'Gradient boosting: maximum depth',\n",
    "   'gradient_boosting_gbc__max_features': 'Gradient boosting: maximum features',\n",
    "   'gradient_boosting_gbc__n_estimators': 'Gradient boosting: number of estimators',\n",
    "   'gradient_boosting_preprocessor__All_text__tfidf__max_features': 'Gradient boosting: text - term frequency inverse document frequency maximum features',\n",
    "   'gradient_boosting_preprocessor__All_text__tf_lda__lda__max_iter': 'Gradient boosting: text - latent dirichlet allocation - maximum iterations',\n",
    "   'gradient_boosting_preprocessor__All_text__tf_lda__lda__n_components': 'Gradient boosting: text - latent dirichlet allocation - number of components',\n",
    "   'gradient_boosting_preprocessor__All_text__tfidf_lda__tfidf__max_features': 'Gradient boosting: text - term frequency inverse document frequency maximum features to feed into latent dirichlet allocation',\n",
    "    'decision_tree_preprocessor__All_text__tf_lda__tf__max_df': 'Decision tree: text - maximum term frequency for terms in topic modelling',\n",
    " 'decision_tree_preprocessor__All_text__tfidf__max_df': 'Decision tree: text - maximum term frequency for term frequency inverse document frequency matrix',\n",
    " 'gradient_boosting_preprocessor__All_text__tf_lda__tf__max_df': 'Gradient boosting: text - maximum term frequency for terms in topic modelling',\n",
    " 'gradient_boosting_preprocessor__All_text__tfidf__max_df': 'Gradient boosting: text - maximum term frequency for term frequency inverse document frequency matrix',\n",
    " 'logistic_regression_preprocessor__All_text__tf_lda__tf__max_df': 'Logistic regression: text - maximum term frequency for terms in topic modelling',\n",
    " 'logistic_regression_preprocessor__All_text__tfidf__max_df': 'Logistic regression: text - maximum term frequency for term frequency inverse document frequency matrix'\n",
    "                                 }, \n",
    "                               inplace = True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round\n",
    "numeric_cols = list(hyperparameters_df.select_dtypes(include='number').columns)\n",
    "numeric_cols.remove('Logistic regression: inverse of the regularisation strength')\n",
    "hyperparameters_df[numeric_cols] = hyperparameters_df[numeric_cols].round(1)\n",
    "hyperparameters_df['Logistic regression: inverse of the regularisation strength'] = hyperparameters_df['Logistic regression: inverse of the regularisation strength'].round(4)\n",
    "hyperparameters_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to allow the tables to fit to a page\n",
    "hyperparameters_df_dtc = hyperparameters_df[[col for col in hyperparameters_df.columns if 'Decision tree' in col]]\n",
    "hyperparameters_df_lr = hyperparameters_df[[col for col in hyperparameters_df.columns if 'Logistic regression' in col]]\n",
    "hyperparameters_df_gb = hyperparameters_df[[col for col in hyperparameters_df.columns if 'Gradient boosting' in col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%script false --no-raise-error\n",
    "if (data_type == 'str') or ((LA == 'LA2') and (data_type == 'all')) or ((LA == 'LA3') and (data_type == 'all')):\n",
    "    print(LA)\n",
    "    print(data_type)\n",
    "    # Get the order right\n",
    "    hyperparameters_df = pd.concat([hyperparameters_df_dtc, hyperparameters_df_lr, hyperparameters_df_gb], axis=1)\n",
    "    hyperparameters_df.to_csv(\"{}/Hyperparameters {}.csv\".format(output_folder, file_stub))\n",
    "    \n",
    "else:\n",
    "    hyperparameters_df_dtc.to_csv(\"{}/Hyperparameters dtc {}.csv\".format(output_folder, file_stub))\n",
    "    hyperparameters_df_lr.to_csv(\"{}/Hyperparameters lr {}.csv\".format(output_folder, file_stub))\n",
    "    hyperparameters_df_gb.to_csv(\"{}/Hyperparameters gb {}.csv\".format(output_folder, file_stub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 1==2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean training time\n",
    "import math\n",
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# corrected\n",
    "if cv=='ss' and LA=='LA4':\n",
    "    num_cv=3\n",
    "elif cv=='ss':\n",
    "    num_cv=5\n",
    "elif cv=='ts':\n",
    "    num_cv=3\n",
    "rank_1_iteration_dict = {}\n",
    "for file_name in file_dict.keys():\n",
    "    print(file_name)\n",
    "    rank_1_iteration = file_dict[file_name].loc[file_dict[file_name]['rank_test_score'] == 1,]\n",
    "    rank_1_iteration_dict[file_name] = rank_1_iteration\n",
    "    df.at[file_name, 'Average training time (seconds)'] = round(rank_1_iteration['mean_fit_time'].values[0], 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_names_dict = {'gradient_boosting': 'Gradient Boosting', 'logistic_regression': 'Logistic Regression',\n",
    "                'decision_tree': 'Decision Tree'}\n",
    "\n",
    "df.rename(index=algorithm_names_dict, inplace = True)\n",
    "print(df)\n",
    "df.to_csv(\"{}/Training time {}.csv\".format(output_folder, file_stub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 1==2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable set-up\n",
    "LA = 'LA4'\n",
    "rq = 'rq2' # Options: 'rq1', 'rq2'\n",
    "cv = 'ts' # Options: 'ts' (time series split, ignore siblings), 'ss' (stratified shuffle, ignore siblings)\n",
    "data_type = 'str' # Options: 'str' (just structured data), 'all' (structured data and list of strings)\n",
    "\n",
    "\n",
    "file_stub = rq + '_' + cv + '_' + data_type # Creates file stub for saving in the format e.g. rq1_ss_str\n",
    "\n",
    "if LA =='LA1':\n",
    "    input_folder = '../LA1 Results July 2020'\n",
    "    output_folder = 'LA1'\n",
    "if LA =='LA2':\n",
    "    input_folder = '../Final transfer out data and code to WWC Jan 2020'\n",
    "    output_folder = 'LA2'\n",
    "if LA =='LA3':\n",
    "    input_folder = '../Anonymised structured data'\n",
    "    output_folder = 'LA3'\n",
    "if LA =='LA4':\n",
    "    input_folder = '../LA4 August 2020'\n",
    "    output_folder = 'LA4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "if data_type != 'all':\n",
    "    df_feature_imp = pd.read_csv('{}/Models/feature_importance_{}.csv'.format(input_folder, file_stub))\n",
    "    df_feature_imp.sort_values(by = 'feature_importance', ascending = False)[0:20]\n",
    "\n",
    "    df_feature_imp['feature_importance'] = df_feature_imp['feature_importance'].abs()\n",
    "    df_feature_imp.sort_values(by = 'feature_importance', ascending = False, inplace = True)\n",
    "    df_feature_imp_best_algorithm = df_feature_imp[df_feature_imp['algorithm'] == best_algorithm] \n",
    "    df_feature_imp_best_algorithm.reset_index(inplace = True, drop = True)\n",
    "    df_feature_imp_best_algorithm = df_feature_imp_best_algorithm.loc[df_feature_imp_best_algorithm['feature_importance']>0,]\n",
    "    feature_names = df_feature_imp_best_algorithm.loc[0:19,'column_name']\n",
    "    feature_importances = round(df_feature_imp_best_algorithm.loc[0:19,'feature_importance'],4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename features\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "feature_labels = pd.read_csv('Graphs Analysis - All variables.csv')\n",
    "feature_names_df = pd.DataFrame(feature_names).merge(feature_labels[['Variable Name', 'Name of variable in the report']], how = 'left', left_on = 'column_name', right_on = 'Variable Name')\n",
    "feature_names_df.drop_duplicates(subset = 'column_name', inplace = True)\n",
    "\n",
    "print(feature_names_df.loc[feature_names_df['Name of variable in the report'].isna(), 'column_name'].unique())\n",
    "assert feature_names_df['Name of variable in the report'].isna().sum() == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_df.to_csv('{}/Feature names for summary stats {}.csv'.format(output_folder, file_stub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 1==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try some automatic renaming\n",
    "if feature_names_df['Name of variable in the report'].isna().sum() != 0:\n",
    "\n",
    "    feature_names_df['Name of variable in the report_all'] = feature_names_df['Name of variable in the report'].fillna(feature_names_df['column_name'])\n",
    "    feature_names_list = [n.replace('previous_exc_current_sum', 'Total Number of Previous') for n in feature_names_df['Name of variable in the report_all']]\n",
    "    feature_names_list = [n.replace('previous_exc_current_mean', 'Average Number of Previous') for n in feature_names_list]\n",
    "\n",
    "    feature_names_list = [n.title() for n in feature_names_list]\n",
    "    feature_names_list = [n.replace('_', ': ') for n in feature_names_list]\n",
    "    feature_names_list = [n.replace('.', ' ') for n in feature_names_list]\n",
    "\n",
    "    word_list = ['date', 'time', 'source', 'code', 'start', 'social', 'work',\n",
    "                 'assessment', 'completion', 'days', 'referral', 'care', 'reason',\n",
    "                'legal', 'status', 'need', 'abuse', 'category', 'of', 'cp', 'length',\n",
    "                'close', 'contact']\n",
    "    for w in word_list:\n",
    "        feature_names_list = [re.sub('{}'.format(w), ' {}'.format(w), t) for t in feature_names_list]\n",
    "    feature_names_list = pd.Series(feature_names_list)\n",
    "    feature_names_list.shape\n",
    "else:\n",
    "    feature_names_list = pd.Series(feature_names_df['Name of variable in the report'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction question\n",
    "if (LA =='LA1') and (rq == 'rq1'):\n",
    "    prediction_qu = '1'\n",
    "elif (LA =='LA1') and (rq == 'rq2'):\n",
    "    prediction_qu = '2'   \n",
    "elif (LA =='LA2') and (rq == 'rq1'):\n",
    "    prediction_qu = '3' \n",
    "elif (LA =='LA2') and (rq == 'rq2'):\n",
    "    prediction_qu = '4' \n",
    "elif (LA =='LA3') and (rq == 'rq1'):\n",
    "    prediction_qu = '5' \n",
    "elif (LA =='LA3') and (rq == 'rq2'):\n",
    "    prediction_qu = '6' \n",
    "elif (LA =='LA4') and (rq == 'rq1'):\n",
    "    prediction_qu = '7' \n",
    "elif (LA =='LA4') and (rq == 'rq2'):\n",
    "    prediction_qu = '8' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify source and base\n",
    "dict_names = {'ss': 'learning from all cases',\n",
    "             'ts': 'learning just from earlier cases',\n",
    "             'str': 'structured data only',\n",
    "             'all': 'structured and text data'}\n",
    "\n",
    "\n",
    "outcome = years_sample_sizes.loc[(years_sample_sizes['Local authority'] == LA) &\n",
    "                       (years_sample_sizes['Research question'] == rq) & \n",
    "                        (years_sample_sizes['Cross-Validation'] == cv),'Shortened outcome'].values[0] \n",
    "\n",
    "years = years_sample_sizes.loc[(years_sample_sizes['Local authority'] == LA) &\n",
    "                       (years_sample_sizes['Research question'] == rq) & \n",
    "                        (years_sample_sizes['Cross-Validation'] == cv),'Years'].values[0]\n",
    "\n",
    "sample_sizes = years_sample_sizes.loc[(years_sample_sizes['Local authority'] == LA) &\n",
    "                       (years_sample_sizes['Research question'] == rq) &\n",
    "                       (years_sample_sizes['Cross-Validation'] == cv), 'Sample size'].values[0]\n",
    "\n",
    "txt_source = 'Prediction: {}'.format(outcome)\n",
    "print(txt_source)\n",
    "txt_model_desc = 'Model: {}, {}'.format(dict_names[cv], dict_names[data_type])\n",
    "print(txt_model_desc)\n",
    "LA_num = LA.replace('LA', '')\n",
    "txt_base = 'Data: {}, {}, N = {}'.format('Local authority {}'.format(LA_num), years, sample_sizes)\n",
    "print(txt_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from textwrap import wrap\n",
    "\n",
    "print(feature_names_list.shape[0])\n",
    "print(feature_importances.shape[0])\n",
    "assert (feature_names_list.shape[0] == feature_importances.shape[0])\n",
    "\n",
    "# Wrap long feature names\n",
    "feature_names_list = pd.Series([ '\\n'.join(wrap(f, 100)) for f in feature_names_list])\n",
    "\n",
    "if data_type != 'all':\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.barh(feature_names_list, feature_importances, color='#ff7057')\n",
    "    ax.tick_params(axis='both', which='both', length=0)\n",
    "    ax.set_yticks(feature_names_list)\n",
    "    x_ticks = [round(n,4) for n in np.linspace(0,max(feature_importances)+0.1*max(feature_importances),5)]\n",
    "    ax.set_xticks(x_ticks)\n",
    "    ax.set_yticklabels(feature_names_list, fontname=\"Arial\", color='#4d4d51', fontsize=12)\n",
    "    ax.set_xticklabels(x_ticks, fontname=\"Arial\", color='#4d4d51', fontsize=12)  \n",
    "    ax.xaxis.grid(color='#d0dde1')\n",
    "    ax.yaxis.grid(False)    \n",
    "    ax.invert_yaxis()  # labels read top-to-bottom\n",
    "    ax.set_title('The relative importance of each feature when making a prediction', fontname=\"Arial\", color = '#4d4d51', fontsize=12, loc='left')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    plt.suptitle('MOST IMPORTANT FEATURES (UP TO TOP 20)', fontname=\"Arial\", color = '#ff7057', fontsize=12, x=0.43)\n",
    "\n",
    "    plt.figtext(0, -0.05, txt_source, wrap=True, fontname=\"Arial\", color='#4d4d51', fontsize=12)\n",
    "    plt.figtext(0, -0.1, txt_model_desc, wrap=True, fontname=\"Arial\", color='#4d4d51', fontsize=12)\n",
    "    plt.figtext(0, -0.15, txt_base, wrap=True, fontname=\"Arial\", color='#4d4d51', fontsize=12)\n",
    "\n",
    "    plt.savefig('{}/Graphs/Feature importances {} ({}).png'.format(input_folder, file_stub, dict_names[cv]), transparent=False, dpi=80, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
