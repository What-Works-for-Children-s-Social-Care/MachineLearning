{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anonymisation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to system path so we can find all the packages\n",
    "import sys\n",
    "sys.path\n",
    "sys.path.append('C:\\\\Program Files\\\\Python37\\\\Lib\\\\site-packages')\n",
    "sys.path.append('C:\\Program Files\\Python37')\n",
    "\n",
    "# Load user-written functions\n",
    "\n",
    "get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "get_ipython().run_line_magic('autoreload', '2')\n",
    "\n",
    "#import text_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"C:\\\\Users\\\\[username]\\\\Downloads\\\\Updated Text Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify personal information to delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Identify names of parents and carers for anonymisation\n",
    "# Run once and then just import the pickled data as it takes a while\n",
    "assert 1==2\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "parent_carers_names = pd.read_excel('My Documents07754.xlsx')\n",
    "parent_carers_names = parent_carers_names[['PSID','ShortAnswer']]\n",
    "\n",
    "# ShortAnswer has the possiblity of including professionals but they are all missing\n",
    "#parents_carers = parent_carers_names.loc[parent_carers_names['SecondaryQuestion']==\"Name of parent(s)/carer(s)\",'ShortAnswer']\n",
    "parent_carers_names['ShortAnswer'] = [name.strip() if not type(name)==float else np.nan for name in parent_carers_names['ShortAnswer']]\n",
    "\n",
    "generic_words = (['Not applicable', 'Mother', 'Father', 'Both parents', 'and', \n",
    "                  'Grandmother', 'Grandfather', 'Aunt', 'Uncle', 'Stepfather', 'Stepmother',\n",
    "                    'Ms', 'Mr', 'Mrs', '&', ''])\n",
    "\n",
    "for word in generic_words:\n",
    "    parent_carers_names['ShortAnswer'] = [re.sub(word, '', name, flags=re.IGNORECASE) if not type(name)==float else np.nan for name in parent_carers_names['ShortAnswer']]\n",
    "    parent_carers_names['ShortAnswer'] = [re.sub(r\"[^\\w\\s]\", \"\", name) if not type(name)==float else np.nan for name in parent_carers_names['ShortAnswer']]\n",
    "\n",
    "# Some involve full sentences including information about the parents - just want their names\n",
    "# Identify Proper nouns using Spacy\n",
    "parent_carers_names['Parent / Carer'] = [[] for i in range(parent_carers_names.shape[0])]\n",
    "for idx in parent_carers_names['ShortAnswer'].index:\n",
    "    string = parent_carers_names['ShortAnswer'][idx]\n",
    "    if string == string:\n",
    "        doc = nlp(string)\n",
    "        prop_noun = list(set([token.text for token in doc if token.pos_ == \"PROPN\"]))\n",
    "        #print(string)\n",
    "        #print(prop_noun)\n",
    "        parent_carers_names.at[idx, 'Parent / Carer'] = prop_noun\n",
    "\n",
    "# One row by PSID with all parents, carers names\n",
    "parent_carers_names[['PSID','Parent / Carer']] = parent_carers_names[['PSID','Parent / Carer']].groupby('PSID', as_index = False).agg(sum) \n",
    "parent_carers_names['Parent / Carer name'] = [list(set(p_c)) if type(p_c) != float else np.nan for p_c in parent_carers_names['Parent / Carer']]\n",
    "\n",
    "# Save names separately to use in anonymisation\n",
    "import pickle\n",
    "with open(\"Other\\\\Parent Carer Names.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(parent_carers_names[['PSID','Parent / Carer name']], handle, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "# Import first and last names of CYP\n",
    "first_names = pd.read_csv(\"Other\\\\First.csv\", encoding = 'latin')\n",
    "last_names = pd.read_csv(\"Other\\\\Last.csv\", encoding = 'latin')\n",
    "\n",
    "# Combine to form a list of unique first and last names\n",
    "first_names_list = [str(name).split(' ') for name in list(first_names['ClientName.1'])]\n",
    "first_names_list = [name for sublist in first_names_list for name in sublist]\n",
    "first_names_list = [name for name in first_names_list if name not in string.punctuation]\n",
    "first_names_list = [name for name in first_names_list if not isinstance(name, int)]\n",
    "first_names_list  = [name.capitalize() for name in first_names_list if type(name)!= float]\n",
    "first_names_list = list(set(first_names_list))\n",
    "print(len(first_names_list))\n",
    "last_names_list = [str(name).split(' ') for name in list(last_names['ClientName.2'])]\n",
    "last_names_list = [name for sublist in last_names_list for name in sublist]\n",
    "last_names_list = [name for name in last_names_list if name not in string.punctuation]\n",
    "last_names_list = [name for name in last_names_list if not isinstance(name, int)]\n",
    "last_names_list  = [name.capitalize() for name in last_names_list if type(name)!= float]\n",
    "last_names_list = list(set(last_names_list))\n",
    "print(len(last_names_list))\n",
    "all_names = list(set(first_names_list).union(set(last_names_list)))\n",
    "all_names = [name for name in all_names if '(' not in name] # As otherwise loop bugs\n",
    "all_names = [name for name in all_names if ')' not in name] \n",
    "print(len(all_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in list of baby names so that we can get rid of other names \n",
    "# List of baby names (1996-2017, England and Wales) from ONS - most comprehensive list available\n",
    "import pandas as pd\n",
    "baby_names_boys = pd.read_excel(\"Other\\\\adhocallbabynames1996to2017.xls\", sheet_name = \"Boys\", skiprows = 5)\n",
    "baby_names_girls = pd.read_excel(\"Other\\\\adhocallbabynames1996to2017.xls\", sheet_name = \"Girls\", skiprows = 5)\n",
    "list_baby_names = list(set(list(baby_names_girls[\"Name\"]) + list(baby_names_boys[\"Name\"])))\n",
    "# Strip, make sentence case\n",
    "list_baby_names = [name.strip() for name in list_baby_names if type(name)!= float]\n",
    "list_baby_names = [name.capitalize() for name in list_baby_names if type(name)!= float]\n",
    "list_baby_names = [name for name in list_baby_names if len(name) >=2 if type(name)!= float]\n",
    "print(len(list_baby_names))\n",
    "# Delete those in the dataset already\n",
    "list_baby_names = list(set(list_baby_names).difference(set(all_names)))\n",
    "print(len(list_baby_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use team names to find local places\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "team_name = pd.read_csv(\"Other\\\\Team Names.csv\")\n",
    "place_names = [name for name in list(team_name['Team Name']) if 'SU' in name]\n",
    "place_names = [re.sub(r'SU[0-9]{2} ', '', name) for name in place_names]\n",
    "place_names.extend([\"xxx\"]) # Obscured for anonymity of LA\n",
    "\n",
    "# Using place information from schools spreadsheet\n",
    "schools = pd.read_csv(\"Other\\\\2017-2018_860_spine.csv\")\n",
    "place_names.extend(list(schools['LOCALITY']))\n",
    "place_names.extend(list(schools['TOWN']))\n",
    "place_names = list(set(place_names))\n",
    "place_names = [name for name in place_names if name is not np.nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Add school names\n",
    "schools = pd.read_csv(\"Other\\\\2017-2018_860_spine.csv\")\n",
    "school_names = list(set(list(schools['SCHNAME']) + list(schools['OTHERSCHNAME'])))\n",
    "school_names = [name for name in school_names if name is not np.nan]\n",
    "\n",
    "school_pattern = re.compile(r\"\"\" Primary School| Primary Academy| First School| Academy|\n",
    "                            Infant School Academy| School| Junior School| High School and Sixth Form\n",
    "                            | High School| Middle School| College| Infants' School| Infant Academy|\n",
    "                             Infant School|  Nursery| Sport and Community| Community Infants and Nursery\n",
    "                             | Catholic School| Training School| Community Special| Controlled School\n",
    "                             | Catholic Voluntary School| Sixth Form| Primary and Nursery| Technology School| Community School| Secondary School| Preparatory School|\n",
    "                              Community Infants and Nursery| Infant & Nursery|  Infant  Nursery| Independent School|\n",
    "                               Sports| Grammar\"\"\")\n",
    "\n",
    "school_names = [re.sub(school_pattern, \"\", name) for name in school_names]\n",
    "school_names = [re.sub(\"\\([A-Z]+\\)\", \"\", name) for name in school_names] # Getting rid of \"(A)\" or \"(C)\"\n",
    "school_names_no_punc = [re.sub(r\"[^\\w\\s]\", \"\", name) for name in school_names] # Creating another list with no punctuation\n",
    "school_names.extend(school_names_no_punc) \n",
    "school_names = [name.strip() for name in school_names]\n",
    "school_names = list(set(school_names))\n",
    "print(len(school_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Social workers may write school names in different ways\n",
    "# Create variations for C of E schools\n",
    "school_names_cofe = [name for name in school_names if ('CE' in name) or ('CofE' in name) or ('Church of England' in name)]\n",
    "school_names_cofe_all_variations = []\n",
    "for school in school_names_cofe:\n",
    "    variation_list = [\"CE\", \"CofE\", \"Church of England\"]\n",
    "    for variation in variation_list:\n",
    "        if variation in school:\n",
    "            variation_list.remove(variation)\n",
    "            school_names_cofe_all_variations.append(school)\n",
    "            for var in variation_list:\n",
    "                sch = re.sub(variation, var, school) \n",
    "                school_names_cofe_all_variations.append(sch)\n",
    "\n",
    "school_names.extend(school_names_cofe_all_variations)\n",
    "school_names_st = [name for name in school_names if ('St ' in name) or ('ST ' in name) or ('St. ' in name) or ('ST. ' in name) or ('Saint ' in name) or ('SAINT ' in name)]\n",
    "\n",
    "# Create variations for schools with St in name\n",
    "school_names_st_all_variations = []\n",
    "for school in school_names_st:\n",
    "    variation_list = ['St ', 'ST ', 'St. ', 'ST. ', 'Saint ', 'SAINT ']\n",
    "    for variation in variation_list:\n",
    "        if variation in school:\n",
    "            variation_list.remove(variation)\n",
    "            school_names_st_all_variations.append(school)\n",
    "            for var in variation_list:\n",
    "                sch = re.sub(variation, var, school) \n",
    "                school_names_st_all_variations.append(sch)\n",
    "                \n",
    "school_names.extend(school_names_st_all_variations)\n",
    "school_names = list(set(school_names))\n",
    "print(len(school_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace known personal information with placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "os.chdir(\"C:\\\\Users\\\\[username]\\\\Downloads\\\\Updated Text Data\") # insert [username]\n",
    "filename = open(\"Created\\\\rq1data_text_for_anonymisation.pkl\", \"rb\")\n",
    "text = pickle.load(filename)\n",
    "print(text.index)\n",
    "text.sort_index(inplace = True) # VC: Use reset instead (but keep as is for the moment)\n",
    "print(text.index)\n",
    "\n",
    "text_cols = (['Contact and Referral Form_text',\n",
    "       'Child Social Work Assessment for Review Child Protection Conference_text_prev',\n",
    "       'Child Social Work Assessment to Initial Child Protection Conference_text_prev',\n",
    "       'Child Social Work Assessment_text_prev',\n",
    "       'Contact and Referral Form_text_prev'])\n",
    "\n",
    "# Drop nas\n",
    "print(text.shape)\n",
    "print(text.isna().sum())\n",
    "text.dropna(subset = text_cols, how = 'all', inplace = True)\n",
    "text[text_cols] = text[text_cols].fillna('')\n",
    "print(text.shape)\n",
    "print(text.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Parent / carer names\n",
    "os.chdir(\"C:\\\\Users\\\\[username]\\\\Downloads\\\\Updated Text Data\") # insert [username]\n",
    "filename = open(\"Other\\\\Parent Carer Names.pkl\", \"rb\")\n",
    "parent_carer_names_df = pickle.load(filename)\n",
    "\n",
    "# Merge into text data\n",
    "print(text.shape)\n",
    "text = text.merge(parent_carer_names_df, how = 'left', on = 'PSID')\n",
    "print(text.shape)\n",
    "print(text['Parent / Carer name'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Replace available names with placeholders\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "## Record how many replacements to keep track of progress\n",
    "# _replaced = total number replaced, _n = number replaced in that document\n",
    "# documents_changed_ = total number of documents changed\n",
    "names_replaced, places_replaced, schools_replaced, parents_carers_names_replaced = 0, 0, 0, 0\n",
    "documents_changed_names, documents_changed_places, documents_changed_schools, documents_changed_p_c_names = 0, 0, 0, 0\n",
    "\n",
    "save_every = 100\n",
    "\n",
    "text_cols = (['Contact and Referral Form_text_prev'])\n",
    "\n",
    "for text_col in text_cols:\n",
    "    print(\"Column: \", text_col)\n",
    "    already_done_list = []\n",
    "    for idx in text.index[5800:]:\n",
    "        print(\"Idx: \", idx)\n",
    "        string = text.loc[idx, text_col]\n",
    "        #print(string)\n",
    "        if (string.strip() != '') and (idx not in already_done_list):\n",
    "            # Dealing with formatting issues\n",
    "            # Strip space characters\n",
    "            string = re.sub(r'\\W+', ' ', str(string))\n",
    "            schools_n = 0\n",
    "            for school in school_names:\n",
    "                stringn = re.subn(r\"\\b{}\\b\".format(school), \"School_Name\", string, flags = re.IGNORECASE)\n",
    "                string = stringn[0]\n",
    "                schools_n += stringn[1]  \n",
    "            names_n = 0\n",
    "            for name in all_names:\n",
    "                stringn = re.subn(r\"\\b{}\\b\".format(name), \"Name\", string, flags = re.IGNORECASE)\n",
    "                string = stringn[0]\n",
    "                names_n +=stringn[1]\n",
    "            places_n = 0\n",
    "            for place in place_names:\n",
    "                stringn = re.subn(r\"\\b{}\\b\".format(place), \"Location\", string, flags = re.IGNORECASE)\n",
    "                string = stringn[0]\n",
    "                places_n += stringn[1]              \n",
    "            parents_carers_names = text.loc[idx, \"Parent / Carer name\"]\n",
    "            #print(\"Parents / carers: \", parents_carers_names)\n",
    "            parents_carers_names_n = 0\n",
    "            if type(parents_carers_names) != float: \n",
    "                for name in parents_carers_names:\n",
    "                    stringn = re.subn(r\"\\b{}\\b\".format(name), \"Parent_Carer_Name\", string, flags = re.IGNORECASE)\n",
    "                    string = stringn[0]\n",
    "                    parents_carers_names_n += stringn[1]                      \n",
    "            repeated_text_indices = list(np.where(text[text_col] ==  text.loc[idx,text_col])[0])\n",
    "            print(repeated_text_indices)\n",
    "            print(\"Names n: \", names_n)\n",
    "            print(\"Places n: \", places_n)\n",
    "            print(\"Schools n: \", schools_n)\n",
    "            print(\"Parent carers n: \", parents_carers_names_n)\n",
    "            if repeated_text_indices:\n",
    "                print(repeated_text_indices)\n",
    "                text.at[repeated_text_indices,text_col] = string\n",
    "                already_done_list.extend(repeated_text_indices)\n",
    "                names_replaced += (len(repeated_text_indices) * names_n)\n",
    "                places_replaced += (len(repeated_text_indices) * places_n)\n",
    "                schools_replaced += (len(repeated_text_indices) * schools_n)\n",
    "                parents_carers_names_replaced += (len(repeated_text_indices) * parents_carers_names_n)\n",
    "                if names_n != 0:\n",
    "                    documents_changed_names += len(repeated_text_indices)\n",
    "                    print(\"Number of documents where names were changed: \", documents_changed_names)\n",
    "                if places_n != 0:\n",
    "                    documents_changed_places += len(repeated_text_indices)\n",
    "                    print(\"Number of documents where places were changed: \", documents_changed_places)\n",
    "                if schools_n != 0:\n",
    "                    documents_changed_schools += len(repeated_text_indices)\n",
    "                    print(\"Number of documents where school names were changed: \", documents_changed_schools)\n",
    "                if parents_carers_names_n != 0:\n",
    "                    documents_changed_p_c_names += len(repeated_text_indices)\n",
    "                    print(\"Number of documents where parents' / carers' names were changed: \", documents_changed_p_c_names)\n",
    "            else:\n",
    "                text.at[idx,text_col] = string\n",
    "                names_replaced += names_n\n",
    "                places_replaced += places_n\n",
    "                schools_replaced += schools_n\n",
    "                parents_carers_names_replaced += parents_carers_names_n\n",
    "                if names_n != 0:\n",
    "                    documents_changed_names += 1\n",
    "                    print(\"Number of documents where names were changed: \", documents_changed_names)\n",
    "                if places_n != 0:\n",
    "                    documents_changed_places += 1\n",
    "                    print(\"Number of documents where places were changed: \", documents_changed_places)\n",
    "                if schools_n != 0:\n",
    "                    documents_changed_schools += 1\n",
    "                    print(\"Number of documents where school names were changed: \", documents_changed_schools)\n",
    "                if parents_carers_names_n != 0:\n",
    "                    documents_changed_p_c_names += 1\n",
    "                    print(\"Number of documents where parents' / carers' names were changed: \", documents_changed_p_c_names)\n",
    "        if (idx % save_every == 0) & (idx != 0):\n",
    "            with open(\"Created/Anonymisation/text_rq1_names_placeholders_{}_{}_{}.pkl\".format(text_col, idx - save_every, idx - 1), \"wb\") as handle:\n",
    "                pickle.dump(text.loc[idx-save_every: idx - 1, text_col], handle, protocol = pickle.HIGHEST_PROTOCOL)   \n",
    "        elif (idx == text.shape[0] - 1):\n",
    "            with open(\"Created/Anonymisation/text_rq1_names_placeholders_{}_{}_{}.pkl\".format(text_col, idx - idx % save_every - 1, idx), \"wb\") as handle:\n",
    "                pickle.dump(text.loc[idx - idx % save_every: idx, text_col], handle, protocol = pickle.HIGHEST_PROTOCOL)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Knit together files\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "os.chdir(\"C:\\\\Users\\\\[username]\\\\Downloads\\\\Updated Text Data\") # insert [username]\n",
    "filename_list = [file for file in glob.glob(\"Created\\\\Anonymisation\\\\text_rq1_names_placeholders_*.pkl\")]\n",
    "\n",
    "# Call in all files in the anonymisation folder\n",
    "file_dict = {}\n",
    "for file in filename_list:\n",
    "    filename = open(file, \"rb\")\n",
    "    f = pickle.load(filename)\n",
    "    file_n = re.sub('.pkl', '', file)\n",
    "    file_n = re.sub(\"Created\\\\\\\\Anonymisation\\\\\\\\text_rq1_names_placeholders_\", '', file_n)\n",
    "    print(file_n)\n",
    "    file_dict[file_n] = f\n",
    "    \n",
    "text_cols = (['Contact and Referral Form_text',\n",
    "   'Child Social Work Assessment for Review Child Protection Conference_text_prev',\n",
    "   'Child Social Work Assessment to Initial Child Protection Conference_text_prev',\n",
    "   'Child Social Work Assessment_text_prev',\n",
    "   'Contact and Referral Form_text_prev'])\n",
    "\n",
    "col_wo_identifier_dict = {}\n",
    "for text_col in text_cols:\n",
    "    text_col_df = [df for df in file_dict.keys() if text_col in df]\n",
    "    if 'prev' in text_col:\n",
    "        text_col_df = [df for df in text_col_df if 'prev' in df]\n",
    "    else:\n",
    "        text_col_df = [df for df in text_col_df if 'prev' not in df]\n",
    "    print(text_col_df)\n",
    "    text_col_df_values = [v for k, v in file_dict.items() if k in text_col_df]\n",
    "    print(len(text_col_df_values))\n",
    "    col_wo_identifier_dict[text_col] = pd.concat(text_col_df_values, axis = 0)\n",
    "    col_wo_identifier_dict[text_col].sort_index(inplace = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Corrected misaligned data\n",
    "\n",
    "col_wo_identifier_dict['Contact and Referral Form_text_2'] = pd.Series(index = col_wo_identifier_dict['Contact and Referral Form_text'].index)\n",
    "col_wo_identifier_dict['Contact and Referral Form_text_2'].loc[6209:] = col_wo_identifier_dict['Contact and Referral Form_text'].loc[6209:]\n",
    "col_wo_identifier_dict['Contact and Referral Form_text_2'] = col_wo_identifier_dict['Contact and Referral Form_text_2'].shift(1)\n",
    "col_wo_identifier_dict['Contact and Referral Form_text_2'].loc[:6208] = col_wo_identifier_dict['Contact and Referral Form_text'].loc[:6208]\n",
    "\n",
    "col_wo_identifier_dict['Child Social Work Assessment for Review Child Protection Conference_text_prev_2'] = pd.Series(index = col_wo_identifier_dict['Child Social Work Assessment for Review Child Protection Conference_text_prev'].index)\n",
    "col_wo_identifier_dict['Child Social Work Assessment for Review Child Protection Conference_text_prev_2'].loc[6169:] = col_wo_identifier_dict['Child Social Work Assessment for Review Child Protection Conference_text_prev'].loc[6169:]\n",
    "col_wo_identifier_dict['Child Social Work Assessment for Review Child Protection Conference_text_prev_2'] = col_wo_identifier_dict['Child Social Work Assessment for Review Child Protection Conference_text_prev_2'].shift(1)\n",
    "col_wo_identifier_dict['Child Social Work Assessment for Review Child Protection Conference_text_prev_2'].loc[:6168] = col_wo_identifier_dict['Child Social Work Assessment for Review Child Protection Conference_text_prev'].loc[:6168]\n",
    "\n",
    "col_wo_identifier_dict['Child Social Work Assessment to Initial Child Protection Conference_text_prev_2'] = pd.Series(index = col_wo_identifier_dict['Child Social Work Assessment to Initial Child Protection Conference_text_prev'].index)\n",
    "col_wo_identifier_dict['Child Social Work Assessment to Initial Child Protection Conference_text_prev_2'].loc[6212:] = col_wo_identifier_dict['Child Social Work Assessment to Initial Child Protection Conference_text_prev'].loc[6212:]\n",
    "col_wo_identifier_dict['Child Social Work Assessment to Initial Child Protection Conference_text_prev_2'] = col_wo_identifier_dict['Child Social Work Assessment to Initial Child Protection Conference_text_prev_2'].shift(1)\n",
    "col_wo_identifier_dict['Child Social Work Assessment to Initial Child Protection Conference_text_prev_2'].loc[:6211] = col_wo_identifier_dict['Child Social Work Assessment to Initial Child Protection Conference_text_prev'].loc[:6211]\n",
    "\n",
    "col_wo_identifier_dict['Child Social Work Assessment_text_prev_2'] = pd.Series(index = col_wo_identifier_dict['Child Social Work Assessment_text_prev'].index)\n",
    "col_wo_identifier_dict['Child Social Work Assessment_text_prev_2'].loc[6212:] = col_wo_identifier_dict['Child Social Work Assessment_text_prev'].loc[6212:]\n",
    "col_wo_identifier_dict['Child Social Work Assessment_text_prev_2'] = col_wo_identifier_dict['Child Social Work Assessment_text_prev_2'].shift(1)\n",
    "col_wo_identifier_dict['Child Social Work Assessment_text_prev_2'].loc[:6211] = col_wo_identifier_dict['Child Social Work Assessment_text_prev'].loc[:6211]\n",
    "\n",
    "col_wo_identifier_dict['Contact and Referral Form_text_prev_2'] = pd.Series(index = col_wo_identifier_dict['Contact and Referral Form_text_prev'].index)\n",
    "col_wo_identifier_dict['Contact and Referral Form_text_prev_2'].loc[4302:] = col_wo_identifier_dict['Contact and Referral Form_text_prev'].loc[4302:]\n",
    "col_wo_identifier_dict['Contact and Referral Form_text_prev_2'] = col_wo_identifier_dict['Contact and Referral Form_text_prev_2'].shift(-2)\n",
    "col_wo_identifier_dict['Contact and Referral Form_text_prev_2'].loc[:4299] = col_wo_identifier_dict['Contact and Referral Form_text_prev'].loc[:4299]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "# Deduplicate by index (in case any rows are repeated in different files)\n",
    "for col, df in col_wo_identifier_dict.items():\n",
    "    print(col)\n",
    "    print(df.shape)\n",
    "    col_wo_identifier_dict[col] = df.loc[~df.index.duplicated(keep = 'first')]\n",
    "    print(col_wo_identifier_dict[col].shape)\n",
    "    col_wo_identifier_dict[col].rename(col, inplace = True)\n",
    "    \n",
    "\n",
    "# Merge together on index\n",
    "df_wo_identifier_together = reduce(lambda left, right: pd.merge(left,right,left_index = True, right_index = True, how = 'outer'), col_wo_identifier_dict.values())\n",
    "df_wo_identifier_together = df_wo_identifier_together.loc[~df_wo_identifier_together.index.duplicated(keep = 'first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename adjusted columns to have the same column names as expected\n",
    "print(df_wo_identifier_together.columns)\n",
    "df_wo_identifier_together.drop(columns = text_cols, inplace = True)\n",
    "print(df_wo_identifier_together.columns)\n",
    "df_wo_identifier_together.rename(columns = dict(zip(df_wo_identifier_together.columns, text_cols)), inplace = True)\n",
    "print(df_wo_identifier_together.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in the original text dataset for anonymisation so that we can merge in the IDs\n",
    "# Reset index so the text lines up df_wo_identifier_together\n",
    "# The problem was a combination of the indices starting at 7221, 7222, 0, 1, 2 etc and missing data\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "os.chdir(\"C:\\\\Users\\\\[username]\\\\Downloads\\\\Updated Text Data\") # insert [username]\n",
    "filename = open(\"Created\\\\rq1data_text_for_anonymisation.pkl\", \"rb\")\n",
    "text = pickle.load(filename)\n",
    "print(text.index)\n",
    "text.reset_index(drop = True, inplace = True) # to line up with df_wo_identifier_together\n",
    "print(text.index)\n",
    "\n",
    "# Merge back in IDs\n",
    "print(text.shape)\n",
    "print(text.columns)\n",
    "text = pd.merge(text[['PSID', 'ReferralDatetime', 'ReferralDatetime_previous']], df_wo_identifier_together, left_index = True, right_index = True, how = 'left')\n",
    "print(text.shape)\n",
    "print(text.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing with empty string\n",
    "\n",
    "print(text.isna().sum()) \n",
    "text[text_cols] = text[text_cols].fillna('')\n",
    "print(text.isna().sum())\n",
    "\n",
    "# Save knitted data all together \n",
    "with open(\"Created/text_rq1_names_placeholders.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(text, handle, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#!python -m spacy download  en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting rid of other names by fuzzy matching on baby name list\n",
    "import difflib\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "save_every = 100\n",
    "\n",
    "filename = open(\"Created\\\\text_rq1_names_placeholders.pkl\", \"rb\")\n",
    "text = pickle.load(filename)\n",
    "\n",
    "print(text.shape) # CHECK whether shape is as expected?\n",
    "print(text.index)\n",
    "print(text.columns)\n",
    "\n",
    "not_names = (['Name'])\n",
    "\n",
    "# _replaced = total number replaced, _n = number replaced in that document\n",
    "# documents_changed_ = total number of documents changed\n",
    "other_names_replaced = 0\n",
    "documents_changed_other_names = 0\n",
    "\n",
    "\n",
    "text_cols = (['Contact and Referral Form_text_prev'])\n",
    "\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "text2 = pd.DataFrame(None, index = text.index, columns = text_cols, dtype = 'object')\n",
    "fuzzy_dict = {}\n",
    "for text_col in text_cols:\n",
    "    already_done_list = []\n",
    "    for idx in text.index[100:]:\n",
    "        print(\"Idx: \", idx)\n",
    "        string = text.loc[idx, text_col]\n",
    "        if (string.strip() != '') and (idx not in already_done_list):\n",
    "            doc = nlp(string)\n",
    "            prop_noun = list(set([token.text for token in doc if token.pos_ == \"PROPN\"]))\n",
    "            print(\"Prop_noun: \", prop_noun)\n",
    "            other_names_n = 0\n",
    "            for p in prop_noun:\n",
    "                if p not in not_names:\n",
    "                    fuzzies_matched = fuzzy_dict.get(p, None)\n",
    "                    if fuzzies_matched is None:\n",
    "                        fuzzy_names = difflib.get_close_matches(p.capitalize(), list_baby_names, cutoff=0.9)\n",
    "                        print(p, fuzzy_names)\n",
    "                        fuzzy_dict[p] = fuzzies_matched\n",
    "                    if fuzzies_matched != []:\n",
    "                        stringn = re.subn(r\"\\b{}\\b\".format(p), 'Other_Name', string, flags = re.IGNORECASE)\n",
    "                        string = stringn[0]\n",
    "                        other_names_n +=stringn[1]\n",
    "            repeated_text_indices = list(np.where(text[text_col] == text.loc[idx, text_col])[0])\n",
    "            print(repeated_text_indices)\n",
    "            if repeated_text_indices:\n",
    "                print(repeated_text_indices)\n",
    "                text2.loc[repeated_text_indices, text_col] = string\n",
    "                already_done_list.extend(repeated_text_indices) \n",
    "                other_names_replaced += (len(repeated_text_indices) * other_names_n)\n",
    "                if other_names_n != 0:\n",
    "                    documents_changed_other_names += len(repeated_text_indices)\n",
    "                    print(\"Number of documents where other names were changed: \", documents_changed_other_names)\n",
    "            else:\n",
    "                text2.at[idx, text_col] = string\n",
    "                other_names_replaced += other_names_n\n",
    "                if other_names_n != 0:\n",
    "                    documents_changed_other_names += 1\n",
    "                    print(\"Number of documents where other names were changed: \", documents_changed_other_names)\n",
    "        if (idx % save_every == 0) & (idx != 0):\n",
    "            with open(\"Created/Anonymisation/text_rq1_names_matched_{}_{}_{}.pkl\".format(text_col, idx - save_every, idx - 1), \"wb\") as handle:\n",
    "                pickle.dump(text2.loc[idx-save_every: idx - 1, text_col], handle, protocol = pickle.HIGHEST_PROTOCOL)   \n",
    "        elif (idx == text.shape[0] - 1):\n",
    "            with open(\"Created/Anonymisation/text_rq1_names_matched_{}_{}_{}.pkl\".format(text_col, idx - idx % save_every - 1, idx), \"wb\") as handle:\n",
    "                pickle.dump(text2.loc[idx - idx % save_every: idx, text_col], handle, protocol = pickle.HIGHEST_PROTOCOL)          \n",
    "\n",
    "with open(\"Created/text_rq1_names_matched.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(text2, handle, protocol = pickle.HIGHEST_PROTOCOL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "# Deduplicate by index (in case any rows are repeated in different files)\n",
    "for col, df in col_wo_identifier_dict.items():\n",
    "    print(col)\n",
    "    print(df.shape)\n",
    "    col_wo_identifier_dict[col] = df.loc[~df.index.duplicated(keep = 'first')]\n",
    "    print(col_wo_identifier_dict[col].shape)\n",
    "    col_wo_identifier_dict[col].rename(col, inplace = True)\n",
    "    \n",
    "# Merge together on index\n",
    "df_wo_identifier_together = reduce(lambda left, right: pd.merge(left,right,left_index = True, right_index = True, how = 'outer'), col_wo_identifier_dict.values())\n",
    "df_wo_identifier_together = df_wo_identifier_together.loc[~df_wo_identifier_together.index.duplicated(keep = 'first')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in the original text dataset for anonymisation so that we can merge in the IDs\n",
    "# Reset index so the text lines up df_wo_identifier_together\n",
    "# The problem was a combination of the indices starting at 7221, 7222, 0, 1, 2 etc and missing data\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "os.chdir(\"C:\\\\Users\\\\[username]\\\\Downloads\\\\Updated Text Data\") # insert [username]\n",
    "filename = open(\"Created\\\\rq1data_text_for_anonymisation.pkl\", \"rb\")\n",
    "text = pickle.load(filename)\n",
    "print(text.index)\n",
    "text.reset_index(drop = True, inplace = True) # to line up with df_wo_identifier_together\n",
    "print(text.index)\n",
    "\n",
    "# Merge back in IDs\n",
    "print(text.shape)\n",
    "print(text.columns)\n",
    "text = pd.merge(text[['PSID', 'ReferralDatetime', 'ReferralDatetime_previous']], df_wo_identifier_together, left_index = True, right_index = True, how = 'left')\n",
    "print(text.shape)\n",
    "print(text.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing with empty string\n",
    "\n",
    "print(text.isna().sum()) \n",
    "text[list(col_wo_identifier_dict.keys())] = text[list(col_wo_identifier_dict.keys())].fillna('')\n",
    "print(text.isna().sum())\n",
    "\n",
    "# Save knitted data all together \n",
    "with open(\"Created/text_rq1_names_matched.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(df_wo_identifier_together, handle, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
