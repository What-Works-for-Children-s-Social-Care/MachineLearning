{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master Results\n",
    "\n",
    "This notebook combines all the results from all the LAs to gain an insight into what the results tell us overall.\n",
    "\n",
    "Illustrated with LA1 and LA2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/[username]/Documents') # insert [username]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def aggregate_results(file_path, result_name, col_names, la_name, pivot):\n",
    "    '''Loops through files in the file_path with the result_name.\n",
    "    col_names is values in the first column that become columns upon pivoting\n",
    "    if the data is arranged as:\n",
    "    \n",
    "        Average precision\t0.06\n",
    "        AUC\t0.55\n",
    "        F score (beta = 0.1)\t0.15\n",
    "        Precision\t0.15\n",
    "        Recall\t0.14\n",
    "    \n",
    "    la_name is the name or number of the LA\n",
    "    pivot takes True when data is structured as follows:\n",
    "    \n",
    "        Average precision\t0.06\n",
    "        AUC\t0.55\n",
    "        F score (beta = 0.1)\t0.15\n",
    "        Precision\t0.15\n",
    "        Recall\t0.14    \n",
    "    \n",
    "    pivot takes False when data is structued as follows:\n",
    "    \n",
    "        Age_at_Referral_Start_cut\taverage_precision_score\tfalse_discovery_rate\tfalse_omission_rate\n",
    "        10-15 years\t0.213166898\t0.366666667\t0.042288557\n",
    "        5-9 years\t0.343099185\t0.255319149\t0.041564792\n",
    "        1-4 years\t0.342333697\t0.31372549\t0.031555222\n",
    "        under 1 year\t0.252508361\t0.333333333\t0.022121015\n",
    "        16+ years\t0.253814083\t0.362068966\t0.055939227\n",
    "    \n",
    "    '''\n",
    "    df_all = pd.DataFrame(columns = col_names)\n",
    "    for file in glob.glob(\"{}/{}*.csv\".format(file_path, result_name)):\n",
    "        if col_names != []:\n",
    "            df = pd.read_csv(file, header = None)\n",
    "        else:\n",
    "            df = pd.read_csv(file, index_col = 0)\n",
    "        file_n = re.sub('.csv', '', file)\n",
    "        file_n = re.sub(file_path + '/' + result_name, '', file_n)\n",
    "        print(file_n)\n",
    "        df['model'] = file_n\n",
    "        if pivot is False:\n",
    "            df_all = pd.concat([df_all, df], axis = 0)\n",
    "        else:\n",
    "            df_pivoted = pd.pivot_table(df, index = 'model', columns = 0)\n",
    "            df_pivoted.columns = [col[1] for col in df_pivoted.columns]\n",
    "            df_pivoted.index.name = None\n",
    "            df_all = pd.concat([df_all, df_pivoted], axis = 0)\n",
    "\n",
    "    if pivot is False:\n",
    "        return df_all\n",
    "    else:\n",
    "        df_all.reset_index(inplace = True)  \n",
    "        df_all.rename(columns = {'index': 'model'}, inplace = True)\n",
    "        df_all['LA'] = la_name\n",
    "        return df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_score_columns = ['AUC', 'Average precision', 'F score (beta = 0.1)', 'Precision','Recall']\n",
    "model_score_la1 = aggregate_results(file_path = 'LA 1 Results', result_name = 'scores_', col_names = model_score_columns , la_name = 1, pivot = True)\n",
    "model_score_la2 = aggregate_results(file_path = 'Models/Scores', result_name = 'scores_', col_names = model_score_columns, la_name = 2, pivot = True) \n",
    "\n",
    "# Concatenate all info about the model scores together\n",
    "model_score_all = pd.concat([model_score_la1, model_score_la2], axis = 0, ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract information about the model\n",
    "import numpy as np\n",
    "model_split = model_score_all['model'].str.split('_', n = 3, expand = True) \n",
    "model_split.rename(columns ={0: 'Research Question Number', 1: 'Cross-validation Method', 2: 'Data Included', 3: 'Test or Holdout'}, inplace = True)\n",
    "# At LA1, we used test and holdout data\n",
    "model_score_all = model_score_all.merge(model_split, left_index = True, right_index = True)\n",
    "model_score_all['Test or Holdout'] = np.where(model_score_all['LA'] == 2, 'holdout', model_score_all['Test or Holdout'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_score_all.drop(columns = 'model', inplace = True, errors = 'ignore')\n",
    "model_score_all.to_csv('All model scores LA1 and LA2.csv')\n",
    "model_score_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Comparison of means\n",
    "\n",
    "# LA\n",
    "average_precision_la_1 = round(model_score_all.loc[model_score_all['LA'] == 1, 'Average precision'].mean(), 2)\n",
    "average_precision_la_2 = round(model_score_all.loc[model_score_all['LA'] == 2, 'Average precision'].mean(), 2)\n",
    "print(average_precision_la_1)\n",
    "print(average_precision_la_2)\n",
    "\n",
    "# Cross-validation method\n",
    "average_precision_cross_val_ss = round(model_score_all.loc[model_score_all['Cross-validation Method'] == 'ss', 'Average precision'].mean(), 2)\n",
    "average_precision_cross_val_ts = round(model_score_all.loc[model_score_all['Cross-validation Method'] == 'ts', 'Average precision'].mean(), 2)\n",
    "print(average_precision_cross_val_ss)\n",
    "print(average_precision_cross_val_ts)\n",
    "\n",
    "# Data included\n",
    "average_precision_data_included_str = round(model_score_all.loc[model_score_all['Data Included'] == 'str', 'Average precision'].mean(), 2)\n",
    "average_precision_data_included_all = round(model_score_all.loc[model_score_all['Data Included'] == 'all', 'Average precision'].mean(), 2)\n",
    "print(average_precision_data_included_str)\n",
    "print(average_precision_data_included_all)\n",
    "\n",
    "aggregate_average_precision = pd.DataFrame(data = {'Local authority 1': average_precision_la_1,\n",
    "                                                  'Local authority 2': average_precision_la_2,\n",
    "                                                  'Contemporary cross-validation': average_precision_cross_val_ss,\n",
    "                                                  'Cross-validation over time': average_precision_cross_val_ts,\n",
    "                                                  'Using just structured data': average_precision_data_included_str,\n",
    "                                                  'Using structured and text data': average_precision_data_included_all},\n",
    "                                          index = [0])\n",
    "\n",
    "aggregate_average_precision.to_csv('Aggregate_model_scores_LAs1and2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Does including all data strictly dominate using structured data?\n",
    "\n",
    "scores = ['AUC', 'Average precision', 'F score (beta = 0.1)',\n",
    "           'Precision', 'Recall']\n",
    "data_included_comparison_all = pd.DataFrame()\n",
    "for la in [1,2]:\n",
    "    for cv in ['ss', 'ts']:\n",
    "        for rq in ['rq1', 'rq2']:\n",
    "            try:\n",
    "                model_str = model_score_all.loc[(model_score_all['LA'] == la) & \n",
    "                                    (model_score_all['Cross-validation Method'] == cv) &\n",
    "                                    (model_score_all['Research Question Number'] == rq) &\n",
    "                                    (model_score_all['Data Included'] == 'str'), scores]\n",
    "                model_str.reset_index(inplace = True, drop = True)\n",
    "\n",
    "\n",
    "                model_all = model_score_all.loc[(model_score_all['LA'] == la) & \n",
    "                                    (model_score_all['Cross-validation Method'] == cv) &\n",
    "                                    (model_score_all['Research Question Number'] == rq) &\n",
    "                                    (model_score_all['Data Included'] == 'all'), scores]\n",
    "                model_all.reset_index(inplace = True, drop = True)\n",
    "\n",
    "                # Check whether all dominates str\n",
    "                data_included_comparison = model_all >=  model_str\n",
    "                data_included_comparison['LA'] = la\n",
    "                data_included_comparison['Cross-validation Method'] = cv\n",
    "                data_included_comparison['Research Question Number'] = rq\n",
    "                data_included_comparison_all = pd.concat([data_included_comparison_all, data_included_comparison], axis = 0, ignore_index = True)\n",
    "            except:\n",
    "                continue\n",
    "data_included_comparison_all.to_csv('Comparison of data included LA1 and LA2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True where including the text data improves the score over just using the structured data\n",
    "data_included_comparison_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of cross-validation methods\n",
    "scores = ['AUC', 'Average precision', 'F score (beta = 0.1)',\n",
    "           'Precision', 'Recall']\n",
    "cross_validation_comparison_all = pd.DataFrame()\n",
    "for la in [1,2]:\n",
    "    for data in ['str', 'all']:\n",
    "        for rq in ['rq1', 'rq2']:\n",
    "            try:\n",
    "                model_ss = model_score_all.loc[(model_score_all['LA'] == la) & \n",
    "                                    (model_score_all['Cross-validation Method'] == 'ss') &\n",
    "                                    (model_score_all['Research Question Number'] == rq) &\n",
    "                                    (model_score_all['Data Included'] == data), scores]\n",
    "                model_ss.reset_index(inplace = True, drop = True)\n",
    "\n",
    "\n",
    "                model_ts = model_score_all.loc[(model_score_all['LA'] == la) & \n",
    "                                    (model_score_all['Cross-validation Method'] == 'ts') &\n",
    "                                    (model_score_all['Research Question Number'] == rq) &\n",
    "                                    (model_score_all['Data Included'] == data), scores]\n",
    "                model_ts.reset_index(inplace = True, drop = True)\n",
    "\n",
    "                # Check whether all dominates str\n",
    "                cross_validation_comparison = model_ss >=  model_ts\n",
    "                cross_validation_comparison['LA'] = la\n",
    "                cross_validation_comparison['Data Included'] = data\n",
    "                cross_validation_comparison['Research Question Number'] = rq\n",
    "                cross_validation_comparison_all = pd.concat([cross_validation_comparison_all, cross_validation_comparison], axis = 0, ignore_index = True)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "cross_validation_comparison_all.to_csv('Comparison of cross-validation methods LA1 and LA2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True = cross validating contemporaneously has a better score than cross-validating over time\n",
    "cross_validation_comparison_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuitive Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intuitive_metrics_columns = (['% of risky cases in top 10%',\n",
    "'% of safe cases in bottom 10%',\n",
    "'Number of true positives in 1000 cases',\n",
    "'Number of true negatives in 1000 cases',\n",
    "'Number of false positives in 1000 cases',\n",
    "'Number of false negatives in 1000 cases'])\n",
    "\n",
    "intuitive_results_la1 = aggregate_results(file_path = 'LA 1 Results', result_name = 'Intuitive metrics ', col_names = intuitive_metrics_columns, la_name = 1, pivot = True)\n",
    "intuitive_results_la2 = aggregate_results(file_path = '/Models/Scores', result_name = 'Intuitive metrics ', col_names = intuitive_metrics_columns, la_name = 2, pivot = True)\n",
    "\n",
    "# Concatenate all info about the model scores together\n",
    "#intuitive_results_all = pd.concat([intuitive_results_la1, intuitive_results_la2], axis = 0, ignore_index = True)\n",
    "intuitive_results_all = intuitive_results_la2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract information about the model\n",
    "import numpy as np\n",
    "model_split = intuitive_results_all['model'].str.split('_', n = 3, expand = True) \n",
    "model_split.rename(columns ={0: 'Research Question Number', 1: 'Cross-validation Method', 2: 'Data Included'}, inplace = True)\n",
    "intuitive_results_all = intuitive_results_all.merge(model_split, left_index = True, right_index = True)\n",
    "intuitive_results_all.drop(columns = ['% of risky cases in top 10%', '% of safe cases in bottom 10%'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of false / true positives and negatives in 1000 cases\n",
    "intuitive_results_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Look at ratios of False Positives to True Positives\n",
    "comparison_FP_TP_all = pd.DataFrame(columns = ['Ratio FP / TP', 'LA', 'Research Question Number',\n",
    "       'Cross-validation Method', 'Data Included'])\n",
    "\n",
    "for la in [1,2]:\n",
    "    for data in ['str', 'all']:\n",
    "        for cv in ['ss', 'ts']:\n",
    "            for rq in ['rq1', 'rq2']:\n",
    "                try:\n",
    "                    mask = ((intuitive_results_all['LA'] == la) & \n",
    "                    (intuitive_results_all['Cross-validation Method'] == cv) &\n",
    "                    (intuitive_results_all['Research Question Number'] == rq) &\n",
    "                    (intuitive_results_all['Data Included'] == data))\n",
    "                    \n",
    "                    comparison_FP_TP = pd.DataFrame(columns = ['LA', 'Research Question Number',\n",
    "                    'Cross-validation Method', 'Data Included', 'Ratio FP / TP'])\n",
    "                    \n",
    "                    comparison_FP_TP.loc[0, 'LA'] = la\n",
    "                    comparison_FP_TP['Research Question Number'] = rq\n",
    "                    comparison_FP_TP['Cross-validation Method'] = cv\n",
    "                    comparison_FP_TP['Data Included'] = data\n",
    "                    comparison_FP_TP['Ratio FP / TP'] = round(int(intuitive_results_all.loc[mask, 'Number of false positives in 1000 cases']) / int(intuitive_results_all.loc[mask, 'Number of true positives in 1000 cases']), 2)\n",
    "\n",
    "                    comparison_FP_TP_all = pd.concat([comparison_FP_TP_all, comparison_FP_TP], axis = 0, ignore_index = True)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "comparison_FP_TP_all = comparison_FP_TP_all[['LA', 'Research Question Number', 'Cross-validation Method', 'Data Included', 'Ratio FP / TP']]\n",
    "comparison_FP_TP_all.sort_values(by = 'Ratio FP / TP', inplace = True)\n",
    "comparison_FP_TP_all.to_csv('Comparison of False Positives to False Negatives LA2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want the ratio to be as close to 0 as possible\n",
    "# Where Ratio FP / TP > 1, a case identified as likely to escalate is more likely to have been falsely identified\n",
    "comparison_FP_TP_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LA1\n",
    "fairness_gender_la1 = aggregate_results(file_path = 'LA 1 Results', result_name = 'Fairness_metrics_Gender_', col_names = [], la_name = 1, pivot = False)\n",
    "fairness_age_la1 = aggregate_results(file_path = 'LA 1 Results', result_name = 'Fairness_metrics_Age_at_Referral_Start_cut_', col_names = [], la_name = 1, pivot = False)\n",
    "fairness_disabled_la1 = aggregate_results(file_path = 'LA 1 Results', result_name = 'Fairness_metrics_Disabled_', col_names = [], la_name = 1, pivot = False)\n",
    "fairness_ethnicity_la1 = aggregate_results(file_path = 'LA 1 Results', result_name = 'Fairness_metrics_Ethnicity_grouped_', col_names = [], la_name = 1, pivot = False)\n",
    "\n",
    "# Add LA name and drop rows with NA in metrics\n",
    "fairness_gender_la1['LA'] = 1\n",
    "fairness_gender_la1.dropna(subset = ['average_precision_score', 'false_discovery_rate','false_omission_rate'], inplace = True)\n",
    "fairness_age_la1['LA'] = 1\n",
    "fairness_age_la1.dropna(subset = ['average_precision_score', 'false_discovery_rate','false_omission_rate'], inplace = True)\n",
    "fairness_age_la1.rename(columns = {'Age_at_Referral_Start_cut': 'Age_cut'}, inplace = True) # Rename so names are same for all LAs\n",
    "fairness_disabled_la1['LA'] = 1\n",
    "fairness_disabled_la1.dropna(subset = ['average_precision_score', 'false_discovery_rate','false_omission_rate'], inplace = True)\n",
    "fairness_ethnicity_la1['LA'] = 1\n",
    "fairness_ethnicity_la1.dropna(subset = ['average_precision_score', 'false_discovery_rate','false_omission_rate'], inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LA2\n",
    "# NB ethnicity not available\n",
    "fairness_gender_la2 = aggregate_results(file_path = '/Models', result_name = 'Fairness_metrics_Gender_', col_names = [], la_name = 2, pivot = False)\n",
    "fairness_age_la2 = aggregate_results(file_path = '/Models', result_name = 'Fairness_metrics_Age_cut_', col_names = [], la_name = 2, pivot = False)\n",
    "fairness_disabled_la2 = aggregate_results(file_path = '/Models', result_name = 'Fairness_metrics_Disabled_', col_names = [], la_name = 2, pivot = False)\n",
    "\n",
    "# Add LA name and drop rows with NA in metrics and here the group\n",
    "fairness_gender_la2['LA'] = 2\n",
    "fairness_gender_la2.dropna(subset = ['Gender', 'average_precision_score', 'false_discovery_rate','false_omission_rate'], inplace = True)\n",
    "fairness_age_la2['LA'] = 2\n",
    "fairness_age_la2.dropna(subset = ['Age_cut', 'average_precision_score', 'false_discovery_rate','false_omission_rate'], inplace = True)\n",
    "fairness_age_la2 = fairness_age_la2.loc[fairness_age_la2['Age_cut'] != 'missing',]\n",
    "fairness_disabled_la2['LA'] = 2\n",
    "fairness_disabled_la2.dropna(subset = ['Disabled', 'average_precision_score', 'false_discovery_rate','false_omission_rate'], inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring together for LA1 and LA2\n",
    "fairness_gender = pd.concat([fairness_gender_la1, fairness_gender_la2], axis = 0)\n",
    "fairness_gender.reset_index(inplace = True, drop = True)\n",
    "\n",
    "fairness_age = pd.concat([fairness_age_la1, fairness_age_la2], axis = 0)\n",
    "fairness_age.reset_index(inplace = True, drop = True)\n",
    "\n",
    "fairness_disabled = pd.concat([fairness_disabled_la1, fairness_disabled_la2], axis = 0)\n",
    "fairness_disabled.reset_index(inplace = True, drop = True)\n",
    "\n",
    "# Ethnicity not available for LA2\n",
    "fairness_ethnicity = fairness_ethnicity_la1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fairness_rankings(df_all, subgroup_name):\n",
    "    '''\n",
    "    Subsets the data by model and then ranks the performance metric of each subgroup.\n",
    "    Finds the median ranking for each subgroup when all models are considered together.\n",
    "    \n",
    "    Parameters\n",
    "    - df_all takes in the output of aggregate_results where result_name = 'Fairness_metrics_*'\n",
    "    - subgroup_name: name of the group e.g. Gender. Needs to match the column name in df_all\n",
    "    \n",
    "    '''\n",
    "    fairness_rankings_all = pd.DataFrame()\n",
    "    # assumes low ranks are good\n",
    "    for la in [1, 2]:\n",
    "        for model in ['rq1_ss_str', 'rq1_ts_str', 'rq2_ts_str', 'rq2_ss_str']:\n",
    "            df_fair = df_all.loc[(df_all['model'] == model) & (df_all['LA'] == la),]\n",
    "            # Average Precision (higher is better => descending so highest value receives the lowest (i.e. best) rank)\n",
    "            df_fair['average_precision_score_rank'] = df_fair['average_precision_score'].rank(ascending = False)\n",
    "            # False Discovery Rate (lower is better => ascending so lowest value receives the lowest (i.e. best) rank)\n",
    "            df_fair['false_discovery_rate_rank'] = df_fair['false_discovery_rate'].rank(ascending = True) \n",
    "            # False Omission Rate\n",
    "            df_fair['false_omission_rate_rank'] = df_fair['false_omission_rate'].rank(ascending = True)\n",
    "            fairness_rankings_all = pd.concat([fairness_rankings_all, df_fair], axis = 0) \n",
    "            fairness_rankings_all_median = fairness_rankings_all.groupby([subgroup_name])[['average_precision_score_rank', 'false_discovery_rate_rank', 'false_omission_rate_rank']].median()\n",
    "            fairness_rankings_all_median = fairness_rankings_all_median.add_prefix('median_')\n",
    "            fairness_rankings_all_median.reset_index(inplace = True)\n",
    "            fairness_rankings_all_median.sort_values(by = 'median_average_precision_score_rank', inplace = True)\n",
    "    return(fairness_rankings_all_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes low ranks are good\n",
    "fairness_gender_w_rankings = add_fairness_rankings(fairness_gender, 'Gender')\n",
    "fairness_age_w_rankings = add_fairness_rankings(fairness_age, 'Age_cut')\n",
    "fairness_disabled_w_rankings = add_fairness_rankings(fairness_disabled, 'Disabled')\n",
    "fairness_ethnicity_w_rankings = add_fairness_rankings(fairness_ethnicity, 'Ethnicity_grouped')\n",
    "\n",
    "fairness_gender_w_rankings.to_csv('Fairness - median ranking by subgroup Gender.csv')\n",
    "fairness_age_w_rankings.to_csv('Fairness - median ranking by subgroup Age.csv')\n",
    "fairness_disabled_w_rankings.to_csv('Fairness - median ranking by subgroup Disabled.csv')\n",
    "fairness_ethnicity_w_rankings.to_csv('Fairness - median ranking by subgroup Ethnicity.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_intervals_la2 = aggregate_results(file_path = '/Models/Prediction Intervals', result_name = 'Prediction intervals - max, ave', col_names = [], la_name = 2, pivot = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the average of average width of prediction interval and width of prediction interval at threshold value\n",
    "# Gives an indication of how precise the prediction probabilities are\n",
    "average_prediction_intervals_la2 = pd.DataFrame()\n",
    "average_prediction_intervals_la2.loc[0, 'Average average width of prediction interval'] = round(prediction_intervals_la2['Average width of prediction interval'].mean(), 4)\n",
    "average_prediction_intervals_la2.loc[0, 'Average width of prediction interval at threshold value'] = round(prediction_intervals_la2['Width of prediction interval at threshold value'].mean(), 4)\n",
    "average_prediction_intervals_la2.to_csv('Average prediction intervals LA2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
