{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modelling\n",
    "\n",
    " - Conducts TFIDF and LDA on training data \n",
    " - Note deviation from trial protocol which stated that we would chose based on coherence - coherence is not available in sklearn and sklearn wrapper for the gensim coherence was buggy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable set-up\n",
    "rq = 'rq1' # Options: 'rq1', 'rq2'\n",
    "cv = 'ss' # Options: 'ts' (time series split, ignore siblings), 'ss' (stratified shuffle, ignore siblings)\n",
    "data_type = 'all' # Options: 'str' (just structured data), 'all' (structured data and list of strings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to vary\n",
    "parameters = {'tfidf__max_features': [25, 30], # Relatively small numbers (keep relatively simple)\n",
    "            'lda__n_components': [2, 4, 6], # Relatively small numbers (keep relatively simple)\n",
    "              'lda__max_iter': [10, 100], # Max learning iterations\n",
    "              'lda__learning_decay': [.7], # Doesn't vary much\n",
    "               'lda__batch_size': [64] # Doesn't vary much\n",
    "             } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rq == 'rq1':\n",
    "    text_cols = (['Contact and Referral Form_text',\n",
    "       'Child Social Work Assessment for Review Child Protection Conference_text_prev',\n",
    "       'Child Social Work Assessment to Initial Child Protection Conference_text_prev',\n",
    "       'Child Social Work Assessment_text_prev'])\n",
    "else:\n",
    "    text_cols = (['Child Social Work Assessment for Review Child Protection Conference_text',\n",
    "           'Child Social Work Assessment to Initial Child Protection Conference_text',\n",
    "           'Child Social Work Assessment_text', 'Contact and Referral Form_text',\n",
    "           'Child Social Work Assessment for Review Child Protection Conference_text_prev',\n",
    "           'Child Social Work Assessment to Initial Child Protection Conference_text_prev',\n",
    "           'Child Social Work Assessment_text_prev',\n",
    "           'Contact and Referral Form_text_prev'])\n",
    "\n",
    "file_stub_y_siblings = rq + '_' + cv + '_str' # use 'str' for all \n",
    "file_stub = rq + '_' + cv + '_' + data_type # Creates file stub for saving in the format e.g. rq1_ss_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to system path so we can find all the packages\n",
    "import sys\n",
    "sys.path\n",
    "sys.path.append('C:\\\\Program Files\\\\Python37\\\\Lib\\\\site-packages')\n",
    "sys.path.append('C:\\Program Files\\Python37')\n",
    "\n",
    "# Load user-written functions\n",
    "\n",
    "get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "get_ipython().run_line_magic('autoreload', '2')\n",
    "\n",
    "#import text_functions\n",
    "\n",
    "import analysis_functions, text_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only want to train the LDA on training data (otherwise there's leakage from the test  data)\n",
    "# NB df_text_list_of_strings dataset indices reset in 5_Combine_Data\n",
    "# Siblings dataset indices need to be reset to match \n",
    "# (not done earlier because structured dataset still needed with original indices and marginally \n",
    "# helpful siblings for siblings to match the structured dataset)\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Set working directory\n",
    "os.chdir(\"C:\\\\Users\\\\[username]\\\\Downloads\\\\Updated Text Data\\\\Created\\\\\") # inser [username]\n",
    "\n",
    "filename = open(\"df_text_list_of_strings_train_{}.pkl\".format(file_stub),\"rb\")\n",
    "df_text_list_of_strings_train = pickle.load(filename)\n",
    "print(df_text_list_of_strings_train.shape)\n",
    "print(df_text_list_of_strings_train.index)\n",
    "\n",
    "filename = open(\"../../Data for Model/y_train_{}.pkl\".format(file_stub_y_siblings), \"rb\")\n",
    "y_train = pickle.load(filename)\n",
    "print(y_train.shape)\n",
    "y_train.reset_index(inplace = True, drop = True)\n",
    "print(y_train.index)\n",
    "\n",
    "filename = open(\"../../Data for Model/siblings_train_{}.pkl\".format(file_stub_y_siblings), \"rb\")\n",
    "siblings_train = pickle.load(filename)\n",
    "print(siblings_train.shape)\n",
    "siblings_train.reset_index(inplace = True, drop = True)\n",
    "print(siblings_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from analysis_functions import TimeSeriesSplitIgnoreSiblings, StratifiedShuffleSplitGroups, grid_search_save_output\n",
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline # ok to use here as not considering the imbalance of the data => no need for imblearn pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection._split import TimeSeriesSplit, GroupKFold\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words = 'english',\n",
    "                        lowercase = True,\n",
    "                        min_df = 1, # lower min_df means more pruning => fewer tokens to build models on, 1 = in only one document, otherwise fraction\n",
    "                        token_pattern=\"\\\\b[a-zA-Z][a-zA-Z]+\\\\b\",#words with >= 2 alpha chars \n",
    "                       use_idf = True) \n",
    "\n",
    "lda_model = LatentDirichletAllocation(learning_method='online',  # Online is faster\n",
    "                                      random_state=3005,          # Random state\n",
    "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                      #n_jobs = -1 # Use all available CPUs\n",
    "                                     )               \n",
    "\n",
    "# Define Search Param\n",
    "pipeline_lda = (Pipeline([\n",
    "                    ('tfidf',tfidf),\n",
    "                    ('lda', lda_model)]))\n",
    "\n",
    "# Create cross-validation splits\n",
    "if cv == 'ts':\n",
    "    tss_sib = TimeSeriesSplitIgnoreSiblings(n_splits=3, sibling_group = siblings_train, sibling_na = \"99999.0\")\n",
    "    cross_val = tss_sib\n",
    "else:\n",
    "    sssg = StratifiedShuffleSplitGroups(n_splits=3, random_state=3005, sibling_group = siblings_train, sibling_na = '99999.0')\n",
    "    cross_val = sssg\n",
    "\n",
    "                  \n",
    "column_model_dict = {}\n",
    "column_model_cv_results_dict = {}\n",
    "\n",
    "for col in text_cols:\n",
    "    try:\n",
    "        gscv = GridSearchCV(pipeline_lda, parameters, cv=cross_val, verbose=5, refit = True)\n",
    "        print(gscv)\n",
    "        df_all, best_parameters, best_estimator = grid_search_save_output(gscv, 'lda', df_text_list_of_strings_train[col], y_train, '../../Models/topic_modelling_results_{}_{}.csv'.format(file_stub, col))\n",
    "        # Best Model\n",
    "        column_model_dict[col] = best_estimator\n",
    "        column_model_cv_results_dict[col] = df_all\n",
    "        print(\"Column: \", col)\n",
    "        print(\"Best Model's Params: \", best_parameters)\n",
    "        print(\"Best score: \", gscv.best_score_)\n",
    "        with open(\"../../Models/column_model_cv_results_dict_{}.pkl\".format(file_stub), \"wb\") as handle:\n",
    "            pickle.dump(column_model_cv_results_dict, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "        with open(\"../../Models/column_model_dict_{}.pkl\".format(file_stub), \"wb\") as handle:\n",
    "            pickle.dump(column_model_dict, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    except(ValueError):\n",
    "        print(col)\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at which words are most frequent in a topic\n",
    "# Gives an indication of how to name topics\n",
    "from text_functions import topic_top_words\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "filename = open(\"../../Models/column_model_dict_{}.pkl\".format(file_stub), \"rb\")\n",
    "column_model_dict = pickle.load(filename)\n",
    "\n",
    "n_top_words = 15\n",
    "avoid_list = [\"name\", \"surname\", \"date\", \"child\", \"social\", \"care\",  \"mr\", \"ms\", \"mrs\"]\n",
    "\n",
    "print(\"Topics in LDA model:\")\n",
    "\n",
    "\n",
    "for col in text_cols:\n",
    "    try:\n",
    "        print(\"Column: \", col)\n",
    "        lda = column_model_dict[col]['lda']\n",
    "        tfidf_vectorizer = column_model_dict[col]['tfidf']\n",
    "        tf_feature_names = tfidf_vectorizer.get_feature_names() \n",
    "        topic_top_words_dict = topic_top_words(lda, tf_feature_names, n_top_words, avoid_list)\n",
    "        filepath = \"../../Topics/top_words_topics_{}_{}.txt\".format(file_stub, col)\n",
    "        with open(filepath, 'w') as file_handler:\n",
    "            for key, value in topic_top_words_dict.items():\n",
    "                print(key, [v[0] for i, v in enumerate(value)])\n",
    "                file_handler.writelines(\"Topics {} {} {}\\n\".format(file_stub, key, [v[0] for i, v in enumerate(value)]))\n",
    "    except(KeyError):\n",
    "        print(\"Error in : \", col)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word clouds of highest weighted topics - one figure per column\n",
    "from text_functions import create_wordcloud\n",
    "\n",
    "wordcloud_avoid_list = [\"surname\", \"date\", \"child\", \"children\", \"yes\", \"social\", \"care\", \"nan\", \"single\",  \"mr\", \"ms\", \"mrs\"]\n",
    "\n",
    "for col in text_cols:\n",
    "    try:\n",
    "        create_wordcloud(column_model_dict[col], wordcloud_avoid_list, 15, file_stub, col, 'tfidf', 2)\n",
    "    except(KeyError):\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add TFIDF features and topics as features\n",
    "# ts\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "filename = open(\"../../Models/column_model_dict_{}.pkl\".format(file_stub), \"rb\")\n",
    "column_model_dict = pickle.load(filename)\n",
    "\n",
    "df_tfidf_all = pd.DataFrame()\n",
    "df_topics_all = pd.DataFrame()\n",
    "for col in column_model_dict.keys(): \n",
    "    # Make dataframe of documents x terms\n",
    "    tfidf_vectorizer = column_model_dict[col]['tfidf']\n",
    "    tfidf_vecs = tfidf_vectorizer.fit_transform(df_text_list_of_strings_train[col])\n",
    "    df_tfidf = pd.DataFrame(tfidf_vecs.todense(), columns=tfidf_vectorizer.get_feature_names())\n",
    "    df_tfidf = df_tfidf.add_prefix(col + '_') # CORRECTED\n",
    "    df_tfidf_all = pd.concat([df_tfidf_all, df_tfidf], axis=1)\n",
    "    # Make dataframe of documents x topics\n",
    "    lda = column_model_dict[col]['lda']\n",
    "    df_topics = lda.transform(tfidf_vecs)\n",
    "    topic_columns = ['{}_Topic_{}'.format(col, n) for n in range(df_topics.shape[1])]\n",
    "    df_topics = pd.DataFrame(df_topics, columns = topic_columns)\n",
    "    df_topics_all = pd.concat([df_topics_all, df_topics], axis=1)\n",
    "\n",
    "with open(\"../../Data for Model/df_tfidf_all_train_{}.pkl\".format(file_stub), \"wb\") as handle:\n",
    "    pickle.dump(df_tfidf_all, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(\"../../Data for Model/df_topics_all_train_{}.pkl\".format(file_stub), \"wb\") as handle:\n",
    "    pickle.dump(df_topics_all, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# Combine tfidf and topics\n",
    "keep_columns = ['ReferralDatetime', 'PSID']\n",
    "df_text_tfidf_features_topics_train = pd.concat([df_text_list_of_strings_train[keep_columns], df_tfidf_all, df_topics_all], axis = 1)\n",
    "\n",
    "with open(\"../../Updated Structured Data/Created/df_tfidf_topics_train_{}.pkl\".format(file_stub), \"wb\") as handle:\n",
    "    pickle.dump(df_text_tfidf_features_topics_train, handle, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use same vectoriser and LDA model to create TFIDF and topics for test  data\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import traceback\n",
    "\n",
    "# Bring in test  list of strings\n",
    "filename = open(\"../../Updated Structured Data/Created/df_text_list_of_strings_test_{}.pkl\".format(file_stub), \"rb\")\n",
    "df_text_list_of_strings_test = pickle.load(filename)\n",
    "print(df_text_list_of_strings_test.shape)\n",
    "print(df_text_list_of_strings_test.index)\n",
    "\n",
    "\n",
    "# Bring in previous models\n",
    "filename = open(\"../../Models/column_model_dict_{}.pkl\".format(file_stub), \"rb\")\n",
    "column_model_dict = pickle.load(filename)\n",
    "\n",
    "## Test\n",
    "df_tfidf_all = pd.DataFrame()\n",
    "df_topics_all = pd.DataFrame()\n",
    "for col in column_model_dict.keys(): \n",
    "    try:\n",
    "        # Make dataframe of documents x terms\n",
    "        tfidf_vectorizer = column_model_dict[col]['tfidf']\n",
    "        tfidf_vecs = tfidf_vectorizer.transform(df_text_list_of_strings_test[col])\n",
    "        df_tfidf = pd.DataFrame(tfidf_vecs.todense(), columns=tfidf_vectorizer.get_feature_names())\n",
    "        df_tfidf = df_tfidf.add_prefix(col + '_')\n",
    "        df_tfidf_all = pd.concat([df_tfidf_all, df_tfidf], axis=1)\n",
    "        # Make dataframe of documents x topics\n",
    "        lda = column_model_dict[col]['lda']\n",
    "        df_topics = lda.transform(tfidf_vecs)\n",
    "        topic_columns = ['{}_Topic_{}'.format(col, n) for n in range(df_topics.shape[1])]\n",
    "        df_topics = pd.DataFrame(df_topics, columns = topic_columns)\n",
    "        df_topics_all = pd.concat([df_topics_all, df_topics], axis=1)        \n",
    "    except Exception as exc:\n",
    "        print(traceback.format_exc())\n",
    "        print(exc)\n",
    "        continue\n",
    "\n",
    "# Run number 7 to combine all\n",
    "with open(\"../../Updated Structured Data/Created/df_tfidf_all_test_{}.pkl\".format(file_stub), \"wb\") as handle:\n",
    "    pickle.dump(df_tfidf_all, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(\"../../Updated Structured Data/Created/df_topics_all_test_{}.pkl\".format(file_stub), \"wb\") as handle:\n",
    "    pickle.dump(df_topics_all, handle, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine tfidf and topics\n",
    "keep_columns = ['ReferralDatetime', 'PSID']\n",
    "# Reset indices of new dataframes to match list of strings index => lines up for concatenation\n",
    "df_tfidf_all.set_index(df_text_list_of_strings_test.index, drop = True, append = False, inplace = True)\n",
    "df_topics_all.set_index(df_text_list_of_strings_test.index, drop = True, append = False, inplace = True)\n",
    "df_text_tfidf_topics_test = pd.concat([df_text_list_of_strings_test[keep_columns], df_tfidf_all, df_topics_all], axis = 1)\n",
    "\n",
    "print(df_tfidf_all.shape)\n",
    "print(df_topics_all.shape)\n",
    "print(df_text_tfidf_topics_test.shape)\n",
    "\n",
    "with open(\"../../Updated Structured Data/Created/df_tfidf_topics_test_{}.pkl\".format(file_stub), \"wb\") as handle:\n",
    "    pickle.dump(df_text_tfidf_topics_test, handle, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
