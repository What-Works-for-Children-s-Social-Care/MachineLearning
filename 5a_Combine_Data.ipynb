{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# ## Structured Data\n",
    "# \n",
    "# - Create final dataset for each research question\n",
    "# - Create test, train, holdout splits\n",
    "\n",
    "# Load user-written functions\n",
    "\n",
    "get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "get_ipython().run_line_magic('autoreload', '2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Set working directory\n",
    "os.chdir(\"C:\\\\Users\\\\[username]\\\\Downloads\\\\Updated Structured Data\\\\Created\") # insert [username]\n",
    "\n",
    "rq1data = pd.read_csv(\"rq1data_w_previous.csv\", index_col = 0)\n",
    "print(rq1data.shape)\n",
    "print(rq1data.index)\n",
    "print(rq1data.columns)\n",
    "\n",
    "rq2data = pd.read_csv(\"rq2data_w_previous.csv\", index_col = 0)\n",
    "print(rq2data.shape)\n",
    "print(rq2data.index)\n",
    "print(rq2data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object to categorical (analysis pipeline doesn't accept objects)\n",
    "rq1data[['detailed_ethnicity', 'Gender', 'ReReferral','hasdisability', 'ethnicity_highlevel']] = rq1data[['detailed_ethnicity', 'Gender', 'ReReferral','hasdisability', 'ethnicity_highlevel']].astype('category')\n",
    "rq2data[['detailed_ethnicity', 'Gender', 'ReReferral','hasdisability', 'ethnicity_highlevel']] = rq2data[['detailed_ethnicity', 'Gender', 'ReReferral','hasdisability', 'ethnicity_highlevel']].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop prescient columns / ID columns / columns used for matching\n",
    "(rq1data.drop(columns = ['NFAandreturnwithintwoyears', 'ActualStartDate', 'ReferralCloseDate',\n",
    "       'ReferralDatetime_previous', 'ReferralCloseDate_previous',\n",
    "       'ActualStartDate_previous'], inplace = True))\n",
    "(rq2data.drop(columns = ['ActualStartDate', 'ReferralCloseDate',\n",
    "       'ReferralDatetime_previous', 'ReferralCloseDate_previous',\n",
    "       'ActualStartDate_previous'], inplace = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by ReferralDatetime and PSID and reset indices so that it matches with the text data\n",
    "import pickle\n",
    "\n",
    "rq1data.sort_values(by = ['PSID', 'ReferralDatetime'], inplace = True)\n",
    "rq1data.reset_index(drop = True, inplace = True)\n",
    "rq2data.sort_values(by = ['PSID', 'ReferralDatetime'], inplace = True)\n",
    "rq2data.reset_index(drop = True, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop na in outcome variable\n",
    "print(rq1data.shape)\n",
    "rq1data.dropna(subset = ['NFAandreturnwithinoneyear'], inplace = True)\n",
    "print(rq1data.shape)\n",
    "\n",
    "print(rq2data.shape)\n",
    "rq2data.dropna(subset = ['escalation'], inplace = True)\n",
    "print(rq2data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Those above 18 aren't eligible for CPP or being LAC \n",
    "# Drop from analysis\n",
    "print(rq1data.shape)\n",
    "print(\"Age missing: \", rq1data['AgeAtReferralDate'].isna().sum())\n",
    "print(rq1data['AgeAtReferralDate'].value_counts().sort_index())\n",
    "rq1data = rq1data.loc[(rq1data['AgeAtReferralDate'] <18.0) | (rq1data['AgeAtReferralDate'].isna()),]\n",
    "print(rq1data['AgeAtReferralDate'].value_counts().sort_index())\n",
    "print(rq1data.shape)\n",
    "\n",
    "print(rq2data.shape)\n",
    "print(\"Age missing: \", rq2data['AgeAtReferralDate'].isna().sum())\n",
    "print(rq2data['AgeAtReferralDate'].value_counts().sort_index())\n",
    "rq2data = rq2data.loc[(rq2data['AgeAtReferralDate'] <18.0)  | (rq2data['AgeAtReferralDate'].isna()),]\n",
    "print(rq2data['AgeAtReferralDate'].value_counts().sort_index()) \n",
    "print(rq2data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating month / year of referral date to sort by for the time series splitting\n",
    "rq1data['ReferralDatetime'] = pd.to_datetime(rq1data['ReferralDatetime'])\n",
    "dates = pd.DataFrame()\n",
    "dates['year'] = rq1data['ReferralDatetime'].dt.year\n",
    "dates['month'] = rq1data['ReferralDatetime'].dt.month\n",
    "dates['day'] = 1\n",
    "rq1data['ReferralDatetime_month_year'] = pd.to_datetime(dates)\n",
    "print(rq1data['ReferralDatetime_month_year'].isna().sum())\n",
    "\n",
    "rq2data['ReferralDatetime'] = pd.to_datetime(rq2data['ReferralDatetime'])\n",
    "dates = pd.DataFrame()\n",
    "dates['year'] = rq2data['ReferralDatetime'].dt.year\n",
    "dates['month'] = rq2data['ReferralDatetime'].dt.month\n",
    "dates['day'] = 1\n",
    "rq2data['ReferralDatetime_month_year'] = pd.to_datetime(dates)\n",
    "print(rq2data['ReferralDatetime_month_year'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each PSID must have only one PseudoID for the purposes of keeping together multiple observations of the same child over time\n",
    "# in cross-validation\n",
    "\n",
    "# rq1\n",
    "sib_unique = rq1data.groupby('PSID')['PseudoID'].nunique()\n",
    "assert (sib_unique == 1).all()\n",
    "\n",
    "#rq2\n",
    "sib_unique = rq2data.groupby('PSID')['PseudoID'].nunique()\n",
    "assert (sib_unique == 1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate sensitive dataset for evaluating bias\n",
    "rq1data_sensitive = rq1data[['PSID', 'ReferralDatetime', \n",
    "                             'ReferralDatetime_month_year',\n",
    "                            'detailed_ethnicity',\n",
    "                             'ethnicity_highlevel',\n",
    "                             'hasdisability',\n",
    "                             'numberofdisabilities']]\n",
    "\n",
    "print(rq1data_sensitive.shape)\n",
    "print(rq1data_sensitive.columns)\n",
    "rq2data_sensitive = rq2data[['PSID', 'ReferralDatetime', \n",
    "                             'ReferralDatetime_month_year',\n",
    "                            'detailed_ethnicity',\n",
    "                             'ethnicity_highlevel',\n",
    "                             'hasdisability',\n",
    "                             'numberofdisabilities']]\n",
    "\n",
    "print(rq2data_sensitive.shape)\n",
    "print(rq2data_sensitive.columns)\n",
    "with open(\"df_outcome1_sensitive_characteristics.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(rq1data_sensitive, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(\"df_outcome2_sensitive_characteristics.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(rq2data_sensitive, handle, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save data sorted by referral start date\n",
    "# From this dataset springs the X, y and siblings => sorting before splitting ensures the datasets line up\n",
    "rq1data = rq1data.sort_values(by = ['ReferralDatetime_month_year'])\n",
    "rq1data.reset_index(inplace = True, drop = True)\n",
    "\n",
    "rq2data = rq2data.sort_values(by = ['ReferralDatetime_month_year'])\n",
    "rq2data.reset_index(inplace = True, drop = True)\n",
    "\n",
    "with open(\"df_outcome1_before_splitting.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(rq1data, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(\"df_outcome2_before_splitting.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(rq2data, handle, protocol = pickle.HIGHEST_PROTOCOL)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test, train, holdout split - RQ1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data ready for test / train / holdout split (already sorted by referral date)\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Set working directory\n",
    "os.chdir(\"C:\\\\Users\\\\[username]\\\\Downloads\\\\Updated Structured Data\\\\Created\") # insert [username]\n",
    "\n",
    "filename = open(\"df_outcome1_before_splitting.pkl\", \"rb\")\n",
    "df_outcome1_before_splitting = pickle.load(filename)\n",
    "print(df_outcome1_before_splitting.shape)\n",
    "\n",
    "filename = open(\"df_outcome2_before_splitting.pkl\", \"rb\")\n",
    "df_outcome2_before_splitting = pickle.load(filename)\n",
    "print(df_outcome2_before_splitting.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stratified shuffle splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rq1, ss\n",
    "from analysis_functions import StratifiedShuffleSplitGroups, create_test_train_splits\n",
    "\n",
    "siblings = df_outcome1_before_splitting['PseudoID'] # pandas Series\n",
    "df_outcome1_before_splitting_no_sib = df_outcome1_before_splitting.drop(columns = ['PseudoID'])\n",
    "outcome = 'NFAandreturnwithinoneyear'\n",
    "\n",
    "# Uses user-written StratifiedShuffleSplitGroups and create_test_train_splits\n",
    "# create_test_train_splits saves test, train and holdout data for X, y and siblings in Created folder\n",
    "# n_splits=1 for ss because create_test_train_splits then splits the second split into 2 for test and holdout\n",
    "# otherwise overlap between test and holdout (not the same for ts)\n",
    "ss = StratifiedShuffleSplitGroups(n_splits=1, test_size = .4, sibling_group = siblings, sibling_na = \"99999.0\", random_state=3005)\n",
    "create_test_train_splits(df_outcome1_before_splitting_no_sib, ss, 'ss', outcome, siblings, 'rq1_ss_str')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check no overlapping siblings or child ids\n",
    "# Check no overlapping siblings (except value denoting missing)\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Set working directory\n",
    "os.chdir(\"C:\\\\Users\\\\[username]\\\\Downloads\\\\Updated Structured Data\\\\Created\") # insert [username]\n",
    "\n",
    "filename = open(\"../../Data for Model/X_train_rq1_ss_str.pkl\", \"rb\")\n",
    "X_tr_ss = pickle.load(filename)\n",
    "print(X_tr_ss.shape)\n",
    "print(X_tr_ss.index)\n",
    "\n",
    "filename = open(\"../../Data for Model/X_test_rq1_ss_str.pkl\", \"rb\")\n",
    "X_test_ss = pickle.load(filename)\n",
    "print(X_test_ss.shape)\n",
    "print(X_test_ss.index)\n",
    "\n",
    "print(set(X_tr_ss['PSID']).intersection(set(X_test_ss['PSID'])))\n",
    "\n",
    "# There should actually be 0 overlapping PSIDs as there shouldn't be any missing \n",
    "# (==1 would allow for missing to be in both)\n",
    "assert len(list(set(X_tr_ss['PSID']).intersection(set(X_test_ss['PSID'])))) <= 1 \n",
    "\n",
    "filename = open(\"../../Data for Model/siblings_train_rq1_ss_str.pkl\", \"rb\")\n",
    "siblings_tr_ss = pickle.load(filename)\n",
    "print(siblings_tr_ss.shape)\n",
    "print(siblings_tr_ss.index)\n",
    "\n",
    "filename = open(\"../../Data for Model/siblings_test_rq1_ss_str.pkl\", \"rb\")\n",
    "siblings_test_ss = pickle.load(filename)\n",
    "print(siblings_test_ss.shape)\n",
    "print(siblings_test_ss.index)\n",
    "\n",
    "print(set(siblings_tr_ss).intersection(set(siblings_test_ss)))\n",
    "\n",
    "# There should actually be 0 overlapping PseudoIDs as there shouldn't be any missing \n",
    "# (==1 would allow for missing to be in both)\n",
    "assert len(list(set(siblings_tr_ss).intersection(set(siblings_test_ss)))) <= 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rq2, ss\n",
    "siblings = df_outcome2_before_splitting['PseudoID'] # pandas Series\n",
    "df_outcome2_before_splitting_no_sib = df_outcome2_before_splitting.drop(columns = ['PseudoID'])\n",
    "outcome = 'escalation'\n",
    "\n",
    "# Uses user-written StratifiedShuffleSplitGroups and create_test_train_splits\n",
    "# create_test_train_splits saves test, train and holdout data for X, y and siblings in Created folder\n",
    "# n_splits=1 for ss because create_test_train_splits then splits the second split into 2 for test and holdout\n",
    "# otherwise overlap between test and holdout (not the same for ts)\n",
    "ss = StratifiedShuffleSplitGroups(n_splits=1, test_size = 0.4, sibling_group = siblings, sibling_na = \"99999.0\", random_state=3005)\n",
    "create_test_train_splits(df_outcome2_before_splitting_no_sib, ss, 'ss', outcome, siblings, 'rq2_ss_str')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check no overlapping siblings or child ids\n",
    "# Check no overlapping siblings (except value denoting missing)\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Set working directory\n",
    "os.chdir(\"C:\\\\Users\\\\[username]\\\\Downloads\\\\Updated Structured Data\\\\Created\") # insert [username]\n",
    "\n",
    "filename = open(\"../../Data for Model/X_train_rq2_ss_str.pkl\", \"rb\")\n",
    "X_tr_ss = pickle.load(filename)\n",
    "print(X_tr_ss.shape)\n",
    "print(X_tr_ss.index)\n",
    "\n",
    "filename = open(\"../../Data for Model/X_test_rq2_ss_str.pkl\", \"rb\")\n",
    "X_test_ss = pickle.load(filename)\n",
    "print(X_test_ss.shape)\n",
    "print(X_test_ss.index)\n",
    "\n",
    "print(set(X_tr_ss['PSID']).intersection(set(X_test_ss['PSID'])))\n",
    "\n",
    "# There should actually be 0 overlapping PSIDs as there shouldn't be any missing \n",
    "# (==1 would allow for missing to be in both)\n",
    "assert len(list(set(X_tr_ss['PSID']).intersection(set(X_test_ss['PSID'])))) <= 1\n",
    "\n",
    "filename = open(\"../../Data for Model/siblings_train_rq2_ss_str.pkl\", \"rb\")\n",
    "siblings_tr_ss = pickle.load(filename)\n",
    "print(siblings_tr_ss.shape)\n",
    "print(siblings_tr_ss.index)\n",
    "\n",
    "filename = open(\"../../Data for Model/siblings_test_rq2_ss_str.pkl\", \"rb\")\n",
    "siblings_test_ss = pickle.load(filename)\n",
    "print(siblings_test_ss.shape)\n",
    "print(siblings_test_ss.index)\n",
    "\n",
    "print(set(siblings_tr_ss).intersection(set(siblings_test_ss)))\n",
    "\n",
    "# There should actually be 0 overlapping PseudoIDs as there shouldn't be any missing \n",
    "# (==1 would allow for missing to be in both)\n",
    "assert len(list(set(siblings_tr_ss).intersection(set(siblings_test_ss)))) <= 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time Series split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from analysis_functions import TimeSeriesSplitIgnoreSiblings, create_test_train_splits\n",
    "\n",
    "# rq1, ts\n",
    "siblings = df_outcome1_before_splitting['PseudoID'] # pandas Series\n",
    "df_outcome1_before_splitting_no_sib = df_outcome1_before_splitting.drop(columns = ['PseudoID'])\n",
    "outcome = 'NFAandreturnwithinoneyear'\n",
    "\n",
    "# Uses user-written functions TimeSeriesSplitIgnoreSiblings and create_test_train_splits\n",
    "# Splitting into 3 (2 splits) is fine because create_test_train_splits takes the first n_splits-1 folds as training\n",
    "# and the n_splits-th as test. Ok because there's no shuffling of data\n",
    "ts = TimeSeriesSplitIgnoreSiblings(n_splits=2, sibling_group = siblings, sibling_na = \"99999.0\")\n",
    "create_test_train_splits(df_outcome1_before_splitting_no_sib, ts, 'ts', outcome, siblings, 'rq1_ts_str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check no overlapping siblings or child ids\n",
    "# Check no overlapping siblings (except value denoting missing)\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Set working directory\n",
    "os.chdir(\"C:\\\\Users\\\\[username]\\\\Downloads\\\\Updated Structured Data\\\\Created\") # insert [username]\n",
    "\n",
    "filename = open(\"../../Data for Model/X_train_rq1_ts_str.pkl\", \"rb\")\n",
    "X_tr_ts = pickle.load(filename)\n",
    "print(X_tr_ts.shape)\n",
    "print(X_tr_ts.index)\n",
    "\n",
    "filename = open(\"../../Data for Model/X_test_rq1_ts_str.pkl\", \"rb\")\n",
    "X_test_ts = pickle.load(filename)\n",
    "print(X_test_ts.shape)\n",
    "print(X_test_ts.index)\n",
    "\n",
    "print(set(X_tr_ts['PSID']).intersection(set(X_test_ts['PSID'])))\n",
    "\n",
    "# There should actually be 0 overlapping PSIDs as there shouldn't be any missing \n",
    "# (==1 would allow for missing to be in both)\n",
    "assert len(list(set(X_tr_ts['PSID']).intersection(set(X_test_ts['PSID'])))) <= 1\n",
    "\n",
    "filename = open(\"../../Data for Model/siblings_train_rq1_ts_str.pkl\", \"rb\")\n",
    "siblings_tr_ts = pickle.load(filename)\n",
    "print(siblings_tr_ts.shape)\n",
    "print(siblings_tr_ts.index)\n",
    "\n",
    "filename = open(\"../../Data for Model/siblings_test_rq1_ts_str.pkl\", \"rb\")\n",
    "siblings_test_ts = pickle.load(filename)\n",
    "print(siblings_test_ts.shape)\n",
    "print(siblings_test_ts.index)\n",
    "\n",
    "print(set(siblings_tr_ts).intersection(set(siblings_test_ts)))\n",
    "\n",
    "# There should actually be 0 overlapping PseudoIDs as there shouldn't be any missing \n",
    "# (==1 would allow for missing to be in both)\n",
    "assert len(list(set(siblings_tr_ts).intersection(set(siblings_test_ts)))) <= 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from analysis_functions import TimeSeriesSplitIgnoreSiblings, create_test_train_splits\n",
    "\n",
    "# rq2, ts\n",
    "siblings = df_outcome2_before_splitting['PseudoID'] # pandas Series\n",
    "df_outcome2_before_splitting_no_sib = df_outcome2_before_splitting.drop(columns = ['PseudoID'])\n",
    "outcome = 'escalation'\n",
    "\n",
    "# Uses user-written functions TimeSeriesSplitIgnoreSiblings and create_test_train_splits\n",
    "# Splitting into 3 (2 splits) is fine because create_test_train_splits takes the first n_splits-1 folds as training\n",
    "# and the n_splits-th as test. Ok because there's no shuffling of data\n",
    "ts = TimeSeriesSplitIgnoreSiblings(n_splits=2, sibling_group = siblings, sibling_na = \"99999.0\")\n",
    "create_test_train_splits(df_outcome2_before_splitting_no_sib, ts, 'ts', outcome, siblings, 'rq2_ts_str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check no overlapping siblings or child ids\n",
    "# Check no overlapping siblings (except value denoting missing)\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Set working directory\n",
    "os.chdir(\"C:\\\\Users\\\\[username]\\\\Downloads\\\\Updated Structured Data\\\\Created\") # insert [username]\n",
    "\n",
    "filename = open(\"../../Data for Model/X_train_rq2_ts_str.pkl\", \"rb\")\n",
    "X_tr_ts = pickle.load(filename)\n",
    "print(X_tr_ts.shape)\n",
    "print(X_tr_ts.index)\n",
    "\n",
    "filename = open(\"../../Data for Model/X_test_rq2_ts_str.pkl\", \"rb\")\n",
    "X_test_ts = pickle.load(filename)\n",
    "print(X_test_ts.shape)\n",
    "print(X_test_ts.index)\n",
    "\n",
    "print(set(X_tr_ts['PSID']).intersection(set(X_test_ts['PSID'])))\n",
    "\n",
    "# There should actually be 0 overlapping PSIDs as there shouldn't be any missing \n",
    "# (==1 would allow for missing to be in both)\n",
    "assert len(list(set(X_tr_ts['PSID']).intersection(set(X_test_ts['PSID'])))) <= 1\n",
    "\n",
    "filename = open(\"../../Data for Model/siblings_train_rq2_ts_str.pkl\", \"rb\")\n",
    "siblings_tr_ts = pickle.load(filename)\n",
    "print(siblings_tr_ts.shape)\n",
    "print(siblings_tr_ts.index)\n",
    "\n",
    "filename = open(\"../../Data for Model/siblings_test_rq2_ts_str.pkl\", \"rb\")\n",
    "siblings_test_ts = pickle.load(filename)\n",
    "print(siblings_test_ts.shape)\n",
    "print(siblings_test_ts.index)\n",
    "\n",
    "print(set(siblings_tr_ts).intersection(set(siblings_test_ts)))\n",
    "\n",
    "# There should actually be 0 overlapping PseudoIDs as there shouldn't be any missing \n",
    "# (==1 would allow for missing to be in both)\n",
    "assert len(list(set(siblings_tr_ts).intersection(set(siblings_test_ts)))) <= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Just text data\n",
    "\n",
    "- Train, test, holdout split for list of strings data\n",
    "- Run after anonymisation (3) and text feature (4) notebooks to feed into tfidf and topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 1==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Set working directory\n",
    "os.chdir(\"C:\\\\Users\\\\[username]\\\\Downloads\\\\Updated Text Data\\\\Created\") # insert [username]\n",
    "\n",
    "filename = open(\"text_rq1_list_of_strings.pkl\", \"rb\")\n",
    "df_text_list_of_strings_rq1 = pickle.load(filename)\n",
    "print(df_text_list_of_strings_rq1.shape)\n",
    "print(df_text_list_of_strings_rq1.index)\n",
    "print(df_text_list_of_strings_rq1.columns)\n",
    "\n",
    "\n",
    "filename = open(\"text_rq2_list_of_strings.pkl\", \"rb\")\n",
    "df_text_list_of_strings_rq2 = pickle.load(filename)\n",
    "print(df_text_list_of_strings_rq2.shape)\n",
    "print(df_text_list_of_strings_rq2.index)\n",
    "print(df_text_list_of_strings_rq2.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Set working directory\n",
    "os.chdir(\"C:\\\\Users\\\\[username]\\\\Downloads\\\\Updated Structured Data\\\\Created\") # insert [username]\n",
    "\n",
    "# Import structured data (already sorted by ReferralDatetime)\n",
    "filename = open(\"df_outcome1_before_splitting.pkl\", \"rb\")\n",
    "df_outcome1_before_splitting = pickle.load(filename)\n",
    "print(df_outcome1_before_splitting.shape)\n",
    "\n",
    "\n",
    "filename = open(\"df_outcome2_before_splitting.pkl\", \"rb\")\n",
    "df_outcome2_before_splitting = pickle.load(filename)\n",
    "print(df_outcome2_before_splitting.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the same rows as the cleaned structured data\n",
    "import pandas as pd\n",
    "# Format \n",
    "df_text_list_of_strings_rq1['ReferralDatetime'] = pd.to_datetime(df_text_list_of_strings_rq1['ReferralDatetime'])\n",
    "#df_text_list_of_strings_rq2['ReferralDatetime'] = pd.to_datetime(df_text_list_of_strings_rq2['ReferralDatetime'])\n",
    "\n",
    "\n",
    "# rq1 - don't need to merge into but still need to sort\n",
    "df_text_list_of_strings_rq1 = pd.merge(df_outcome1_before_splitting[['PSID', 'ReferralDatetime', 'ReferralDatetime_month_year']], df_text_list_of_strings_rq1, on = ['PSID', 'ReferralDatetime'], how = 'left')\n",
    "df_text_list_of_strings_rq1.drop_duplicates(subset = ['PSID', 'ReferralDatetime'], inplace = True)\n",
    "df_text_list_of_strings_rq1.sort_values(by = 'ReferralDatetime_month_year', inplace = True)\n",
    "print(df_text_list_of_strings_rq1.shape)\n",
    "\n",
    "# rq2\n",
    "df_text_list_of_strings_rq2 = pd.merge(df_outcome2_before_splitting[['PSID', 'ReferralDatetime', 'ReferralDatetime_month_year']], df_text_list_of_strings_rq2, on = ['PSID', 'ReferralDatetime'], how = 'left')\n",
    "df_text_list_of_strings_rq2.drop_duplicates(subset = ['PSID', 'ReferralDatetime'], inplace = True)\n",
    "df_text_list_of_strings_rq2.sort_values(by = 'ReferralDatetime_month_year', inplace = True)\n",
    "print(df_text_list_of_strings_rq2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only want to train the LDA on training data (otherwise there's leakage from the test and holdout data)\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Set working directory\n",
    "os.chdir(\"C:\\\\Users\\\\[username]\\\\Downloads\\\\Updated Structured Data\\\\Created\") # insert [username]\n",
    "\n",
    "filename = open(\"../../Data for Model/X_train_rq1_ts_str.pkl\", \"rb\")\n",
    "X_train_rq1_ts = pickle.load(filename)\n",
    "print(X_train_rq1_ts.shape)\n",
    "print(X_train_rq1_ts.index)\n",
    "\n",
    "filename = open(\"../../Data for Model/X_test_rq1_ts_str.pkl\", \"rb\")\n",
    "X_test_rq1_ts = pickle.load(filename)\n",
    "print(X_test_rq1_ts.shape)\n",
    "print(X_test_rq1_ts.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only want to train the LDA on training data (otherwise there's leakage from the test and holdout data)\n",
    "\n",
    "import pickle\n",
    "\n",
    "filename = open(\"../../Data for Model/X_train_rq1_ss_str.pkl\", \"rb\")\n",
    "X_train_rq1_ss = pickle.load(filename)\n",
    "print(X_train_rq1_ss.shape)\n",
    "print(X_train_rq1_ss.index)\n",
    "\n",
    "filename = open(\"../../Data for Model/X_test_rq1_ss_str.pkl\", \"rb\")\n",
    "X_test_rq1_ss = pickle.load(filename)\n",
    "print(X_test_rq1_ss.shape)\n",
    "print(X_test_rq1_ss.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only want to train the LDA on training data (otherwise there's leakage from the test and holdout data)\n",
    "import pickle\n",
    "\n",
    "filename = open(\"../../Data for Model/X_train_rq2_ts_str.pkl\", \"rb\")\n",
    "X_train_rq2_ts = pickle.load(filename)\n",
    "print(X_train_rq2_ts.shape)\n",
    "print(X_train_rq2_ts.index)\n",
    "\n",
    "filename = open(\"../../Data for Model/X_test_rq2_ts_str.pkl\", \"rb\")\n",
    "X_test_rq2_ts = pickle.load(filename)\n",
    "print(X_test_rq2_ts.shape)\n",
    "print(X_test_rq2_ts.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = open(\"../../Data for Model/X_train_rq2_ss_str.pkl\", \"rb\")\n",
    "X_train_rq2_ss = pickle.load(filename)\n",
    "print(X_train_rq2_ss.shape)\n",
    "print(X_train_rq2_ss.index)\n",
    "\n",
    "filename = open(\"../../Data for Model/X_test_rq2_ss_str.pkl\", \"rb\")\n",
    "X_test_rq2_ss = pickle.load(filename)\n",
    "print(X_test_rq2_ss.shape)\n",
    "print(X_test_rq2_ss.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_list_of_strings_rq1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test, holdout split for list of strings for feeding into topic models and also modelling\n",
    "# Identify the right rows by merging the key columns from the train, test, holdout structured datasets\n",
    "\n",
    "text_columns = (['Contact and Referral Form_text',\n",
    "       'Child Social Work Assessment for Review Child Protection Conference_text_prev',\n",
    "       'Child Social Work Assessment to Initial Child Protection Conference_text_prev',\n",
    "       'Child Social Work Assessment_text_prev'])\n",
    "\n",
    "print(df_text_list_of_strings_rq1.index)\n",
    "df_text_list_of_strings_rq1_ts_train = pd.merge(X_train_rq1_ts[['PSID', 'ReferralDatetime']], df_text_list_of_strings_rq1, how = 'left', on = ['PSID', 'ReferralDatetime']) \n",
    "print(df_text_list_of_strings_rq1_ts_train.shape)\n",
    "df_text_list_of_strings_rq1_ts_train.reset_index(inplace = True, drop = True)\n",
    "print(df_text_list_of_strings_rq1_ts_train.index)\n",
    "df_text_list_of_strings_rq1_ts_train[text_columns] = df_text_list_of_strings_rq1_ts_train[text_columns].fillna('')\n",
    "print(df_text_list_of_strings_rq1_ts_train[text_columns].isna().sum())\n",
    "\n",
    "print(df_text_list_of_strings_rq1.index)\n",
    "df_text_list_of_strings_rq1_ts_test = pd.merge(X_test_rq1_ts[['PSID', 'ReferralDatetime']], df_text_list_of_strings_rq1, how = 'left', on = ['PSID', 'ReferralDatetime']) \n",
    "print(df_text_list_of_strings_rq1_ts_test.shape)\n",
    "df_text_list_of_strings_rq1_ts_test.reset_index(inplace = True, drop = True)\n",
    "print(df_text_list_of_strings_rq1_ts_test.index)\n",
    "df_text_list_of_strings_rq1_ts_test[text_columns] = df_text_list_of_strings_rq1_ts_test[text_columns].fillna('')\n",
    "print(df_text_list_of_strings_rq1_ts_test[text_columns].isna().sum())\n",
    "\n",
    "print(df_text_list_of_strings_rq1.index)\n",
    "df_text_list_of_strings_rq1_ss_train = pd.merge(X_train_rq1_ss[['PSID', 'ReferralDatetime']], df_text_list_of_strings_rq1, how = 'left', on = ['PSID', 'ReferralDatetime']) \n",
    "print(df_text_list_of_strings_rq1_ss_train.shape)\n",
    "df_text_list_of_strings_rq1_ss_train.reset_index(inplace = True, drop = True)\n",
    "print(df_text_list_of_strings_rq1_ss_train.index)\n",
    "df_text_list_of_strings_rq1_ss_train[text_columns] = df_text_list_of_strings_rq1_ss_train[text_columns].fillna('')\n",
    "print(df_text_list_of_strings_rq1_ss_train[text_columns].isna().sum())\n",
    "\n",
    "print(df_text_list_of_strings_rq1.index)\n",
    "df_text_list_of_strings_rq1_ss_test = pd.merge(X_test_rq1_ss[['PSID', 'ReferralDatetime']], df_text_list_of_strings_rq1, how = 'left', on = ['PSID', 'ReferralDatetime']) \n",
    "print(df_text_list_of_strings_rq1_ss_test.shape)\n",
    "df_text_list_of_strings_rq1_ss_test.reset_index(inplace = True, drop = True)\n",
    "print(df_text_list_of_strings_rq1_ss_test.index)\n",
    "df_text_list_of_strings_rq1_ss_test[text_columns] = df_text_list_of_strings_rq1_ss_test[text_columns].fillna('')\n",
    "print(df_text_list_of_strings_rq1_ss_test[text_columns].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training, test and holdout data\n",
    "\n",
    "# RQ1\n",
    "with open(\"df_text_list_of_strings_train_rq1_ts_all.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(df_text_list_of_strings_rq1_ts_train, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(\"df_text_list_of_strings_test_rq1_ts_all.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(df_text_list_of_strings_rq1_ts_test, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(\"df_text_list_of_strings_train_rq1_ss_all.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(df_text_list_of_strings_rq1_ss_train, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(\"df_text_list_of_strings_test_rq1_ss_all.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(df_text_list_of_strings_rq1_ss_test, handle, protocol = pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test, holdout split for list of strings for feeding into topic models and also modelling\n",
    "# Identify the right rows by merging the key columns from the train, test, holdout structured datasets\n",
    "\n",
    "print(df_text_list_of_strings_rq2.index)\n",
    "df_text_list_of_strings_rq2_ts_train = pd.merge(X_train_rq2_ts[['PSID', 'ReferralDatetime']], df_text_list_of_strings_rq2, how = 'left', on = ['PSID', 'ReferralDatetime']) \n",
    "print(df_text_list_of_strings_rq2_ts_train.shape)\n",
    "df_text_list_of_strings_rq2_ts_train.reset_index(inplace = True, drop = True)\n",
    "print(df_text_list_of_strings_rq2_ts_train.index)\n",
    "df_text_list_of_strings_rq2_ts_train[text_columns] = df_text_list_of_strings_rq2_ts_train[text_columns].fillna('')\n",
    "print(df_text_list_of_strings_rq2_ts_train[text_columns].isna().sum())\n",
    "\n",
    "print(df_text_list_of_strings_rq2.index)\n",
    "df_text_list_of_strings_rq2_ts_test = pd.merge(X_test_rq2_ts[['PSID', 'ReferralDatetime']], df_text_list_of_strings_rq2, how = 'left', on = ['PSID', 'ReferralDatetime']) \n",
    "print(df_text_list_of_strings_rq2_ts_test.shape)\n",
    "df_text_list_of_strings_rq2_ts_test.reset_index(inplace = True, drop = True)\n",
    "print(df_text_list_of_strings_rq2_ts_test.index)\n",
    "df_text_list_of_strings_rq2_ts_test[text_columns] = df_text_list_of_strings_rq2_ts_test[text_columns].fillna('')\n",
    "print(df_text_list_of_strings_rq2_ts_test[text_columns].isna().sum())\n",
    "\n",
    "print(df_text_list_of_strings_rq2.index)\n",
    "df_text_list_of_strings_rq2_ss_train = pd.merge(X_train_rq2_ss[['PSID', 'ReferralDatetime']], df_text_list_of_strings_rq2, how = 'left', on = ['PSID', 'ReferralDatetime']) \n",
    "print(df_text_list_of_strings_rq2_ss_train.shape)\n",
    "df_text_list_of_strings_rq2_ss_train.reset_index(inplace = True, drop = True)\n",
    "print(df_text_list_of_strings_rq2_ss_train.index)\n",
    "df_text_list_of_strings_rq2_ss_train[text_columns] = df_text_list_of_strings_rq2_ss_train[text_columns].fillna('')\n",
    "print(df_text_list_of_strings_rq2_ss_train[text_columns].isna().sum())\n",
    "\n",
    "print(df_text_list_of_strings_rq2.index)\n",
    "df_text_list_of_strings_rq2_ss_test = pd.merge(X_test_rq2_ss[['PSID', 'ReferralDatetime']], df_text_list_of_strings_rq2, how = 'left', on = ['PSID', 'ReferralDatetime']) \n",
    "print(df_text_list_of_strings_rq2_ss_test.shape)\n",
    "df_text_list_of_strings_rq2_ss_test.reset_index(inplace = True, drop = True)\n",
    "print(df_text_list_of_strings_rq2_ss_test.index)\n",
    "df_text_list_of_strings_rq2_ss_test[text_columns] = df_text_list_of_strings_rq2_ss_test[text_columns].fillna('')\n",
    "print(df_text_list_of_strings_rq2_ss_test[text_columns].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training, test and holdout data\n",
    "\n",
    "# RQ2\n",
    "with open(\"df_text_list_of_strings_train_rq2_ts_all.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(df_text_list_of_strings_rq2_ts_train, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(\"df_text_list_of_strings_test_rq2_ts_all.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(df_text_list_of_strings_rq2_ts_test, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(\"df_text_list_of_strings_train_rq2_ss_all.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(df_text_list_of_strings_rq2_ss_train, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(\"df_text_list_of_strings_test_rq2_ss_all.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(df_text_list_of_strings_rq2_ss_test, handle, protocol = pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change data to csvs for inspection\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "os.chdir(\"C:\\\\Users\\\\[username]\\\\Downloads\\\\Updated Structured Data\\\\Created\") # insert [username]\n",
    "filename_list = [file for file in glob.glob(\"df_text_list_of_strings_*.pkl\")]\n",
    "\n",
    "file_dict = {}\n",
    "for file in filename_list:\n",
    "    try:\n",
    "        filename = open(file, \"rb\")\n",
    "        f = pickle.load(filename)\n",
    "        file_n = re.sub('.pkl', '', file)\n",
    "        print(file_n)\n",
    "        f.to_csv('{}.csv'.format(file_n))\n",
    "    except(EOFError):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
