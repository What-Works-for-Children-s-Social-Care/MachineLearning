{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable set-up\n",
    "\n",
    "rq = 'rq2' # Options: 'rq1', 'rq2'\n",
    "cv = 'ss' # Options: 'ts' (time series split, ignore siblings), 'ss' (stratified shuffle, ignore siblings)\n",
    "data_type = 'str' # Options: 'str' (just structured data), 'all' (structured data and list of strings)\n",
    "algorithm_names = ['decision_tree', 'logistic_regression', 'gradient_boosting'] \n",
    "resampling_name = 'ada' # anything other than 'ada' does 'smote' \n",
    "select_features_alpha = 0.001 # Should be highest 0.001 as otherwise all dropped\n",
    "rcv_n_iter = 50 # The more iterations, the more the randomised search searches for an optimal solution\n",
    "parameters = 2 # \n",
    "\n",
    "# Don't change\n",
    "file_stub_y_siblings = rq + '_' + cv + '_str' # use 'str' for all \n",
    "file_stub = rq + '_' + cv + '_' + data_type # Creates file stub for saving in the format e.g. rq1_ss_str\n",
    "levers = resampling_name + '_' + str(select_features_alpha) + '_' + str(rcv_n_iter) + '_' + str(parameters)\n",
    "print(file_stub + '_' + levers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional variables\n",
    "\n",
    "bootstrap_num = 500\n",
    "algorithm_names = ['decision_tree', 'logistic_regression', 'gradient_boosting']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## File directories\n",
    "local_dir = '/Users/[username]/Documents/Final transfer out data and code to WWC Jan 2020' # insert [username]\n",
    "hard_drive_dir = '/Volumes/diskAshur2/Final transfer out data and code to WWC Jan 2020/Data for model/Use'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below here - generic set-up - don't vary. Change which models are run and output saved by varying the parameters above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load user-written functions \n",
    "\n",
    "get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "get_ipython().run_line_magic('autoreload', '2')\n",
    "\n",
    "import analysis_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory\n",
    "import os\n",
    "import pickle\n",
    "os.chdir(hard_drive_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import structured data -train, test split\n",
    "\n",
    "# Import structured data -train, test data\n",
    "import pandas as pd\n",
    "X_tr = pd.read_csv(\"Final/X_train_{}_final.csv\".format(file_stub), index_col = 0)\n",
    "print(X_tr.shape)\n",
    "X_tr.reset_index(inplace = True, drop = True)\n",
    "print(X_tr.index)\n",
    "\n",
    "X_test = pd.read_csv(\"Final/X_test_{}_final.csv\".format(file_stub), index_col = 0)\n",
    "print(X_test.shape)\n",
    "X_test.reset_index(inplace = True, drop = True)\n",
    "print(X_test.index)\n",
    "\n",
    "y_tr = pd.read_csv(\"y_train_{}.csv\".format(file_stub_y_siblings), index_col = 0, header = None)\n",
    "print(y_tr.shape)\n",
    "y_tr.reset_index(inplace = True, drop = True)\n",
    "y_tr = pd.Series(y_tr[1])\n",
    "print(y_tr.index)\n",
    "\n",
    "y_test = pd.read_csv(\"y_test_{}.csv\".format(file_stub_y_siblings), index_col = 0, header = None)\n",
    "print(y_test.shape)\n",
    "y_test.reset_index(inplace = True, drop = True)\n",
    "y_test = pd.Series(y_test[1])\n",
    "print(y_test.index)\n",
    "\n",
    "siblings_tr = pd.read_csv(\"siblings_train_{}.csv\".format(file_stub_y_siblings), index_col = 0, header = None)\n",
    "print(siblings_tr.shape)\n",
    "siblings_tr.reset_index(inplace = True, drop = True)\n",
    "siblings_tr = pd.Series(siblings_tr[1])\n",
    "print(siblings_tr.index)\n",
    "\n",
    "siblings_test = pd.read_csv(\"siblings_test_{}.csv\".format(file_stub_y_siblings), index_col = 0, header = None)\n",
    "print(siblings_test.shape)\n",
    "siblings_test.reset_index(inplace = True, drop = True)\n",
    "siblings_test = pd.Series(siblings_test[1])\n",
    "print(siblings_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop key columns (after data is saved as still need the keys for merging in 7a_Combine_and_Split_Data and 8_Fairness)\n",
    "print(X_tr.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "key_cols = ['PSID',  'ReferralDatetime', 'ReferralDatetime_month_year']\n",
    "X_tr.drop(columns = key_cols, inplace = True, errors = 'ignore')\n",
    "X_test.drop(columns = key_cols, inplace = True, errors = 'ignore')\n",
    "print(X_tr.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "X_tr = X_tr.select_dtypes(include = 'number') \n",
    "X_test = X_test.select_dtypes(include = 'number') \n",
    "print(X_tr.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import best model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "model_outputs = []\n",
    "try:\n",
    "    model_output_dtc = pd.read_csv(\"{}/Models/model_output_{}_decision_tree_{}.csv\".format(local_dir, file_stub, levers), index_col = 0)\n",
    "    model_outputs.append(model_output_dtc)\n",
    "except(FileNotFoundError):    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    model_output_lr = pd.read_csv(\"{}/Models/model_output_{}_logistic_regression_{}.csv\".format(local_dir, file_stub, levers), index_col = 0)\n",
    "    model_outputs.append(model_output_lr)\n",
    "except(FileNotFoundError):    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    model_output_gbc = pd.read_csv(\"{}/Models/model_output_{}_gradient_boosting_{}.csv\".format(local_dir, file_stub, levers), index_col = 0)\n",
    "    model_outputs.append(model_output_gbc)\n",
    "except(FileNotFoundError):    \n",
    "    pass\n",
    "\n",
    "model_output = pd.concat(model_outputs, axis = 0, ignore_index = True)\n",
    "# Find the model which finds the best worst case scenario\n",
    "max_min_test_score = np.argmax(model_output['mean_test_score'] - 2* model_output['std_test_score']) \n",
    "best_algorithm = model_output.loc[max_min_test_score,'algorithm']\n",
    "print(\"Best algorithm: \", best_algorithm)\n",
    "print(\"Max min score\", model_output.loc[max_min_test_score, 'mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load saved models\n",
    "filename = open(\"{}/Models/best_estimator_{}_dict.pkl\".format(local_dir, file_stub), \"rb\")\n",
    "best_estimator_dict = pickle.load(filename)\n",
    "fitted_model = best_estimator_dict[best_algorithm]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Predictive Inference\n",
    "\n",
    "# Bootstrapping for prediction intervals\n",
    "# https://stackoverflow.com/questions/47414842/confidence-interval-of-probability-prediction-from-logistic-regression-statsmode\n",
    "\n",
    "preprocessor = fitted_model['preprocessor']\n",
    "preds = {k: [] for k in X_test.index}\n",
    "for i in range(bootstrap_num):\n",
    "    print('Bootstrap number: ', i)\n",
    "    boot_idx = np.random.choice(X_tr.index, replace=True, size=X_tr.shape[0]) # pick indices at random of length of X_tr\n",
    "    if data_type == 'all':\n",
    "        X_tr_transformed = pd.DataFrame(preprocessor.fit_transform(X_tr))\n",
    "        fitted_model.fit(X_tr.loc[boot_idx,], y_tr[boot_idx]) # Fit the model\n",
    "    else:\n",
    "        try:\n",
    "            fitted_model.fit(X_tr.loc[boot_idx,], y_tr[boot_idx]) # Fit the model\n",
    "        except(ValueError):\n",
    "            continue\n",
    "    y_pred_prob_1 = [prob[1] for prob in fitted_model.predict_proba(X_test)] # Find the probabilities for all the predictions\n",
    "    y_pred_prob_1 = pd.Series(y_pred_prob_1, index = X_test.index) # Add the index\n",
    "    for idx in y_pred_prob_1.index:\n",
    "        preds[idx].append(y_pred_prob_1[idx]) # for each index, append the probability to a list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual predictions (90% prediction intervals)\n",
    "# Average width of prediction intervals\n",
    "import numpy as np\n",
    "widths, mean_probabilities = [], []\n",
    "for key, values in preds.items():\n",
    "    width = 1.645 * (np.std(preds[key]) / np.sqrt(len(preds[key]))) * 2 # 1.645 is z value for 90% interval\n",
    "    widths.append(width)\n",
    "    mean_probability = round(np.mean(preds[key]),2) # take the mean of the prediction probabilities\n",
    "    mean_probabilities.append(mean_probability)\n",
    "\n",
    "preds_interval_widths = dict(zip(preds.keys(), widths))\n",
    "preds_interval_mean_probs = dict(zip(preds.keys(), mean_probabilities))\n",
    "average_width = round(np.mean(widths), 4)\n",
    "print(\"Averge width of prediction interval: \", average_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the bootstrapped predictions \n",
    "\n",
    "with open(\"{}/Models/Prediction Intervals/preds_interval_widths_{}.pkl\".format(local_dir, file_stub), \"wb\") as handle:\n",
    "    pickle.dump(preds_interval_widths, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(\"{}/Models/Prediction Intervals/preds_interval_mean_probs_{}.pkl\".format(local_dir, file_stub), \"wb\") as handle:\n",
    "    pickle.dump(preds_interval_mean_probs, handle, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of predicted probabilities\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "algorithm_names_full = ['Decision tree', 'Logistic regression', 'Gradient boosting']\n",
    "\n",
    "if cv == 'ts':\n",
    "    cv_for_graph = 'predicting the future'\n",
    "if cv == 'ss':\n",
    "    cv_for_graph = 'predicting contemporaneously'\n",
    "\n",
    "\n",
    "all_probabilities = [item for sublist in preds.values() for item in sublist]\n",
    "\n",
    "p = np.array(all_probabilities)\n",
    "plt.hist(p, density = False, color = '#ff7057')\n",
    "plt.xlabel('Predicted Probabilities')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Predicted Probabilities {}'.format(cv_for_graph))\n",
    "plt.savefig('{}/Graphs/Histogram of Predicted Probabilities {} ({}).png'.format(local_dir, file_stub, cv_for_graph), transparent=False, dpi=80, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_tr.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check different thresholds\n",
    "# high threshold for positive class means model is more precise\n",
    "# low threshold for positive class means model recalls more\n",
    "from sklearn.metrics import precision_score, recall_score, fbeta_score\n",
    "\n",
    "spacing = 0.03\n",
    "thresholds = np.arange(0.1,1,0.1)\n",
    "thresholds = [round(t, 2) for t in thresholds]\n",
    "\n",
    "precision_scores, recall_scores, widths_at_interval = [], [], []\n",
    "f_scores = {}\n",
    "y_pred_proba = fitted_model.predict_proba(X_test)\n",
    "for threshold in thresholds:\n",
    "    y_test_predictions = y_pred_proba[:,1] > threshold\n",
    "    precision = round(precision_score(y_test, y_test_predictions), 2)\n",
    "    recall = round(recall_score(y_test, y_test_predictions), 2)\n",
    "    f_score = round(fbeta_score(y_test, y_test_predictions, beta = 0.1), 2)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f_scores[threshold]= f_score\n",
    "    print(\"Precision (threshold = {}): \".format(threshold), precision)\n",
    "    print(\"Recall (threshold = {}): \".format(threshold), recall)\n",
    "    print(\"F score (threshold = {}): \".format(threshold), f_score)\n",
    "    # Prediction interval of the threshold value \n",
    "    indices_threshold = [k for k, v in preds_interval_mean_probs.items() if (v >= threshold - spacing) & (v <= threshold + spacing)]\n",
    "    widths_threshold = [v for k, v in preds_interval_widths.items() if k in indices_threshold]\n",
    "    width_at_threshold = round(np.mean(widths_threshold), 4)\n",
    "    print(\"Width of prediction interval at threshold value: \", width_at_threshold)\n",
    "    widths_at_interval.append(width_at_threshold)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "predictions_intervals_all_df = pd.DataFrame({'Threshold': thresholds, 'Precision': precision_scores, \n",
    "                                             'Recall': recall_scores, 'F score (beta = 0.1)': f_scores.values(),\n",
    "                          'Prediction interval (threshold +/- {})'.format(spacing): widths_at_interval})\n",
    "predictions_intervals_all_df.to_csv('{}/Models/Prediction Intervals/Prediction intervals - all {}.csv'.format(local_dir, file_stub))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction interval of the threshold value\n",
    "threshold = max(f_scores, key=f_scores.get) \n",
    "print('Threshold which f-score is highest: ', threshold)\n",
    "indices_threshold = [k for k, v in preds_interval_mean_probs.items() if (v >= threshold - spacing) & (v <= threshold + spacing)]\n",
    "widths_threshold = [v for k, v in preds_interval_widths.items() if k in indices_threshold]\n",
    "width_at_threshold = round(np.mean(widths_threshold), 4)\n",
    "print(\"Width of prediction interval at threshold value: \", width_at_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "\n",
    "prediction_intervals_df = pd.DataFrame({'Average width of prediction interval': [average_width], 'Width of prediction interval at threshold value': [width_at_threshold]})\n",
    "prediction_intervals_df.to_csv('{}/Models/Prediction Intervals/Prediction intervals - max, ave{}.csv'.format(local_dir, file_stub))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
