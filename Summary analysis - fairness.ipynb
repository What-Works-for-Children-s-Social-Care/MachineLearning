{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consolidates fairness measures for summary graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in list of model performance metrics\n",
    "import glob\n",
    "import pandas as pd\n",
    "file_list = glob.glob(\"*/*.csv\")\n",
    "\n",
    "file_list = [f for f in file_list if 'Fairness' in f]\n",
    "file_list = [f for f in file_list if 'Average Fairness Metrics' not in f]\n",
    "file_list = [f for f in file_list if 'Fairness grouped by characteristic' not in f]\n",
    "file_list = [f for f in file_list if 'Fairness_metrics_comparing_subgroups' not in f]\n",
    "file_list = [f for f in file_list if 'Fairness unaggregated' not in f]\n",
    "file_list = [f for f in file_list if 'Fairness unaggregated with N' not in f]\n",
    "\n",
    "\n",
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dict = {}\n",
    "for file_name in file_list:\n",
    "\n",
    "    file = pd.read_csv(file_name, index_col = 0) \n",
    "    file_name = file_name.replace(\"Fairness \", \"\")\n",
    "    file_name = file_name.replace(\"Fairness_metrics_CI_\", \"\") \n",
    "    file_name = file_name.replace(\".csv\", \"\")\n",
    "    LA, model_id = file_name.split('/')\n",
    "    file['LA'] = LA\n",
    "    file['model_id'] = model_id\n",
    "    if 'False Discovery Rate' in file.columns:\n",
    "        file.rename(columns = {'False Discovery Rate': 'False discovery rate',\n",
    "                              'False Omission Rate': 'False omission rate'}, inplace = True)\n",
    "    if 'Average precision score' in file.columns:\n",
    "        file.rename(columns = {'Average precision score': 'Pinned average precision'}, inplace = True)\n",
    "    file_dict[file_name] = file\n",
    "\n",
    "print(file_dict.keys())\n",
    "\n",
    "results = pd.concat(file_dict.values(), axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for categories to be combined\n",
    "# Expect 16 for ethnicity as not available for LA3\n",
    "results['Characteristic'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['Characteristic'].unique()\n",
    "\n",
    "age = (['Under 1 Year', '1-4 Years', '5-9 Years', '10-15 Years',\n",
    "       '16+ Years', 'Missing age'])\n",
    "    \n",
    "gender = (['Female', 'Male', 'Unknown, Unborn or Indeterminate'])\n",
    "        \n",
    "disability = (['Disabled', 'Not Disabled', 'Missing Disability'])\n",
    "    \n",
    "ethnicity = (['Asian / Asian British',\n",
    "       'Black / African / Caribbean / Black British', 'Mixed Ethnicity',\n",
    "       'Other Ethnicity', 'Ethnicity Not Known', 'White'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "results['Characteristic'] = np.where((results['Characteristic']=='Ethnicity Not Known') |\n",
    "                                    (results['Characteristic']=='Declined / Missing') ,\n",
    "                                    'Ethnicity Not Known', results['Characteristic'])\n",
    "\n",
    "results['Characteristic'] = np.where((results['Characteristic']=='Black / British Black') |\n",
    "                                    (results['Characteristic']=='Black / African / Caribbean /Black British') |\n",
    "                                    (results['Characteristic']=='Black/Black British'),\n",
    "                                    'Black / African / Caribbean / Black British', results['Characteristic'])\n",
    "\n",
    "results['Characteristic'] = np.where((results['Characteristic']=='Unknown, Unborn or Indeterminate') |\n",
    "                                     (results['Characteristic']=='Unborn'),\n",
    "                                    'Unknown, Unborn or Indeterminate', results['Characteristic'])\n",
    "\n",
    "results['Characteristic'] = np.where((results['Characteristic']=='Asian British') |\n",
    "                                     (results['Characteristic']=='Asian/Asian British'),\n",
    "                                    'Asian / Asian British', results['Characteristic'])\n",
    "\n",
    "\n",
    "'''\n",
    "results['Characteristic'] = np.where((results['Characteristic']=='Mixed / Multiple Ethnic Groups   ') |\n",
    "                                    (results['Characteristic']=='Mixed Ethnicity'),\n",
    "                                    'Mixed Ethnicity', results['Characteristic'])\n",
    "\n",
    "results['Characteristic'] = np.where((results['Characteristic']=='Other Ethnic Groups') |\n",
    "                                    (results['Characteristic']=='Other Ethnicity') |\n",
    "                                    (results['Characteristic']=='Arab') ,\n",
    "                                    'Other Ethnicity', results['Characteristic'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check recategorising worded\n",
    "results['Characteristic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['Group'] = np.where(results['Characteristic'].isin(age), 'Age',\n",
    "                           results['Group'])\n",
    "\n",
    "results['Group'] = np.where(results['Characteristic'].isin(gender), 'Gender',\n",
    "                           results['Group'])\n",
    "\n",
    "results['Group'] = np.where(results['Characteristic'].isin(disability), 'Disability',\n",
    "                           results['Group'])\n",
    "\n",
    "results['Group'] = np.where(results['Characteristic'].isin(ethnicity), 'Ethnicity',\n",
    "                           results['Group'])\n",
    "\n",
    "results['Group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['N'].value_counts(dropna = False)\n",
    "results.drop(columns = 'N', inplace = True)\n",
    "fairness_w_N = pd.read_csv('Fairness unaggregated with N.csv', index_col = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc[results['Characteristic'] == 'Missing age',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(results['Characteristic']).difference(set(fairness_w_N['Characteristic'])))\n",
    "print(set(fairness_w_N['Characteristic']).difference(set(results['Characteristic'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_w_N['Characteristic'] = fairness_w_N['Characteristic'].replace({'Missing': 'Missing Disability',\n",
    "                                                                       'Asian British': 'Asian / Asian British',\n",
    "                                                                         'Black / African / Caribbean /Black British': \n",
    "                                                                         'Black / African / Caribbean / Black British'})\n",
    "                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(results['Characteristic']).difference(set(fairness_w_N['Characteristic'])))\n",
    "print(set(fairness_w_N['Characteristic']).difference(set(results['Characteristic'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.shape)\n",
    "results_with_N = results.merge(fairness_w_N[['Characteristic', 'N', 'LA', 'model_id']], on = ['Characteristic', 'LA', 'model_id'], how = 'left')\n",
    "print(results_with_N.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_with_N.shape)\n",
    "results_with_N.drop_duplicates(inplace = True)\n",
    "print(results_with_N.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Save unaggregated version\n",
    "results_with_N.to_csv('Output/Fairness unaggregated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create version for graph\n",
    "results_for_graph = results[['Group', 'Characteristic', 'Pinned average precision', \n",
    "                            'Average precision score 95% CI (LL)', 'Average precision score 95% CI (UL)']]\n",
    "\n",
    "results_for_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_for_graph.to_csv('Output/Average Fairness Metrics.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "disabled = results.loc[results['Characteristic']=='Disabled','Pinned average precision']\n",
    "not_disabled = results.loc[results['Characteristic']=='Not Disabled','Pinned average precision']\n",
    "print(len(disabled))\n",
    "\n",
    "mw_disability = mannwhitneyu(disabled, not_disabled, alternative='two-sided')\n",
    "print(mw_disability)\n",
    "\n",
    "male = results.loc[results['Characteristic']=='Male','Pinned average precision']\n",
    "female = results.loc[results['Characteristic']=='Female','Pinned average precision']\n",
    "print(len(male))\n",
    "\n",
    "mw_gender = mannwhitneyu(male, female, alternative='two-sided')\n",
    "print(mw_gender)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " results['Characteristic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3+ samples, paired, not drawn from a normal distribution\n",
    "# p.32 https://www.biochemia-medica.com/assets/images/upload/xml_tif/Marusteri_M_-_Comparing_groups_for_statistical_differences.pdf\n",
    "# https://en.wikipedia.org/wiki/Friedman_test\n",
    "\n",
    "from scipy.stats import friedmanchisquare\n",
    "\n",
    "# Age\n",
    "under_one = results.loc[results['Characteristic']=='Under 1 Year','Pinned average precision']\n",
    "one_to_four = results.loc[results['Characteristic']=='1-4 Years','Pinned average precision']\n",
    "five_to_nine = results.loc[results['Characteristic']=='5-9 Years','Pinned average precision']\n",
    "ten_to_fifteen = results.loc[results['Characteristic']=='10-15 Years','Pinned average precision']\n",
    "sixteen_plus = results.loc[results['Characteristic']=='16+ Years','Pinned average precision']\n",
    "\n",
    "friedman_age = friedmanchisquare(under_one, one_to_four, five_to_nine, ten_to_fifteen, sixteen_plus)\n",
    "print(friedman_age)\n",
    "\n",
    "# Ethnicity\n",
    "black_ethnicity = results.loc[(results['Characteristic']== 'Black / African / Caribbean / Black British'),\n",
    "                    'Pinned average precision']\n",
    "unknown_ethnicity = results.loc[(results['Characteristic']=='Ethnicity Not Known') ,\n",
    "                                'Pinned average precision']\n",
    "mixed_ethnicity = results.loc[(results['Characteristic']=='Mixed Ethnicity'),\n",
    "                              'Pinned average precision']\n",
    "other_ethnicity = results.loc[(results['Characteristic']=='Other Ethnicity') ,\n",
    "                              'Pinned average precision']\n",
    "\n",
    "white_ethnicity = results.loc[(results['Characteristic']=='White'),\n",
    "                              'Pinned average precision']\n",
    "print(len(black_ethnicity))\n",
    "print((len(unknown_ethnicity))) # Excluding as not available for all LAs\n",
    "print(len(mixed_ethnicity))\n",
    "print(len(other_ethnicity))\n",
    "print(len(white_ethnicity ))\n",
    "\n",
    "friedman_ethnicity = friedmanchisquare(black_ethnicity, mixed_ethnicity, other_ethnicity, white_ethnicity)\n",
    "print(friedman_ethnicity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit_posthocs\n",
    "# Age\n",
    "import numpy as np\n",
    "import scikit_posthocs as sp\n",
    "\n",
    "pw_age = sp.posthoc_nemenyi_friedman(np.array([under_one, one_to_four, five_to_nine, ten_to_fifteen, sixteen_plus]).T)\n",
    "\n",
    "\n",
    "age_names = (['Under 1 Year', '1-4 Years', '5-9 Years', '10-15 Years',\n",
    "       '16+ Years'])\n",
    "\n",
    "pw_age.columns = age_names\n",
    "pw_age.index = age_names\n",
    "\n",
    "print(pw_age[(pw_age < 0.05) & (pw_age != -1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ethnicity\n",
    "pw_ethnicity = sp.posthoc_nemenyi_friedman(np.array([black_ethnicity, mixed_ethnicity, other_ethnicity, white_ethnicity]).T)\n",
    "\n",
    "ethnicity_names = (['Black / African / Caribbean / Black British', \n",
    "                    'Mixed Ethnicity',\n",
    "                    'Other Ethnicity',\n",
    "                    'White'])\n",
    "pw_ethnicity.columns = ethnicity_names \n",
    "pw_ethnicity.index = ethnicity_names        \n",
    "print(pw_ethnicity[(pw_ethnicity < 0.05) & (pw_ethnicity != -1)])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_metrics_summarised = pd.DataFrame(index = ['Pinned average precision', \n",
    "                                                   'False discovery Rate', \n",
    "                                                   'False omission Rate'],\n",
    "                                          data = {'Mean': \n",
    "                                                 [results['Pinned average precision'].mean(),\n",
    "                                                 results['False discovery rate'].mean(),\n",
    "                                                 results['False omission rate'].mean()],\n",
    "                                                 'Standard Deviation':\n",
    "                                                 [results['Pinned average precision'].std(),\n",
    "                                                 results['False discovery rate'].std(),\n",
    "                                                 results['False omission rate'].std()]})\n",
    "fairness_metrics_summarised = fairness_metrics_summarised.round(2)\n",
    "fairness_metrics_summarised.to_csv('Output/Fairness Metrics summarised.csv') # Data for fairness graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Create version for report\n",
    "fairness_metrics_by_subgroup = results.groupby('Characteristic')[['Pinned average precision', 'False discovery rate',\n",
    "       'False omission rate']].mean().reset_index()\n",
    "# Sort the metrics\n",
    "fairness_metrics_by_subgroup['Characteristic'] = pd.Categorical(fairness_metrics_by_subgroup['Characteristic'],\n",
    "                                                 categories = ['Under 1 Year', '1-4 Years', '5-9 Years', '10-15 Years',\n",
    "                                   '16+ Years', 'Missing age', 'Female', 'Male',\n",
    "                                   'Unknown, Unborn or Indeterminate', 'Disabled', 'Not Disabled',\n",
    "                                   'Missing Disability', 'Asian / Asian British',\n",
    "                                   'Black / African / Caribbean / Black British', 'Mixed Ethnicity',\n",
    "                                   'Other Ethnicity', 'Ethnicity Not Known', 'White'], ordered = True)\n",
    "fairness_metrics_by_subgroup.sort_values(by = 'Characteristic', inplace = True)\n",
    "\n",
    "# Round the metrics\n",
    "fairness_metrics_by_subgroup[['Pinned average precision', 'False discovery rate',\n",
    "       'False omission rate']] = fairness_metrics_by_subgroup[['Pinned average precision', 'False discovery rate',\n",
    "       'False omission rate']].round(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table for report\n",
    "fairness_metrics_by_subgroup['Characteristic Type'] = np.where(fairness_metrics_by_subgroup['Characteristic'].isin(['Under 1 Year', '1-4 Years', '5-9 Years', '10-15 Years',\n",
    "                                               '16+ Years', 'Missing age']),'Age Group',\n",
    "                                                  np.where(fairness_metrics_by_subgroup['Characteristic'].isin(['Female', 'Male', 'Unknown, Unborn or Indeterminate']),\n",
    "                                                          'Gender',\n",
    "                                                          np.where(fairness_metrics_by_subgroup['Characteristic'].isin(['Disabled', 'Not Disabled', 'Missing Disability']),'Disability',\n",
    "                                                             np.where(fairness_metrics_by_subgroup['Characteristic'].isin(['Asian / Asian British',\n",
    "       'Black / African / Caribbean / Black British', 'Mixed Ethnicity',\n",
    "       'Other Ethnicity', 'Ethnicity Not Known', 'White']), 'Ethnicity', 'NA'))))\n",
    "\n",
    "fairness_metrics_by_subgroup = fairness_metrics_by_subgroup[['Characteristic Type', 'Characteristic', \n",
    "                                                             'Pinned average precision', 'False discovery rate',\n",
    "                                                               'False omission rate']]\n",
    "\n",
    "fairness_metrics_by_subgroup.to_csv('Output/Fairness grouped by characteristic.csv', index = False)\n",
    "                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['Characteristic'].unique()\n",
    "\n",
    "age = (['Under 1 Year', '1-4 Years', '5-9 Years', '10-15 Years',\n",
    "       '16+ Years', 'Missing age'])\n",
    "    \n",
    "gender = (['Female', 'Male', 'Unknown, Unborn or Indeterminate'])\n",
    "        \n",
    "disability = (['Disabled', 'Not Disabled', 'Missing Disability'])\n",
    "    \n",
    "ethnicity = (['Asian / Asian British',\n",
    "       'Black / African / Caribbean / Black British', 'Mixed Ethnicity',\n",
    "       'Other Ethnicity', 'Ethnicity Not Known', 'White'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_with_N_LA3 = results_with_N.loc[results_with_N['LA'] == 'LA3', [ 'model_id', 'Characteristic',\n",
    "                                                                        'Group', 'Average precision score 95% CI (LL)',\n",
    "                                                                       'Average precision score 95% CI (UL)', ]]\n",
    "results_with_N_LA3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "group_names_dict = {'Age': age, 'Gender': gender, 'Disabled': disability}\n",
    "\n",
    "post_test_melted_list = []\n",
    "for model in results_with_N_LA3['model_id'].unique():\n",
    "    for group, names in group_names_dict.items():\n",
    "        post_test = pd.DataFrame(columns = group_names_dict[group], index = group_names_dict[group])\n",
    "        post_test.reset_index(inplace = True)\n",
    "        post_test_melted = pd.melt(post_test, id_vars=['index'], value_vars=group_names_dict[group])\n",
    "        post_test_melted.rename(columns = {'index': 'Subgroup 1', 'variable': 'Subgroup 2'}, inplace = True)\n",
    "        post_test_melted = post_test_melted.loc[post_test_melted['Subgroup 1'] != post_test_melted['Subgroup 2'],]\n",
    "\n",
    "\n",
    "        # Only keep unique permutations\n",
    "        post_test_melted['Duplicated Subgroups'] = [sorted([a,b]) for a,b in zip(post_test_melted['Subgroup 1'], post_test_melted['Subgroup 2'])]\n",
    "        post_test_melted['Duplicated Subgroups'] = post_test_melted['Duplicated Subgroups'].astype(str)\n",
    "        post_test_melted.drop_duplicates(subset=['Duplicated Subgroups'], inplace=True)\n",
    "        post_test_melted.drop(columns = 'Duplicated Subgroups', inplace = True)\n",
    "        post_test_melted['Subgroup 1'] = pd.Categorical(post_test_melted['Subgroup 1'], categories = group_names_dict[group], ordered = True)\n",
    "        post_test_melted['Subgroup 2'] = pd.Categorical(post_test_melted['Subgroup 2'], categories = group_names_dict[group], ordered = True)\n",
    "        post_test_melted.sort_values(by=['Subgroup 1', 'Subgroup 2'], inplace = True)\n",
    "        post_test_melted = post_test_melted.merge(results_with_N_LA3[['Average precision score 95% CI (LL)', 'Group', 'Characteristic', 'model_id']],\n",
    "                                                  how = 'left', left_on = 'Subgroup 1', right_on = 'Characteristic')\n",
    "        post_test_melted = post_test_melted.merge(results_with_N_LA3[['Average precision score 95% CI (UL)', 'Characteristic', 'model_id']],\n",
    "                                                  how = 'left', left_on = ['Subgroup 1', 'model_id'], right_on = ['Characteristic', 'model_id'])\n",
    "        post_test_melted.rename(columns = {'Average precision score 95% CI (LL)': 'Subgroup 1: Average precision score 95% CI (LL)',\n",
    "                                            'Average precision score 95% CI (UL)': 'Subgroup 1: Average precision score 95% CI (UL)'},\n",
    "                                 inplace = True)\n",
    "\n",
    "        post_test_melted.drop(columns = ['Characteristic_x', 'Characteristic_y'], inplace = True)\n",
    "        post_test_melted = post_test_melted.merge(results_with_N_LA3[['Average precision score 95% CI (LL)', 'Characteristic', 'model_id']],\n",
    "                                              how = 'left', left_on = ['Subgroup 2', 'model_id'], right_on = ['Characteristic', 'model_id'])\n",
    "        post_test_melted = post_test_melted.merge(results_with_N_LA3[['Average precision score 95% CI (UL)', 'Characteristic', 'model_id']],\n",
    "                                              how = 'left', left_on = ['Subgroup 2', 'model_id'], right_on = ['Characteristic', 'model_id'])\n",
    "\n",
    "        post_test_melted.drop(columns = ['Characteristic_x', 'Characteristic_y'], inplace = True)\n",
    "        post_test_melted.rename(columns = {'Average precision score 95% CI (LL)': 'Subgroup 2: Average precision score 95% CI (LL)',\n",
    "                                        'Average precision score 95% CI (UL)': 'Subgroup 2: Average precision score 95% CI (UL)'},\n",
    "                             inplace = True)\n",
    "\n",
    "        post_test_melted['Subgroups are significantly different according to a comparison of confidence intervals'] = np.where((post_test_melted['Subgroup 1: Average precision score 95% CI (LL)'] >\n",
    "                                                                                        post_test_melted['Subgroup 2: Average precision score 95% CI (UL)']) |\n",
    "                                                                                                                               (post_test_melted['Subgroup 2: Average precision score 95% CI (LL)'] >\n",
    "                                                                                        post_test_melted['Subgroup 1: Average precision score 95% CI (UL)']), True, False)\n",
    "        post_test_melted_list.append(post_test_melted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_test_melted_LA3 = pd.concat(post_test_melted_list, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(post_test_melted_LA3.shape)\n",
    "post_test_melted_LA3.drop_duplicates(inplace = True)\n",
    "print(post_test_melted_LA3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_test_melted_LA3.loc[(post_test_melted_LA3['Group'] == 'Age') &\n",
    "                         (post_test_melted_LA3['model_id'] == 'rq2_ss_str'),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(post_test_melted_LA3.shape)\n",
    "post_test_melted_LA3.drop_duplicates(inplace = True)\n",
    "print(post_test_melted_LA3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_test_melted_LA3.to_csv('Output/Fairness comparison LA3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_final_LA3 = post_test_melted_LA3.groupby(['Group', 'model_id'])['Subgroups are significantly different according to a comparison of confidence intervals'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_final_LA3.to_csv('Bias metrics LA3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_with_N_LA4_gender = results_with_N.loc[(results_with_N['LA'] == 'LA4') &\n",
    "                  (results_with_N['model_id'] == 'rq2_ts_all') & \n",
    "                  (results_with_N['Group'] == 'Gender'),]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender for LA2\n",
    "group = 'Gender'\n",
    "post_test = pd.DataFrame(columns = group_names_dict[group], index = group_names_dict[group])\n",
    "post_test.reset_index(inplace = True)\n",
    "post_test_melted = pd.melt(post_test, id_vars=['index'], value_vars=group_names_dict[group])\n",
    "post_test_melted.rename(columns = {'index': 'Subgroup 1', 'variable': 'Subgroup 2'}, inplace = True)\n",
    "post_test_melted = post_test_melted.loc[post_test_melted['Subgroup 1'] != post_test_melted['Subgroup 2'],]\n",
    "\n",
    "\n",
    "# Only keep unique permutations\n",
    "post_test_melted['Duplicated Subgroups'] = [sorted([a,b]) for a,b in zip(post_test_melted['Subgroup 1'], post_test_melted['Subgroup 2'])]\n",
    "post_test_melted['Duplicated Subgroups'] = post_test_melted['Duplicated Subgroups'].astype(str)\n",
    "post_test_melted.drop_duplicates(subset=['Duplicated Subgroups'], inplace=True)\n",
    "post_test_melted.drop(columns = 'Duplicated Subgroups', inplace = True)\n",
    "post_test_melted['Subgroup 1'] = pd.Categorical(post_test_melted['Subgroup 1'], categories = group_names_dict[group], ordered = True)\n",
    "post_test_melted['Subgroup 2'] = pd.Categorical(post_test_melted['Subgroup 2'], categories = group_names_dict[group], ordered = True)\n",
    "post_test_melted.sort_values(by=['Subgroup 1', 'Subgroup 2'], inplace = True)\n",
    "post_test_melted = post_test_melted.merge(results_with_N_LA4_gender[['Average precision score 95% CI (LL)', 'Group', 'Characteristic', 'model_id']],\n",
    "                                          how = 'left', left_on = 'Subgroup 1', right_on = 'Characteristic')\n",
    "post_test_melted = post_test_melted.merge(results_with_N_LA4_gender[['Average precision score 95% CI (UL)', 'Characteristic', 'model_id']],\n",
    "                                          how = 'left', left_on = ['Subgroup 1', 'model_id'], right_on = ['Characteristic', 'model_id'])\n",
    "post_test_melted.rename(columns = {'Average precision score 95% CI (LL)': 'Subgroup 1: Average precision score 95% CI (LL)',\n",
    "                                    'Average precision score 95% CI (UL)': 'Subgroup 1: Average precision score 95% CI (UL)'},\n",
    "                         inplace = True)\n",
    "\n",
    "post_test_melted.drop(columns = ['Characteristic_x', 'Characteristic_y'], inplace = True)\n",
    "post_test_melted = post_test_melted.merge(results_with_N_LA4_gender[['Average precision score 95% CI (LL)', 'Characteristic', 'model_id']],\n",
    "                                      how = 'left', left_on = ['Subgroup 2', 'model_id'], right_on = ['Characteristic', 'model_id'])\n",
    "post_test_melted = post_test_melted.merge(results_with_N_LA4_gender[['Average precision score 95% CI (UL)', 'Characteristic', 'model_id']],\n",
    "                                      how = 'left', left_on = ['Subgroup 2', 'model_id'], right_on = ['Characteristic', 'model_id'])\n",
    "\n",
    "post_test_melted.drop(columns = ['Characteristic_x', 'Characteristic_y'], inplace = True)\n",
    "post_test_melted.rename(columns = {'Average precision score 95% CI (LL)': 'Subgroup 2: Average precision score 95% CI (LL)',\n",
    "                                'Average precision score 95% CI (UL)': 'Subgroup 2: Average precision score 95% CI (UL)'},\n",
    "                     inplace = True)\n",
    "\n",
    "post_test_melted['Subgroups are significantly different according to a comparison of confidence intervals'] = np.where((post_test_melted['Subgroup 1: Average precision score 95% CI (LL)'] >\n",
    "                                                                                post_test_melted['Subgroup 2: Average precision score 95% CI (UL)']) |\n",
    "                                                                                                                       (post_test_melted['Subgroup 2: Average precision score 95% CI (LL)'] >\n",
    "                                                                                post_test_melted['Subgroup 1: Average precision score 95% CI (UL)']), True, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_test_melted.to_csv('Output/Fairness comparison LA4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_final_LA4_gender = post_test_melted.groupby(['Group', 'model_id'])['Subgroups are significantly different according to a comparison of confidence intervals'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_final_LA4_gender.to_csv('Bias metrics LA4 gender.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
