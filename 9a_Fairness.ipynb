{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness Metrics\n",
    "\n",
    "- False Discovery Rate\n",
    "- False Omission Rate\n",
    "- Pinned AUC\n",
    "\n",
    "## Fairness subgroups\n",
    "\n",
    "\n",
    "- age (depends on categorisation in datasets but likely to follow the Department for Education statistical return categories: under 1 year, 1-4 years, 5-9 years, 10-15 years, 16+ years), \n",
    "- gender (likely to follow the Department for Education categorisation - male, female, unknown), \n",
    "- disability (recorded or not recorded)\n",
    "- ethnicity (depends on categorisation in datasets but likely to follow the Department for Education statistical return categories: white, mixed, Asian or Asian British, black or black British, other, unknown).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable set-up\n",
    "\n",
    "rq = 'rq2' # Options: 'rq1', 'rq2'\n",
    "cv = 'ts' # Options: 'ts' (time series split, ignore siblings), 'ss' (stratified shuffle, ignore siblings)\n",
    "data_type = 'all' # Options: 'str' (just structured data), 'all' (structured data and list of strings)\n",
    "algorithm_names = ['decision_tree', 'logistic_regression', 'gradient_boosting'] \n",
    "resampling_name = 'ada' # anything other than 'ada' does 'smote' \n",
    "select_features_alpha = 0.001 # Should be highest 0.001 as otherwise all dropped\n",
    "rcv_n_iter = 50 # The more iterations, the more the randomised search searches for an optimal solution\n",
    "parameters = 2 # \n",
    "\n",
    "# Don't change\n",
    "file_stub_y_siblings = rq + '_' + cv + '_str' # use 'str' for all \n",
    "file_stub = rq + '_' + cv + '_' + data_type # Creates file stub for saving in the format e.g. rq1_ss_str\n",
    "levers = resampling_name + '_' + str(select_features_alpha) + '_' + str(rcv_n_iter) + '_' + str(parameters)\n",
    "print(file_stub + '_' + levers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up directories\n",
    "local_dir = '/Users/[username]/Documents/Final transfer out data and code to WWC Jan 2020' # insert [username]\n",
    "hard_drive_dir = '/Volumes/diskAshur2/Final transfer out data and code to WWC Jan 2020/Data for model/Use'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load user-written functions\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#import text_functions, analysis_functions\n",
    "import analysis_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory\n",
    "import os\n",
    "\n",
    "os.chdir(hard_drive_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X_test_w_sensitive = pd.read_csv('X_test_sensitive_{}.csv'.format(file_stub), index_col = 0)\n",
    "print(X_test_w_sensitive.shape)\n",
    "X_test_w_sensitive.reset_index(inplace = True, drop = True)\n",
    "print(X_test_w_sensitive.index)\n",
    "\n",
    "y_test = pd.read_csv(\"y_test_{}.csv\".format(file_stub_y_siblings), index_col = 0, header = None)\n",
    "print(y_test.shape)\n",
    "y_test.reset_index(inplace = True, drop = True)\n",
    "y_test = pd.Series(y_test[1])\n",
    "print(y_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recategorise groups for estimating fairness metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X_test_w_sensitive['AgeAtReferralDate'].value_counts().sort_index()\n",
    "X_test_w_sensitive['Age_cut'] = pd.cut(X_test_w_sensitive['AgeAtReferralDate'], \n",
    "                   bins=[-1, 0, 4, 9, 15, 25], \n",
    "                   labels=['under 1 year', '1-4 years', '5-9 years', '10-15 years', '16+ years'],\n",
    "                   include_lowest=True)\n",
    "X_test_w_sensitive['Age_cut'] = np.where(X_test_w_sensitive['Age_cut'].isnull(),'missing',X_test_w_sensitive['Age_cut'])\n",
    "print(pd.crosstab(X_test_w_sensitive['AgeAtReferralDate'], X_test_w_sensitive['Age_cut']))\n",
    "print(X_test_w_sensitive['Age_cut'].value_counts())\n",
    "\n",
    "# Gender\n",
    "# No recoding needed\n",
    "print(X_test_w_sensitive['Gender'].value_counts(dropna=False))\n",
    "\n",
    "# Disabled (fill missing)\n",
    "print(X_test_w_sensitive['hasdisability'].value_counts(dropna=False))\n",
    "#X_test_w_sensitive['hasdisability'] = np.where(X_test_w_sensitive['hasdisability'].isnull(),'missing',X_test_w_sensitive['hasdisability'])\n",
    "X_test_w_sensitive['hasdisability'] = np.where(X_test_w_sensitive['hasdisability']==True,'Disabled',\n",
    "                                              np.where(X_test_w_sensitive['hasdisability']==False, 'Not Disabled', 'Disability Missing'))\n",
    "\n",
    "print(X_test_w_sensitive['hasdisability'].value_counts(dropna=False))\n",
    "\n",
    "# Ethnicity\n",
    "# white, mixed, Asian or Asian British, black or black British, other, unknown, Arab\n",
    "\n",
    "\n",
    "X_test_w_sensitive['ethnicity_highlevel'] = np.where((X_test_w_sensitive['ethnicity_highlevel']=='Declined/Missing') | \n",
    "                                                     (X_test_w_sensitive['ethnicity_highlevel']=='Ethnicity Not Given'),'Ethnicity Not Known',\n",
    "                                              np.where((X_test_w_sensitive['ethnicity_highlevel']=='Arab') |\n",
    "                                                       (X_test_w_sensitive['ethnicity_highlevel']=='Other'), 'Other Ethnicity', \n",
    "                                              np.where((X_test_w_sensitive['ethnicity_highlevel']=='Mixed'), 'Mixed Ethnicity',          \n",
    "                                                       X_test_w_sensitive['ethnicity_highlevel'])))\n",
    "X_test_w_sensitive['ethnicity_highlevel'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = open(\"{}/Models/Prediction Intervals/preds_{}.pkl\".format(local_dir, file_stub), 'rb')\n",
    "preds = pickle.load(filename)\n",
    "\n",
    "filename = open(\"{}/Models/Prediction Intervals/preds_proba_{}.pkl\".format(local_dir, file_stub), 'rb')\n",
    "preds_proba = pickle.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make bootstrapped predictions into dataframes\n",
    "assert (len(preds[0]) == len(preds_proba[0]))\n",
    "\n",
    "num_predictions = len(preds[0])\n",
    "\n",
    "preds_df = pd.DataFrame(preds).transpose()\n",
    "preds_df = preds_df.add_prefix('pred_binary_')\n",
    "\n",
    "preds_proba_df = pd.DataFrame(preds_proba).transpose()\n",
    "preds_proba_df = preds_proba_df.add_prefix('pred_proba_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine predictions, actual, input data and sensitive\n",
    "df_fairness  = pd.concat([preds_df, preds_proba_df, y_test, X_test_w_sensitive], axis = 1)\n",
    "df_fairness.rename(columns = {1: 'y_test'}, inplace=True)\n",
    "print(df_fairness.shape)\n",
    "df_fairness.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make as categories for sorting\n",
    "for col in ['Age_cut', 'Gender', 'hasdisability',  'ethnicity_highlevel']:\n",
    "    print(df_fairness[col].dtype)\n",
    "    df_fairness[col] = df_fairness[col].astype('category')\n",
    "    print(df_fairness[col].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get N\n",
    "\n",
    "variable_subgroup_dict = ({'Age_cut': df_fairness['Age_cut'].unique(),\n",
    "                        'Gender': df_fairness['Gender'].unique(),  \n",
    "                        'hasdisability': df_fairness['hasdisability'].unique(),\n",
    "                          'ethnicity_highlevel': df_fairness['ethnicity_highlevel'].unique()})\n",
    "\n",
    "counts_list = []\n",
    "for variable in variable_subgroup_dict.keys():\n",
    "    counts = df_fairness.astype(str).groupby([variable])['pred_binary_0'].count()\n",
    "    counts = counts.reset_index()\n",
    "    counts.columns = ['Characteristic', 'N']\n",
    "    counts_list.append(counts)\n",
    "    print(counts)\n",
    "counts_df = pd.concat(counts_list, axis = 0)\n",
    "counts_df.to_csv('{}/Models/Fairness N {}.csv'.format(local_dir, file_stub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 1==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "#  add in numbers of observations for each subgroup (so that it gives context to the proportion)\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "from analysis_functions import pinned_metrics\n",
    "\n",
    "# Drop nas from prediction and true columns as metrics don't like it\n",
    "pred_cols = [col for col in df_fairness.columns if 'pred_binary' in col]\n",
    "pred_proba_cols = [col for col in df_fairness.columns if 'pred_proba' in col]\n",
    "\n",
    "df_fairness.dropna(subset = ['y_test', *pred_cols, *pred_proba_cols], inplace = True)\n",
    "\n",
    "metrics = ['average_precision_score', 'false_discovery_rate', 'false_omission_rate']\n",
    "\n",
    "variable_subgroup_dict = ({'Age_cut': df_fairness['Age_cut'].unique(),\n",
    "                        'Gender': df_fairness['Gender'].unique(),  \n",
    "                        'hasdisability': df_fairness['hasdisability'].unique(),\n",
    "                         'ethnicity_highlevel': df_fairness['ethnicity_highlevel'].unique()})\n",
    "\n",
    "df_dict_variables_metrics = {}\n",
    "for variable, subgroup in variable_subgroup_dict.items():\n",
    "    print(variable)\n",
    "    df_metrics_bootstrapped = {}\n",
    "    for p in range(0,num_predictions):\n",
    "        if p % 100 == 0:\n",
    "            print(p)\n",
    "        df_list_metrics = []\n",
    "        for metric in metrics:\n",
    "            subgroup_metrics = pinned_metrics(df = df_fairness, group = variable, subgroups = subgroup, y_true = 'y_test', y_scores  = 'pred_binary_{}'.format(p), y_proba_scores = 'pred_proba_{}'.format(p), metric = metric, upsampling = True) # return list of metrics for subgroup\n",
    "            df_fairness_metric = pd.DataFrame({'{}'.format(variable): subgroup, '{}'.format(metric): subgroup_metrics})\n",
    "            df_list_metrics.append(df_fairness_metric)\n",
    "        df_metrics_merged = reduce(lambda  left,right: pd.merge(left,right,on=[variable], how='outer'), df_list_metrics)\n",
    "        df_metrics_merged = df_metrics_merged.sort_values(by = variable)\n",
    "        counts = df_fairness.astype(str).groupby([variable])['pred_binary_0'].count()\n",
    "        counts.rename('N', inplace = True)\n",
    "        \n",
    "        counts = counts.astype('float64')\n",
    "        df_metrics_merged[variable] = df_metrics_merged[variable].astype('category')\n",
    "        df_metrics_merged = df_metrics_merged.merge(counts, on=[variable], how='left')\n",
    "        df_metrics_bootstrapped[p] = df_metrics_merged\n",
    "    print(counts)\n",
    "    df_dict_variables_metrics[variable] = df_metrics_bootstrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate the 95% CI for the pinned metrics\n",
    "\n",
    "metric_name = {'average_precision_score': 'Average precision score',\n",
    "    'false_discovery_rate': 'False discovery rate',\n",
    "    'false_omission_rate': 'False omission rate'} \n",
    "        \n",
    "\n",
    "# For each metric\n",
    "fairness_metrics_CI_metric_list = []\n",
    "for metric in metrics:\n",
    "    fairness_metrics_CI_var_list = []\n",
    "    # For each group\n",
    "    for variable in variable_subgroup_dict.keys():\n",
    "        # Find median, upper and lower bounds\n",
    "        median = round(pd.concat(df_dict_variables_metrics[variable]).groupby(variable)[metric].quantile(0.5), 4)\n",
    "        lower = round(pd.concat(df_dict_variables_metrics[variable]).groupby(variable)[metric].quantile(0.025), 4)\n",
    "        upper = round(pd.concat(df_dict_variables_metrics[variable]).groupby(variable)[metric].quantile(0.975), 4)\n",
    "\n",
    "        fairness_metrics_CI = pd.concat([median, lower, upper], axis=1)\n",
    "        fairness_metrics_CI.columns = (['{}'.format(metric_name[metric]),  \n",
    "                                        '{} 95% CI (LL)'.format(metric_name[metric]),\n",
    "                                        '{} 95% CI (UL)'.format(metric_name[metric])])\n",
    "        fairness_metrics_CI['Group'] = '{}'.format(variable)\n",
    "        fairness_metrics_CI.index.rename('', inplace = True)\n",
    "        fairness_metrics_CI_var_list.append(fairness_metrics_CI)\n",
    "    fairness_metrics_CI_var_all = pd.concat(fairness_metrics_CI_var_list, axis=0)\n",
    "    fairness_metrics_CI_metric_list.append(fairness_metrics_CI_var_all)\n",
    "\n",
    "# To handle duplicate Group columns\n",
    "for n, l in enumerate(fairness_metrics_CI_metric_list):\n",
    "    if n!=0:\n",
    "        fairness_metrics_CI_metric_list[n].drop(columns = 'Group', inplace = True)\n",
    "fairness_metrics_CI = pd.concat(fairness_metrics_CI_metric_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise names\n",
    "fairness_metrics_CI['Characteristic'] = fairness_metrics_CI.index\n",
    "fairness_metrics_CI['Characteristic'].replace({'under 1 year': 'Under 1 Year',\n",
    "                                '1-4 years': '1-4 Years',\n",
    "                                '10-15 years': '10-15 Years',\n",
    "                                '16+ years': '16+ Years',\n",
    "                                   '5-9 years':'5-9 Years',\n",
    "                                  'Asian/Asian British': 'Asian British',                                      \n",
    "                                   'Black/Black British': 'Black / African / Caribbean / Black British', \n",
    "                                'Disability Missing': 'Missing Disability'}, inplace = True)\n",
    "\n",
    "\n",
    "fairness_metrics_CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce report ready version of table\n",
    "fairness_metrics_CI['Characteristic'] = pd.Categorical(fairness_metrics_CI['Characteristic'],\n",
    "                                                 categories = ['Under 1 Year', '1-4 Years', '5-9 Years', '10-15 Years',\n",
    "       '16+ Years', 'Female', 'Male', 'Unknown, Unborn or Indeterminate',\n",
    "       'Disabled', 'Not Disabled', 'Missing Disability', 'Asian British',\n",
    "       'Black / African / Caribbean / Black British', 'Mixed Ethnicity',\n",
    "       'Other Ethnicity', 'Ethnicity Not Known', 'White'], ordered = True)\n",
    "fairness_metrics_CI.reset_index(drop=True, inplace=True)\n",
    "fairness_metrics_CI.sort_values(by = 'Characteristic', inplace = True)\n",
    "fairness_metrics_CI.to_csv('{}/Models/Fairness_metrics_CI_{}.csv'.format(local_dir, file_stub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder dictionaries to allow for all the predictions for a subgroup to be in one list\n",
    "from collections import defaultdict\n",
    "metric_scores_dict = {}\n",
    "for m in metrics:\n",
    "    print(m)\n",
    "    var_scores_dict = {}\n",
    "    for var in df_dict_variables_metrics.keys():\n",
    "        print(var)\n",
    "        subgroup_dict = defaultdict(list)\n",
    "        for p in df_dict_variables_metrics[var].keys():\n",
    "            print(p)\n",
    "            df_temp = df_dict_variables_metrics[var][p]\n",
    "            for subgroup in df_temp[var].unique():\n",
    "                print(subgroup)\n",
    "                subgroup_score = df_temp.loc[df_temp[var] == subgroup,m].values[0]\n",
    "                print(subgroup_score)\n",
    "                subgroup_dict[subgroup].append(subgroup_score)\n",
    "                var_scores_dict[var] = subgroup_dict\n",
    "            metric_scores_dict[m] = var_scores_dict       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle metrics scores\n",
    "import pickle\n",
    "with open(\"{}/Models/Prediction Intervals/metric_scores_dict_{}.pkl\".format(local_dir, file_stub), \"wb\") as handle:\n",
    "    pickle.dump(metric_scores_dict, handle, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = open(\"{}/Models/Prediction Intervals/metric_scores_dict_{}.pkl\".format(local_dir, file_stub), 'rb')\n",
    "metric_scores_dict = pickle.load(filename)\n",
    "\n",
    "fairness_metrics_CI = pd.read_csv('{}/Models/Fairness_metrics_CI_{}.csv'.format(local_dir, file_stub), index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set up test for difference between the scores on different subgroups\n",
    "\n",
    "from scipy.stats import friedmanchisquare\n",
    "import numpy as np\n",
    "#!pip install scikit_posthocs\n",
    "import scikit_posthocs as sp\n",
    "\n",
    "# Age\n",
    "under_one = metric_scores_dict['average_precision_score']['Age_cut']['under 1 year']\n",
    "one_to_four = metric_scores_dict['average_precision_score']['Age_cut']['1-4 years']\n",
    "five_to_nine = metric_scores_dict['average_precision_score']['Age_cut']['5-9 years']\n",
    "ten_to_fifteen = metric_scores_dict['average_precision_score']['Age_cut']['10-15 years']\n",
    "sixteen_plus = metric_scores_dict['average_precision_score']['Age_cut']['16+ years']\n",
    "\n",
    "age_list = [under_one, one_to_four, five_to_nine, ten_to_fifteen, sixteen_plus]\n",
    "age_names = ['Under 1 Year', '1-4 Years', '5-9 Years', '10-15 Years', '16+ Years']\n",
    "\n",
    "# Gender\n",
    "female = metric_scores_dict['average_precision_score']['Gender']['Female']\n",
    "male = metric_scores_dict['average_precision_score']['Gender']['Male']\n",
    "unknown = metric_scores_dict['average_precision_score']['Gender']['Unknown, Unborn or Indeterminate']\n",
    "\n",
    "gender_list = [female, male, unknown]\n",
    "gender_names = sorted(metric_scores_dict['average_precision_score']['Gender'].keys())\n",
    "\n",
    "# Disability\n",
    "disabled = metric_scores_dict['average_precision_score']['hasdisability']['Disabled']\n",
    "not_disabled = metric_scores_dict['average_precision_score']['hasdisability']['Not Disabled']\n",
    "disabled_missing = metric_scores_dict['average_precision_score']['hasdisability']['Disability Missing']\n",
    "\n",
    "disability_list = [disabled, not_disabled, disabled_missing]\n",
    "disability_names = ['Disabled', 'Not Disabled', 'Missing Disability']\n",
    "\n",
    "# Ethnicity\n",
    "asian = metric_scores_dict['average_precision_score']['ethnicity_highlevel']['Asian/Asian British']\n",
    "black = metric_scores_dict['average_precision_score']['ethnicity_highlevel']['Black/Black British']\n",
    "unknown_ethnicity = metric_scores_dict['average_precision_score']['ethnicity_highlevel']['Ethnicity Not Known']\n",
    "mixed_ethnicity = metric_scores_dict['average_precision_score']['ethnicity_highlevel']['Mixed Ethnicity']\n",
    "other_ethnicity = metric_scores_dict['average_precision_score']['ethnicity_highlevel']['Other Ethnicity']\n",
    "white = metric_scores_dict['average_precision_score']['ethnicity_highlevel']['White']\n",
    "\n",
    "ethnicity_list = [asian, black, unknown_ethnicity, mixed_ethnicity, other_ethnicity, white]\n",
    "ethnicity_names = (['Asian British', 'Black / African / Caribbean / Black British',\n",
    "                    'Ethnicity Not Known', 'Mixed Ethnicity', 'Other Ethnicity',\n",
    "                     'White'])\n",
    "\n",
    "group_dict = {'Age_cut': age_list, 'Gender': gender_list, \n",
    "              'hasdisability': disability_list, \n",
    "              'ethnicity_highlevel': ethnicity_list}\n",
    "\n",
    "group_names_dict = {'Age_cut': age_names, 'Gender': gender_names,  \n",
    "                    'hasdisability': disability_names, 'ethnicity_highlevel': ethnicity_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ratio of sds\n",
    "\n",
    "import statistics\n",
    "max_sd_dict, min_sd_dict = {}, {}\n",
    "for k in group_dict.keys():\n",
    "    print(k)\n",
    "    max_sd = 0\n",
    "    min_sd = 1\n",
    "    for g in group_dict[k]:\n",
    "        sd = statistics.stdev(g)\n",
    "        #print(sd)\n",
    "        if sd > max_sd:\n",
    "            max_sd = sd\n",
    "            max_sd_dict[k] = max_sd\n",
    "        if sd < min_sd:\n",
    "            min_sd = sd\n",
    "            min_sd_dict[k] = min_sd\n",
    "\n",
    "min_sds = list(min_sd_dict.values())\n",
    "max_sds = list(max_sd_dict.values())\n",
    "sds_ratios = [b / m for b,m in zip(max_sds, min_sds)]\n",
    "sds_ratios_dict = dict(zip(group_dict.keys(), sds_ratios))\n",
    "pd.DataFrame([sds_ratios_dict]).to_csv('{}/Models/Fairness_metrics_sds_{}.csv'.format(local_dir, file_stub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test for differences between subgroup and if stat sig different, find which subgroups\n",
    "# NB This will produce a different answer than looking at whether the CI overall. CI is the CI around the median\n",
    "# whilst the Friedman tests for differences in the ranking. \n",
    "# Looking at CIs leads to more Type 2 errors (failure to identify differences)\n",
    "# https://statisticsbyjim.com/hypothesis-testing/confidence-intervals-compare-means/\n",
    "post_test_melted_list = []\n",
    "friedman_dict, subgroups_bias_dict = {}, {}\n",
    "for group, group_list in group_dict.items():\n",
    "    N = len(group_dict[group][0])\n",
    "    # Are the scores different?\n",
    "    friedman = friedmanchisquare(*group_list)\n",
    "    friedman_dict[group] = \"Friedman chi squared statistic ={}, n for each group = {}, P={}, two-tailed\".format(round(friedman.statistic, 1), N, round(friedman.pvalue, 2))\n",
    "    print()\n",
    "\n",
    "    # Post test: if the scores are different, for which subgroups?\n",
    "    if friedman.pvalue < 0.05:\n",
    "        post_test = sp.posthoc_nemenyi_friedman(np.array([*group_list]).T)\n",
    "        post_test.columns = group_names_dict[group]\n",
    "        post_test.index = group_names_dict[group]  \n",
    "        \n",
    "\n",
    "        print(post_test[(post_test < 0.05) & (post_test != -1)])\n",
    "        \n",
    "\n",
    "        biased = post_test[(post_test < 0.05) & (post_test != -1)].to_numpy()\n",
    "        bias_dict = {}\n",
    "        for idrow, row in enumerate(biased):\n",
    "            for idcol, col in enumerate(row):\n",
    "                if not pd.isnull(col):       \n",
    "                    print(post_test.columns[idcol])\n",
    "                    print(post_test.index[idrow])\n",
    "                    bias_dict[post_test.columns[idcol]] = post_test.index[idrow]\n",
    "\n",
    "        bias_list = []\n",
    "        for subgroup in bias_dict.keys():\n",
    "            AP1 = fairness_metrics_CI.loc[fairness_metrics_CI['Characteristic'] == subgroup,'Average precision score'].values[0]\n",
    "            LL1 = fairness_metrics_CI.loc[fairness_metrics_CI['Characteristic'] == subgroup,'Average precision score 95% CI (LL)'].values[0]\n",
    "            UL1 = fairness_metrics_CI.loc[fairness_metrics_CI['Characteristic'] == subgroup,'Average precision score 95% CI (UL)'].values[0]\n",
    "            AP2 = fairness_metrics_CI.loc[fairness_metrics_CI['Characteristic'] == bias_dict[subgroup],'Average precision score'].values[0]\n",
    "            LL2 = fairness_metrics_CI.loc[fairness_metrics_CI['Characteristic'] == bias_dict[subgroup],'Average precision score 95% CI (LL)'].values[0]\n",
    "            UL2 = fairness_metrics_CI.loc[fairness_metrics_CI['Characteristic'] == bias_dict[subgroup],'Average precision score 95% CI (UL)'].values[0]\n",
    "            if fairness_metrics_CI.loc[fairness_metrics_CI['Characteristic'] == subgroup,'Average precision score'].values[0] > fairness_metrics_CI.loc[fairness_metrics_CI['Characteristic'] == bias_dict[subgroup],'Average precision score'].values[0]:\n",
    "                bias = \"\"\"The model makes fewer errors for {} than {}. The average precision for {} is {} (95% CI [{}, {}]) whilst the average precision for {} is {} (95% CI [{}, {}])\"\"\".format(subgroup, bias_dict[subgroup], subgroup, AP1, LL1, UL1,\n",
    "                                                   bias_dict[subgroup], AP2, LL2, UL2)\n",
    "                bias_list.append(bias)\n",
    "            else:\n",
    "                bias = \"\"\"The model makes fewer errors for {} than {}. The average precision for {} is {} (95% CI [{}, {}]) whilst the average precision for {} is {} (95% CI [{}, {}])\"\"\".format(bias_dict[subgroup], subgroup, bias_dict[subgroup], AP2, LL2, UL2,\n",
    "                                                   subgroup, AP1, LL1, UL1)\n",
    "                \n",
    "                bias_list.append(bias)\n",
    "\n",
    "        bias_list = list(set(bias_list))\n",
    "        subgroups_bias_dict[group] = bias_list\n",
    "\n",
    "        # Create dataframe for the appendix\n",
    "        post_test.reset_index(inplace = True)\n",
    "        post_test_melted = pd.melt(post_test, id_vars=['index'], value_vars=group_names_dict[group])\n",
    "        post_test_melted.rename(columns = {'index': 'Subgroup 1', 'variable': 'Subgroup 2', 'value': 'P-value'}, inplace = True)\n",
    "        post_test_melted = post_test_melted.loc[post_test_melted['Subgroup 1'] != post_test_melted['Subgroup 2'],]\n",
    "\n",
    "\n",
    "        # Only keep unique permutations\n",
    "        post_test_melted['Duplicated Subgroups'] = [sorted([a,b]) for a,b in zip(post_test_melted['Subgroup 1'], post_test_melted['Subgroup 2'])]\n",
    "        post_test_melted['Duplicated Subgroups'] = post_test_melted ['Duplicated Subgroups'].astype(str)\n",
    "        post_test_melted.drop_duplicates(subset=['Duplicated Subgroups'], inplace=True)\n",
    "        post_test_melted.drop(columns = 'Duplicated Subgroups', inplace = True)\n",
    "        post_test_melted['Subgroup 1'] = pd.Categorical(post_test_melted['Subgroup 1'], categories = group_names_dict[group], ordered = True)\n",
    "        post_test_melted['Subgroup 2'] = pd.Categorical(post_test_melted['Subgroup 2'], categories = group_names_dict[group], ordered = True)\n",
    "        post_test_melted.sort_values(by=['P-value', 'Subgroup 1', 'Subgroup 2'], inplace = True)\n",
    "\n",
    "        # Compare to thresholds determined by Hochberg's step up procedure\n",
    "        k = post_test_melted.shape[0]\n",
    "        sig_threshold = [0.01/k*(r+1) for r in range(0,post_test_melted.shape[0])]\n",
    "        post_test_melted[\"Hochberg's significance threshold\"] = sig_threshold\n",
    "        post_test_melted[\"P-value lower than the significance threshold according to Hochberg's step up procedure\"] = post_test_melted['P-value'] <= post_test_melted[\"Hochberg's significance threshold\"]\n",
    "        post_test_melted['P-value'] = round(post_test_melted['P-value'], 4)\n",
    "        post_test_melted[\"Hochberg's significance threshold\"] = round(post_test_melted[\"Hochberg's significance threshold\"], 4)\n",
    "\n",
    "        # See whether CIs overlap\n",
    "        post_test_melted = post_test_melted.merge(fairness_metrics_CI[['Average precision score 95% CI (LL)', 'Characteristic']],\n",
    "                                              how = 'left', left_on = 'Subgroup 1', right_on = 'Characteristic')\n",
    "        post_test_melted = post_test_melted.merge(fairness_metrics_CI[['Average precision score 95% CI (UL)', 'Characteristic']],\n",
    "                                              how = 'left', left_on = 'Subgroup 1', right_on = 'Characteristic')\n",
    "        post_test_melted.rename(columns = {'Average precision score 95% CI (LL)': 'Subgroup 1: Average precision score 95% CI (LL)',\n",
    "                                        'Average precision score 95% CI (UL)': 'Subgroup 1: Average precision score 95% CI (UL)'},\n",
    "                             inplace = True)\n",
    "        post_test_melted.drop(columns = ['Characteristic_x', 'Characteristic_y'], inplace = True)\n",
    "        post_test_melted = post_test_melted.merge(fairness_metrics_CI[['Average precision score 95% CI (LL)', 'Characteristic']],\n",
    "                                              how = 'left', left_on = 'Subgroup 2', right_on = 'Characteristic')\n",
    "        post_test_melted = post_test_melted.merge(fairness_metrics_CI[['Average precision score 95% CI (UL)', 'Characteristic']],\n",
    "                                              how = 'left', left_on = 'Subgroup 2', right_on = 'Characteristic')\n",
    "\n",
    "        post_test_melted.drop(columns = ['Characteristic_x', 'Characteristic_y'], inplace = True)\n",
    "        post_test_melted.rename(columns = {'Average precision score 95% CI (LL)': 'Subgroup 2: Average precision score 95% CI (LL)',\n",
    "                                        'Average precision score 95% CI (UL)': 'Subgroup 2: Average precision score 95% CI (UL)'},\n",
    "                             inplace = True)\n",
    "\n",
    "        post_test_melted['Subgroups are significantly different according to a comparison of confidence intervals'] = np.where((post_test_melted['Subgroup 1: Average precision score 95% CI (LL)'] >\n",
    "                                                                                        post_test_melted['Subgroup 2: Average precision score 95% CI (UL)']) |\n",
    "                                                                                                                               (post_test_melted['Subgroup 2: Average precision score 95% CI (LL)'] >\n",
    "                                                                                        post_test_melted['Subgroup 1: Average precision score 95% CI (UL)']), True, False)\n",
    "        post_test_melted_list.append(post_test_melted)\n",
    "        \n",
    "fairness_comparisons = pd.concat(post_test_melted_list, axis = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save biases\n",
    "fairness_comparisons.to_csv('{}/Models/Fairness_metrics_comparing_subgroups_{}.csv'.format(local_dir, file_stub), index = False)\n",
    "pd.Series(friedman_dict).to_csv('{}/Models/Friedman tests_{}.csv'.format(local_dir, file_stub))\n",
    "pd.Series(subgroups_bias_dict).to_csv('{}/Models/Bias in model_{}.csv'.format(local_dir, file_stub))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
