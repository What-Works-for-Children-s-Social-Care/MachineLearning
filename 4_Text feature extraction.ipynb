{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB Only for text_referall_SA_SD_s47 - copy for df_outcome1\n",
    "# named df_rq1 to save re-writing everything but only text_referall_SA_SD_s47 post-anonymisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to system path so we can find all the packages\n",
    "import sys\n",
    "sys.path\n",
    "sys.path.append('C:\\\\Program Files\\\\Python37\\\\Lib\\\\site-packages')\n",
    "sys.path.append('C:\\Program Files\\Python37')\n",
    "\n",
    "# Load user-written functions\n",
    "\n",
    "get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "get_ipython().run_line_magic('autoreload', '2')\n",
    "\n",
    "#import text_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking number of documents\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "os.chdir(\"C:\\\\Users\\\\[username]\\\\Downloads\\\\Updated Text Data\") # insert [username]\n",
    "\n",
    "filename = open(\"Created\\\\rq1data_text_for_anonymisation.pkl\", \"rb\")\n",
    "rq1data_text_for_anonymisation = pickle.load(filename)\n",
    "print(rq1data_text_for_anonymisation.shape)\n",
    "\n",
    "rq1_text_cols = (['Contact and Referral Form_text',\n",
    "       'Child Social Work Assessment for Review Child Protection Conference_text_prev',\n",
    "       'Child Social Work Assessment to Initial Child Protection Conference_text_prev',\n",
    "       'Child Social Work Assessment_text_prev',\n",
    "       'Contact and Referral Form_text_prev'])\n",
    "\n",
    "rq1_no_docs = 0\n",
    "for col in rq1_text_cols:\n",
    "    print(col)\n",
    "    non_empty = sum(rq1data_text_for_anonymisation[col] != '')\n",
    "    print(non_empty)\n",
    "    rq1_no_docs += non_empty\n",
    "    \n",
    "print(\"Number of documents for rq1: \", rq1_no_docs)\n",
    "\n",
    "# Rq2 \n",
    "filename = open(\"Created\\\\rq2data_text_for_anonymisation.pkl\", \"rb\")\n",
    "rq2data_text_for_anonymisation = pickle.load(filename)\n",
    "print(rq2data_text_for_anonymisation.shape)\n",
    "rq2_text_cols = (['Child Social Work Assessment for Review Child Protection Conference_text',\n",
    "       'Child Social Work Assessment to Initial Child Protection Conference_text',\n",
    "       'Child Social Work Assessment_text', 'Contact and Referral Form_text',\n",
    "       'Child Social Work Assessment for Review Child Protection Conference_text_prev',\n",
    "       'Child Social Work Assessment to Initial Child Protection Conference_text_prev',\n",
    "       'Child Social Work Assessment_text_prev',\n",
    "       'Contact and Referral Form_text_prev'])\n",
    "\n",
    "rq2_no_docs = 0\n",
    "for col in rq2_text_cols:\n",
    "    print(col)\n",
    "    non_empty = sum(rq2data_text_for_anonymisation[col] != '')\n",
    "    print(non_empty)\n",
    "    rq2_no_docs += non_empty\n",
    "    \n",
    "print(\"Number of documents for rq2: \", rq2_no_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "os.chdir(\"C:\\\\Users\\\\[username]\\\\Downloads\\\\Updated Text Data\") # insert [username]\n",
    "filename = open(\"Created\\\\text_rq1_names_matched.pkl\", \"rb\")\n",
    "text = pickle.load(filename)\n",
    "print(text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bring in emotion and concreteness lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call emotion lexicon\n",
    "# emolex_words is a dataframe of words and 0/1 as to whether they are associated with particular emotion\n",
    "import pandas as pd\n",
    "filepath = ('''Other\\\\Lexicons\\\\NRC-emotion-lexicon-wordlevel-alphabetized-v0.92.txt''')\n",
    "emolex_df = pd.read_csv(filepath, skiprows=28,\n",
    "                        names=[\"word\", \"emotion\", \"association\"],\n",
    "                        sep='\\t')\n",
    "emolex_words = emolex_df.pivot(index='word',\n",
    "                               columns='emotion',\n",
    "                               values='association').reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Concreteness\n",
    "# concreteness_df is a dataframe with the word and \"Conc.M\" is the mean of a concreteness score given by MTurks\n",
    "filepath = ('''Other\\\\Lexicons\\\\Concreteness_ratings_Brysbaert_et_al_BRM.txt''')\n",
    "concreteness_df = pd.read_csv(filepath,sep='\\t')\n",
    "concreteness_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U textblob\n",
    "#!python -m textblob.download_corporafrom textblob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify name and text columns\n",
    "text_columns = list(set(text.columns) - set(['PSID', 'ReferralDatetime', 'ReferralDatetime_previous']))\n",
    "print(text_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_functions import text_feature_creation\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Create features relating to polarity, subjectivity, emotions and concreteness using text_feature_creation function\n",
    "emotions = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'negative', 'positive', 'sadness', 'surprise', 'trust']\n",
    "\n",
    "df_emotions_list = []\n",
    "for column in text_columns[3:]:\n",
    "    print(column)\n",
    "    # Create text features\n",
    "    df_emotions = text_feature_creation(text, column, emotions)\n",
    "    df_emotions_list.append(df_emotions)\n",
    "    \n",
    "with open(\"Created\\\\df_emotions_list.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(df_emotions_list, handle, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the IDs with the features columns\n",
    "df_text_features = pd.concat(df_emotions_list, axis = 1)\n",
    "#print(df_text_features.head())\n",
    "\n",
    "with open(\"Created\\\\df_text_features.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(df_text_features, handle, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vulnerabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Social worker stop words\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "social_work_stop_words = ([\"children's social care\", \n",
    "                           \"case worker\", \"social worker\", \"duty worker\", \"foster carer\", \"sw\",\n",
    "                        \"contact\", \"referral\", \"single assessment\", \"assessment\", \n",
    "                           \"initial child protection conference\", \"child in need\", \"child protection\", \n",
    "                       \"strategy discussion\", \"section 47\", \"section 17\", \"section 20\" , \"looked after\",\n",
    "                           \"no further action\", \"enquiries\", \"recommendation\", \n",
    "                           \"SA\", \"S47\", \"S17\", \"S20\", \"CP\", \"CIN\", \"NFA\", \"LAC\", \"CLA\", \"CAMHS\", \"child\"])\n",
    "\n",
    "# Adding social work stop words\n",
    "for word in social_work_stop_words:\n",
    "    STOP_WORDS.add(word)\n",
    "\n",
    "for i, word in enumerate(STOP_WORDS):\n",
    "    lexeme = nlp.vocab[i]\n",
    "    lexeme.is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "from text_functions import RecogniseTagPhrases, vulnerabilities_phrases_dict\n",
    "\n",
    "# Add vulnerabilities to recognise\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "vulnerabilities_phrases = [y for x in vulnerabilities_phrases_dict.values() for y in x]\n",
    "vulnerabilities_merger = RecogniseTagPhrases(nlp, vulnerabilities_phrases, \"is_vulnerability\")\n",
    "nlp.add_pipe(vulnerabilities_merger, last=True)  # Add component to the pipeline\n",
    "\n",
    "# Other:  \"step mother\", \"step father\" - how to classify?\n",
    "\n",
    "text_columns = (['text_rq2_names_matched_Child Social Work Assessment to Initial Child Protection Conference_text'])\n",
    "rq = 'rq2'\n",
    "\n",
    "save_every = 100\n",
    "\n",
    "text_vulnerabilities = pd.DataFrame(0, index=text.index, columns = vulnerabilities_phrases_dict.keys())\n",
    "text_list_of_strings = pd.DataFrame(None, index=text.index, columns = text_columns, dtype = 'object')\n",
    "text_tokenised = pd.DataFrame(None, index=text.index, columns = text_columns, dtype = 'object')\n",
    "# Pre-process text data (lemmatise, remove punctuation, remove stop words)\n",
    "for col in text_columns:\n",
    "    print(\"Column: \", col)\n",
    "    text_list_of_strings[col] = ''\n",
    "    text_tokenised[col] = ''\n",
    "    already_done_indices = []\n",
    "    for idx in text[col].index:\n",
    "        print(idx)\n",
    "        if int(idx) not in already_done_indices:\n",
    "            string = text.loc[idx, col]\n",
    "            # If string is NaN\n",
    "            if string != string:\n",
    "                print(string)\n",
    "                #print(type(string[0]))\n",
    "                # Replacing NaN with empty strings\n",
    "                text_list_of_strings.at[idx, col] = ''\n",
    "                text_tokenised.at[idx, col] = ''\n",
    "            else:\n",
    "                doc = nlp(string) \n",
    "                # Make dummy if vulnerability appears in any of the columns\n",
    "                vulnerabilities = set([token.text for token in doc if token._.is_vulnerability is True])\n",
    "                if vulnerabilities:\n",
    "                    vulnerabilities_grouped = set([key for key, value in vulnerabilities_phrases_dict.items() for v in value if v in vulnerabilities])\n",
    "                    #print(\"Vulnerabilities:\", vulnerabilities)\n",
    "                    print(\"Vulnerabilities grouped:\", vulnerabilities_grouped)\n",
    "                    for vulnerability in vulnerabilities_grouped:\n",
    "                        text_vulnerabilities.at[idx, vulnerability] = 1\n",
    "                # Lemmatise and remove punctuation\n",
    "                alphas = [token for token in doc if token.is_alpha]\n",
    "                #print(\"Alphas: \", alphas)\n",
    "                # Remove stop words including social worker stop words\n",
    "                not_stop = [token for token in alphas if token.is_stop is False]\n",
    "                #print(\"Stop: \", [token for token in alphas if token.is_stop is True])\n",
    "                lemmas = [token.lemma_ for token in not_stop]\n",
    "                #print(\"Lemmas\", lemmas)\n",
    "                text_list_of_strings.at[idx, col] = ' '.join(lemmas)# sklearn tfidf vectoriser wants list of strings\n",
    "                text_tokenised.at[idx, col] = lemmas # gensim lda wants list of list of strings\n",
    "                # If already processed, use processed data\n",
    "                repeated_text_indices = text.index[np.where(text[col] ==  text.loc[idx,col])]\n",
    "                print(\"Repeated text indices: \", repeated_text_indices)\n",
    "                already_done_indices.extend(list(repeated_text_indices))\n",
    "                if repeated_text_indices != []:\n",
    "                    # String\n",
    "                    text_list_of_strings.loc[repeated_text_indices, col] = ' '.join(lemmas)\n",
    "                    # List of tokens needs more complex version\n",
    "                    text_tokenised_repeated = pd.concat([pd.DataFrame(text_tokenised.loc[idx,]).T]*len(repeated_text_indices))\n",
    "                    text_tokenised_repeated.set_index(repeated_text_indices, inplace = True)\n",
    "                    text_tokenised.update(text_tokenised_repeated)\n",
    "                    # Dataframe needs more complex version\n",
    "                    if vulnerabilities:\n",
    "                        text_vulnerabilities_repeated = pd.concat([pd.DataFrame(text_vulnerabilities.loc[idx,]).T]*len(repeated_text_indices))\n",
    "                        text_vulnerabilities_repeated.set_index(repeated_text_indices, inplace = True)\n",
    "                        text_vulnerabilities.update(text_vulnerabilities_repeated)\n",
    "        if (idx % save_every == 0) & (idx != 0):\n",
    "            # text_vulnerabilities column names describe the vulnerabilities not the document => save all\n",
    "            with open(\"Created/Preprocessed Text/text_{}_vulnerabilities_{}_{}_{}.pkl\".format(rq, col, idx - save_every, idx - 1), \"wb\") as handle:\n",
    "                pickle.dump(text_vulnerabilities.loc[idx-save_every: idx - 1,], handle, protocol = pickle.HIGHEST_PROTOCOL)   \n",
    "            with open(\"Created/Preprocessed Text/text_{}_list_of_strings_{}_{}_{}.pkl\".format(rq, col, idx - save_every, idx - 1), \"wb\") as handle:\n",
    "                pickle.dump(text_list_of_strings.loc[idx-save_every: idx - 1, col], handle, protocol = pickle.HIGHEST_PROTOCOL)   \n",
    "            with open(\"Created/Preprocessed Text/text_{}_tokenised_{}_{}_{}.pkl\".format(rq, col, idx - save_every, idx - 1), \"wb\") as handle:\n",
    "                pickle.dump(text_tokenised.loc[idx-save_every: idx - 1, col], handle, protocol = pickle.HIGHEST_PROTOCOL)   \n",
    "        elif (idx == text.shape[0] - 1):\n",
    "            with open(\"Created/Preprocessed Text/text_{}_vulnerabilities_{}_{}_{}.pkl\".format(rq, col, idx - idx % save_every - 1, idx), \"wb\") as handle:\n",
    "                pickle.dump(text_vulnerabilities.loc[idx - idx % save_every: idx,], handle, protocol = pickle.HIGHEST_PROTOCOL)   \n",
    "            with open(\"Created/Preprocessed Text/text_{}_list_of_strings_{}_{}_{}.pkl\".format(rq, col, idx - idx % save_every - 1, idx), \"wb\") as handle:\n",
    "                pickle.dump(text_list_of_strings.loc[idx - idx % save_every: idx, col], handle, protocol = pickle.HIGHEST_PROTOCOL)   \n",
    "            with open(\"Created/Preprocessed Text/text_{}_tokenised_{}_{}_{}.pkl\".format(rq, col, idx - idx % save_every - 1, idx), \"wb\") as handle:\n",
    "                    pickle.dump(text_tokenised.loc[idx - idx % save_every: idx, col], handle, protocol = pickle.HIGHEST_PROTOCOL)    \n",
    "\n",
    "\n",
    "# Count number of vulnerabilities\n",
    "text_vulnerabilities[\"Number of vulnerabilities\"] = text_vulnerabilities[list(vulnerabilities_phrases_dict.keys())].sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test Preprocessed\n",
    "\n",
    "# Knit together files\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# assert 1==2\n",
    "# Run for 'list_of_strings', 'tokenised', 'vulnerabilities'\n",
    "# Note different text_cols for rq2\n",
    "\n",
    "rq = 'rq1'\n",
    "file_type = 'vulnerabilities'\n",
    "\n",
    "os.chdir(\"C:\\\\Users\\\\[username]\\\\Downloads\\\\Updated Text Data\") # insert [username]\n",
    "filename_list = [file for file in glob.glob(\"Created\\\\Preprocessed Text\\\\text_{}_{}_*.pkl\".format(rq, file_type))]\n",
    "\n",
    "\n",
    "# Call in all files in the anonymisation folder\n",
    "file_dict = {}\n",
    "for file in filename_list:\n",
    "    try:\n",
    "        filename = open(file, \"rb\")\n",
    "        f = pickle.load(filename)\n",
    "        file_n = re.sub('.pkl', '', file)\n",
    "        file_n = re.sub(\"Created\\\\\\\\Preprocessed Text\\\\\\\\text_{}_{}_\".format(rq, file_type), '', file_n)\n",
    "        print(file_n)\n",
    "        file_dict[file_n] = f\n",
    "    except(EOFError):\n",
    "        continue\n",
    "\n",
    "col_wo_identifier_dict = {}\n",
    "for text_col in text_cols:\n",
    "    text_col_df = [df for df in file_dict.keys() if text_col in df]\n",
    "    if 'prev' in text_col:\n",
    "        text_col_df = [df for df in text_col_df if 'prev' in df]\n",
    "    else:\n",
    "        text_col_df = [df for df in text_col_df if 'prev' not in df]\n",
    "    print(text_col_df)\n",
    "    text_col_df_values = [v for k, v in file_dict.items() if k in text_col_df]\n",
    "    print(len(text_col_df_values))\n",
    "    col_wo_identifier_dict[text_col] = pd.concat(text_col_df_values, axis = 0)\n",
    "    col_wo_identifier_dict[text_col].sort_index(inplace = True)\n",
    "    \n",
    "from functools import reduce\n",
    "\n",
    "# Deduplicate by index (in case any rows are repeated in different files)\n",
    "for col, df in col_wo_identifier_dict.items():\n",
    "    print(col)\n",
    "    print(df.shape)\n",
    "    col_wo_identifier_dict[col] = df.loc[~df.index.duplicated(keep = 'first')]\n",
    "    print(col_wo_identifier_dict[col].shape)\n",
    "    if file_type == 'vulnerabilities':\n",
    "        col_wo_identifier_dict[col] = col_wo_identifier_dict[col].add_prefix(col + '_')\n",
    "    else:\n",
    "        col_wo_identifier_dict[col].rename(col, inplace = True)\n",
    "\n",
    "# Merge together on index\n",
    "df_wo_identifier_together = reduce(lambda left, right: pd.merge(left,right,left_index = True, right_index = True, how = 'outer'), col_wo_identifier_dict.values())\n",
    "df_wo_identifier_together = df_wo_identifier_together.loc[~df_wo_identifier_together.index.duplicated(keep = 'first')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in the original text dataset for anonymisation so that we can merge in the IDs\n",
    "# Reset index so the text lines up df_wo_identifier_together\n",
    "# The problem was a combination of the indices starting at 7221, 7222, 0, 1, 2 etc and missing data\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "os.chdir(\"C:\\\\Users\\\\[username]\\\\Downloads\\\\Updated Text Data\") # insert [username]\n",
    "filename = open(\"Created\\\\{}data_text_for_anonymisation.pkl\".format(rq), \"rb\")\n",
    "original_text = pickle.load(filename)\n",
    "print(original_text.index)\n",
    "original_text.reset_index(drop = True, inplace = True) # to line up with df_wo_identifier_together\n",
    "print(original_text.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge back in IDs\n",
    "print(original_text.shape)\n",
    "print(original_text.columns)\n",
    "final_text = pd.merge(original_text[['PSID', 'ReferralDatetime', 'ReferralDatetime_previous']], df_wo_identifier_together, left_index = True, right_index = True, how = 'left')\n",
    "print(final_text.shape)\n",
    "print(final_text.columns)\n",
    "\n",
    "# Replace missing with empty string\n",
    "\n",
    "if file_type != 'vulnerabilities':\n",
    "    print(final_text.isna().sum()) \n",
    "    final_text[text_cols] = final_text[text_cols].fillna('')\n",
    "    print(final_text.isna().sum())\n",
    "\n",
    "# Save knitted data all together \n",
    "with open(\"Created/text_{}_{}.pkl\".format(rq, file_type), \"wb\") as handle:\n",
    "    pickle.dump(final_text, handle, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
